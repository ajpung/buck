{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e5eff5-c905-4da2-b77d-3ed7a7ca0e9d",
   "metadata": {},
   "source": [
    "### Check RTX5090 running CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ab5f0-9231-4ae0-8fc4-d27217483190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Check if CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"❌ CUDA not detected by PyTorch\")\n",
    "\n",
    "# Test ResNet50 specifically\n",
    "model = models.resnet50(pretrained=True).cuda()\n",
    "test_batch = torch.randn(2, 3, 224, 224).cuda()\n",
    "try:\n",
    "    output = model(test_batch)\n",
    "    print(\"ResNet50 works!\")\n",
    "except Exception as e:\n",
    "    print(f\"ResNet50 failed: {e}\")\n",
    "\n",
    "# Test EfficientNet\n",
    "try:\n",
    "    model_eff = models.efficientnet_b0(pretrained=True).cuda()\n",
    "    output_eff = model_eff(test_batch)\n",
    "    print(\"EfficientNet works!\")\n",
    "except Exception as e:\n",
    "    print(f\"EfficientNet failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71bc11f-e647-462f-a1f3-466206538d7d",
   "metadata": {},
   "source": [
    "### Process deer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43154fba-18c6-4690-a3cd-6d4e8c863549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 75\n",
      "ACADEMICALLY RIGOROUS JAWBONE MODEL EVALUATION\n",
      "================================================================================\n",
      "Methodology:\n",
      "1. Split data: 80% training, 20% held-out test\n",
      "2. Test set is NEVER used until final evaluation\n",
      "3. Cross-validation on training data to select best architecture\n",
      "4. Train final model on all training data\n",
      "5. Evaluate once on held-out test set\n",
      "================================================================================\n",
      "Total images: 243\n",
      "Age distribution: {0.5: 39, 2.5: 33, 3.5: 29, 1.5: 62, 4.5: 20, 5.5: 60}\n",
      "\n",
      "ACADEMIC TRAIN/TEST SPLIT\n",
      "Training: 194 images\n",
      "Test (held-out): 49 images\n",
      "Test set will NOT be touched until final evaluation\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n",
      "GPU Memory: 34.2 GB\n",
      "\n",
      "CROSS-VALIDATION ON TRAINING DATA\n",
      "5-fold CV across 33 architectures\n",
      "Total CV experiments: 165\n",
      "\n",
      "============================================================\n",
      "TESTING: EFFICIENTNET_B0\n",
      "============================================================\n",
      "\n",
      "Fold 1/5\n",
      "  Parameters: 4,777,200 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 73.4%, Val 76.8%\n",
      "    Epoch 20: Train 100.0%, Val 77.8%\n",
      "    Epoch 40: Train 100.0%, Val 78.3%\n",
      "    Early stopping at epoch 48\n",
      "  Fold 1 Val Acc: 80.2%\n",
      "\n",
      "Fold 2/5\n",
      "  Parameters: 4,777,200 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 75.5%, Val 59.7%\n",
      "    Epoch 20: Train 100.0%, Val 64.1%\n",
      "    Epoch 40: Train 100.0%, Val 69.1%\n",
      "    Epoch 60: Train 100.0%, Val 68.7%\n",
      "    Early stopping at epoch 65\n",
      "  Fold 2 Val Acc: 69.1%\n",
      "\n",
      "Fold 3/5\n",
      "  Parameters: 4,777,200 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 74.9%, Val 78.7%\n",
      "    Epoch 20: Train 100.0%, Val 83.1%\n",
      "    Epoch 40: Train 100.0%, Val 83.1%\n",
      "    Epoch 60: Train 100.0%, Val 83.3%\n",
      "  Fold 3 Val Acc: 84.8%\n",
      "\n",
      "Fold 4/5\n",
      "  Parameters: 4,777,200 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 75.9%, Val 71.0%\n",
      "    Epoch 20: Train 100.0%, Val 76.1%\n",
      "    Early stopping at epoch 26\n",
      "  Fold 4 Val Acc: 77.5%\n",
      "\n",
      "Fold 5/5\n",
      "  Parameters: 4,777,200 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 74.4%, Val 64.7%\n",
      "    Epoch 20: Train 100.0%, Val 75.0%\n",
      "    Early stopping at epoch 35\n",
      "  Fold 5 Val Acc: 76.1%\n",
      "\n",
      "efficientnet_b0 CV Summary: 77.5% ± 5.2%\n",
      "\n",
      "============================================================\n",
      "TESTING: EFFICIENTNET_B1\n",
      "============================================================\n",
      "\n",
      "Fold 1/5\n",
      "  Parameters: 7,271,514 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 73.0%, Val 74.4%\n",
      "    Epoch 20: Train 100.0%, Val 78.1%\n",
      "    Early stopping at epoch 36\n",
      "  Fold 1 Val Acc: 79.4%\n",
      "\n",
      "Fold 2/5\n",
      "  Parameters: 7,271,514 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 74.8%, Val 62.0%\n",
      "    Epoch 20: Train 100.0%, Val 66.1%\n",
      "    Epoch 40: Train 100.0%, Val 67.8%\n",
      "    Epoch 60: Train 100.0%, Val 69.2%\n",
      "    Early stopping at epoch 77\n",
      "  Fold 2 Val Acc: 70.1%\n",
      "\n",
      "Fold 3/5\n",
      "  Parameters: 7,271,514 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 72.7%, Val 79.2%\n",
      "    Epoch 20: Train 100.0%, Val 82.1%\n",
      "    Epoch 40: Train 100.0%, Val 84.2%\n",
      "    Early stopping at epoch 56\n",
      "  Fold 3 Val Acc: 85.8%\n",
      "\n",
      "Fold 4/5\n",
      "  Parameters: 7,271,514 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 74.1%, Val 78.0%\n",
      "    Epoch 20: Train 100.0%, Val 79.6%\n",
      "    Early stopping at epoch 36\n",
      "  Fold 4 Val Acc: 83.0%\n",
      "\n",
      "Fold 5/5\n",
      "  Parameters: 7,271,514 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 75.9%, Val 66.5%\n",
      "    Epoch 20: Train 100.0%, Val 75.8%\n",
      "    Epoch 40: Train 100.0%, Val 77.9%\n",
      "    Early stopping at epoch 49\n",
      "  Fold 5 Val Acc: 79.6%\n",
      "\n",
      "efficientnet_b1 CV Summary: 79.6% ± 5.3%\n",
      "\n",
      "============================================================\n",
      "TESTING: EFFICIENTNET_B2\n",
      "============================================================\n",
      "\n",
      "Fold 1/5\n",
      "  Parameters: 8,524,860 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 83.1%, Val 70.7%\n",
      "    Epoch 20: Train 100.0%, Val 81.1%\n",
      "    Epoch 40: Train 100.0%, Val 81.1%\n",
      "    Early stopping at epoch 41\n",
      "  Fold 1 Val Acc: 82.1%\n",
      "\n",
      "Fold 2/5\n",
      "  Parameters: 8,524,860 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 84.6%, Val 60.5%\n",
      "    Epoch 20: Train 99.8%, Val 68.4%\n",
      "    Early stopping at epoch 33\n",
      "  Fold 2 Val Acc: 71.8%\n",
      "\n",
      "Fold 3/5\n",
      "  Parameters: 8,524,860 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 83.3%, Val 80.4%\n",
      "    Epoch 20: Train 100.0%, Val 87.4%\n",
      "    Early stopping at epoch 35\n",
      "  Fold 3 Val Acc: 88.6%\n",
      "\n",
      "Fold 4/5\n",
      "  Parameters: 8,524,860 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 83.5%, Val 77.5%\n",
      "    Epoch 20: Train 100.0%, Val 84.1%\n",
      "    Epoch 40: Train 100.0%, Val 79.2%\n",
      "    Early stopping at epoch 42\n",
      "  Fold 4 Val Acc: 85.0%\n",
      "\n",
      "Fold 5/5\n",
      "  Parameters: 8,524,860 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 86.0%, Val 73.3%\n",
      "    Epoch 20: Train 100.0%, Val 79.6%\n",
      "    Epoch 40: Train 100.0%, Val 80.3%\n",
      "    Early stopping at epoch 56\n",
      "  Fold 5 Val Acc: 81.2%\n",
      "\n",
      "efficientnet_b2 CV Summary: 81.7% ± 5.6%\n",
      "\n",
      "============================================================\n",
      "TESTING: EFFICIENTNET_B3\n",
      "============================================================\n",
      "\n",
      "Fold 1/5\n",
      "  Parameters: 11,563,264 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 87.5%, Val 73.2%\n",
      "    Epoch 20: Train 100.0%, Val 76.4%\n",
      "    Early stopping at epoch 30\n",
      "  Fold 1 Val Acc: 82.7%\n",
      "\n",
      "Fold 2/5\n",
      "  Parameters: 11,563,264 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 86.3%, Val 67.8%\n",
      "    Epoch 20: Train 100.0%, Val 70.1%\n",
      "    Early stopping at epoch 26\n",
      "  Fold 2 Val Acc: 72.8%\n",
      "\n",
      "Fold 3/5\n",
      "  Parameters: 11,563,264 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 85.0%, Val 83.9%\n",
      "    Epoch 20: Train 100.0%, Val 84.8%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "IMAGE_SIZE = (672, 1344)\n",
    "AUGMENTATION_TARGET = 1000\n",
    "NUM_FOLDS = 5\n",
    "MIXED_PRECISION = True\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    'efficientnet_b0': {'model_fn': models.efficientnet_b0, 'batch_size': 48, 'freeze_layers': 3},\n",
    "    'efficientnet_b1': {'model_fn': models.efficientnet_b1, 'batch_size': 36, 'freeze_layers': 3},\n",
    "    'efficientnet_b2': {'model_fn': models.efficientnet_b2, 'batch_size': 30, 'freeze_layers': 3},\n",
    "    'efficientnet_b3': {'model_fn': models.efficientnet_b3, 'batch_size': 24, 'freeze_layers': 3},\n",
    "    'efficientnet_b4': {'model_fn': models.efficientnet_b4, 'batch_size': 18, 'freeze_layers': 3},\n",
    "    'efficientnet_b5': {'model_fn': models.efficientnet_b5, 'batch_size': 12, 'freeze_layers': 3},\n",
    "    'efficientnet_b6': {'model_fn': models.efficientnet_b6, 'batch_size': 9, 'freeze_layers': 3},\n",
    "    'efficientnet_b7': {'model_fn': models.efficientnet_b7, 'batch_size': 6, 'freeze_layers': 3},\n",
    "    'resnet18': {'model_fn': models.resnet18, 'batch_size': 72, 'freeze_layers': 2},\n",
    "    'resnet34': {'model_fn': models.resnet34, 'batch_size': 60, 'freeze_layers': 2},\n",
    "    'resnet50': {'model_fn': models.resnet50, 'batch_size': 48, 'freeze_layers': 2},\n",
    "    'resnet101': {'model_fn': models.resnet101, 'batch_size': 30, 'freeze_layers': 2},\n",
    "    'resnet152': {'model_fn': models.resnet152, 'batch_size': 24, 'freeze_layers': 2},\n",
    "    'densenet121': {'model_fn': models.densenet121, 'batch_size': 36, 'freeze_layers': 2},\n",
    "    'densenet169': {'model_fn': models.densenet169, 'batch_size': 30, 'freeze_layers': 2},\n",
    "    'densenet201': {'model_fn': models.densenet201, 'batch_size': 24, 'freeze_layers': 2},\n",
    "    'mobilenet_v2': {'model_fn': models.mobilenet_v2, 'batch_size': 60, 'freeze_layers': 3},\n",
    "    'mobilenet_v3_small': {'model_fn': models.mobilenet_v3_small, 'batch_size': 72, 'freeze_layers': 3},\n",
    "    'mobilenet_v3_large': {'model_fn': models.mobilenet_v3_large, 'batch_size': 60, 'freeze_layers': 3},\n",
    "    'regnet_y_400mf': {'model_fn': models.regnet_y_400mf, 'batch_size': 48, 'freeze_layers': 2},\n",
    "    'regnet_y_800mf': {'model_fn': models.regnet_y_800mf, 'batch_size': 36, 'freeze_layers': 2},\n",
    "    'regnet_y_1_6gf': {'model_fn': models.regnet_y_1_6gf, 'batch_size': 30, 'freeze_layers': 2},\n",
    "    'regnet_y_3_2gf': {'model_fn': models.regnet_y_3_2gf, 'batch_size': 24, 'freeze_layers': 2},\n",
    "    'convnext_tiny': {'model_fn': models.convnext_tiny, 'batch_size': 36, 'freeze_layers': 2},\n",
    "    'convnext_small': {'model_fn': models.convnext_small, 'batch_size': 30, 'freeze_layers': 2},\n",
    "    'convnext_base': {'model_fn': models.convnext_base, 'batch_size': 24, 'freeze_layers': 2},\n",
    "    'maxvit_t': {'model_fn': models.maxvit_t, 'batch_size': 18, 'freeze_layers': 2},\n",
    "    'swin_t': {'model_fn': models.swin_t, 'batch_size': 24, 'freeze_layers': 2},\n",
    "    'swin_s': {'model_fn': models.swin_s, 'batch_size': 18, 'freeze_layers': 2},\n",
    "    'swin_b': {'model_fn': models.swin_b, 'batch_size': 12, 'freeze_layers': 2},\n",
    "    'vit_b_16': {'model_fn': models.vit_b_16, 'batch_size': 24, 'freeze_layers': 6},\n",
    "    'vit_b_32': {'model_fn': models.vit_b_32, 'batch_size': 36, 'freeze_layers': 6},\n",
    "    'vit_l_16': {'model_fn': models.vit_l_16, 'batch_size': 12, 'freeze_layers': 8},\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'backbone_lr': 0.0001,\n",
    "    'classifier_lr': 0.0005,\n",
    "    'optimizer': 'adamw',\n",
    "    'weight_decay': 0.05,\n",
    "    'scheduler': 'cosine',\n",
    "    'label_smoothing': 0.1,\n",
    "    'dropout': 0.3,\n",
    "    'max_epochs': 80,\n",
    "    'patience': 25,\n",
    "    'augmentation_strength': 'medium'\n",
    "}\n",
    "\n",
    "def load_jawbone_data():\n",
    "    fpath = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\jawbone\\\\images\\\\*.png\"\n",
    "    \n",
    "    image_paths = glob.glob(fpath)\n",
    "    if not image_paths:\n",
    "        raise FileNotFoundError(f\"No images found at {fpath}\")\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            \n",
    "            bbb_part = parts[1]\n",
    "            \n",
    "            if 'p' not in bbb_part:\n",
    "                continue\n",
    "            \n",
    "            value_str = bbb_part.replace('p', '.')\n",
    "            try:\n",
    "                age_value = float(value_str)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            images.append(img_resized)\n",
    "            ages.append(age_value)\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if not images:\n",
    "        raise ValueError(\"No valid images loaded\")\n",
    "    \n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    \n",
    "    for img, age in zip(images, ages_grouped):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "    \n",
    "    print(f\"Total images: {len(filtered_images)}\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    \n",
    "    return np.array(filtered_images, dtype=np.uint8), filtered_ages\n",
    "\n",
    "def enhanced_augment_image(image, strength='medium'):\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    if strength == 'light':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.5, 0.3, 0.6, 0.2, 0.1\n",
    "        rot_range, bright_range = 8, (0.85, 1.15)\n",
    "    elif strength == 'medium':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.7, 0.5, 0.8, 0.4, 0.3\n",
    "        rot_range, bright_range = 12, (0.75, 1.25)\n",
    "    else:\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.8, 0.6, 0.9, 0.5, 0.4\n",
    "        rot_range, bright_range = 18, (0.65, 1.35)\n",
    "    \n",
    "    if random.random() < rot_prob:\n",
    "        angle = random.uniform(-rot_range, rot_range)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < flip_prob:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    if random.random() < bright_prob:\n",
    "        alpha = random.uniform(*bright_range)\n",
    "        beta = random.randint(-20, 20)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    if random.random() < gamma_prob:\n",
    "        gamma = random.uniform(0.85, 1.15)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    if random.random() < noise_prob:\n",
    "        noise = np.random.normal(0, 5, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class OptimizedDataset(Dataset):\n",
    "    def __init__(self, base_images, labels, aug_strength='medium', target_per_class=1000, training=True):\n",
    "        self.base_images = base_images\n",
    "        self.labels = np.array(labels)\n",
    "        self.aug_strength = aug_strength\n",
    "        self.training = training\n",
    "        self.target_per_class = target_per_class\n",
    "        \n",
    "        unique_classes = np.unique(labels)\n",
    "        self.class_to_indices = {}\n",
    "        for cls in unique_classes:\n",
    "            self.class_to_indices[cls] = np.where(self.labels == cls)[0]\n",
    "        \n",
    "        self.num_classes = len(unique_classes)\n",
    "        self.class_list = sorted(unique_classes)\n",
    "        self.length = self.num_classes * self.target_per_class\n",
    "        \n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(3, 1, 1)\n",
    "        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        class_idx = idx // self.target_per_class\n",
    "        within_class_idx = idx % self.target_per_class\n",
    "        \n",
    "        target_class = self.class_list[class_idx]\n",
    "        available_indices = self.class_to_indices[target_class]\n",
    "        \n",
    "        base_idx = available_indices[within_class_idx % len(available_indices)]\n",
    "        image = self.base_images[base_idx].copy()\n",
    "        \n",
    "        if self.training and within_class_idx >= len(available_indices):\n",
    "            image = enhanced_augment_image(image, self.aug_strength)\n",
    "        \n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.transpose(2, 0, 1)\n",
    "        \n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = np.flip(image, axis=2).copy()\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return torch.from_numpy(image.astype(np.float32)), target_class\n",
    "\n",
    "class AcademicModelTrainer:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"academic_jawbone_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.cv_results = []\n",
    "        self.best_cv_model_info = None\n",
    "        self.best_cv_score = 0.0\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "            \n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            \n",
    "            if MIXED_PRECISION:\n",
    "                self.scaler = torch.amp.GradScaler('cuda')\n",
    "            else:\n",
    "                self.scaler = None\n",
    "    \n",
    "    def create_model(self, model_name, model_config):\n",
    "        model = model_config['model_fn'](weights='DEFAULT')\n",
    "        \n",
    "        freeze_layers = model_config.get('freeze_layers', 2)\n",
    "        \n",
    "        if 'mobilenet' in model_name:\n",
    "            layers_to_freeze = list(model.features.children())[:freeze_layers]\n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            original_features = model.classifier[-1].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        elif hasattr(model, 'features') and hasattr(model, 'classifier'):\n",
    "            layers_to_freeze = list(model.features.children())[:freeze_layers]\n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            if isinstance(model.classifier, nn.Sequential):\n",
    "                original_features = model.classifier[-1].in_features\n",
    "            else:\n",
    "                original_features = model.classifier.in_features\n",
    "            \n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        elif 'resnet' in model_name:\n",
    "            layers_to_freeze = [model.conv1, model.bn1]\n",
    "            if freeze_layers >= 1:\n",
    "                layers_to_freeze.append(model.layer1)\n",
    "            if freeze_layers >= 2:\n",
    "                layers_to_freeze.append(model.layer2)\n",
    "            \n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            original_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        elif 'vit' in model_name or 'swin' in model_name or 'maxvit' in model_name or 'convnext' in model_name:\n",
    "            if hasattr(model, 'head'):\n",
    "                if hasattr(model.head, 'in_features'):\n",
    "                    original_features = model.head.in_features\n",
    "                else:\n",
    "                    original_features = model.head[-1].in_features\n",
    "                \n",
    "                model.head = nn.Sequential(\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                    nn.Linear(original_features, 512),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                    nn.Linear(256, self.num_classes)\n",
    "                )\n",
    "            elif hasattr(model, 'heads'):\n",
    "                original_features = model.heads.head.in_features\n",
    "                model.heads.head = nn.Sequential(\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                    nn.Linear(original_features, 512),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                    nn.Linear(256, self.num_classes)\n",
    "                )\n",
    "        \n",
    "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        frozen = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "        print(f\"  Parameters: {trainable:,} trainable, {frozen:,} frozen\")\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def train_model(self, model, model_name, train_loader, val_loader, fold_num):\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'fc' in name or 'classifier' in name or 'head' in name:\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': TRAINING_CONFIG['backbone_lr']},\n",
    "            {'params': classifier_params, 'lr': TRAINING_CONFIG['classifier_lr']}\n",
    "        ], weight_decay=TRAINING_CONFIG['weight_decay'])\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAINING_CONFIG['max_epochs'])\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=TRAINING_CONFIG['label_smoothing'])\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        best_state = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(TRAINING_CONFIG['max_epochs']):\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                if MIXED_PRECISION and self.scaler is not None:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "                    \n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    \n",
    "                    if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                        self.scaler.step(optimizer)\n",
    "                        self.scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            train_acc = 100. * train_correct / train_total\n",
    "            \n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            val_acc = 100. * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_state = model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= TRAINING_CONFIG['patience']:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        return best_state, best_val_acc\n",
    "    \n",
    "    def run_cv_on_training_data(self, X_train, y_train, label_mapping):\n",
    "        print(f\"\\nCROSS-VALIDATION ON TRAINING DATA\")\n",
    "        print(f\"{NUM_FOLDS}-fold CV across {len(MODEL_CONFIGS)} architectures\")\n",
    "        print(f\"Total CV experiments: {len(MODEL_CONFIGS) * NUM_FOLDS}\")\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "        \n",
    "        for model_name, model_config in MODEL_CONFIGS.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"TESTING: {model_name.upper()}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            fold_scores = []\n",
    "            \n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "                print(f\"\\nFold {fold_idx}/{NUM_FOLDS}\")\n",
    "                \n",
    "                try:\n",
    "                    X_fold_train = X_train[train_idx]\n",
    "                    y_fold_train = y_train[train_idx]\n",
    "                    X_fold_val = X_train[val_idx]\n",
    "                    y_fold_val = y_train[val_idx]\n",
    "                    \n",
    "                    train_dataset = OptimizedDataset(X_fold_train, y_fold_train, \n",
    "                                                   TRAINING_CONFIG['augmentation_strength'], AUGMENTATION_TARGET, True)\n",
    "                    val_dataset = OptimizedDataset(X_fold_val, y_fold_val, 'light', 200, False)\n",
    "                    \n",
    "                    batch_size = model_config['batch_size']\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                            num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "                    \n",
    "                    model = self.create_model(model_name, model_config)\n",
    "                    best_state, val_acc = self.train_model(model, model_name, train_loader, val_loader, fold_idx)\n",
    "                    \n",
    "                    fold_scores.append(val_acc)\n",
    "                    print(f\"  Fold {fold_idx} Val Acc: {val_acc:.1f}%\")\n",
    "                    \n",
    "                    del model, train_dataset, val_dataset, train_loader, val_loader\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  FAILED: {str(e)}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "            \n",
    "            if fold_scores:\n",
    "                mean_cv_score = np.mean(fold_scores)\n",
    "                std_cv_score = np.std(fold_scores)\n",
    "                \n",
    "                self.cv_results.append({\n",
    "                    'model_name': model_name,\n",
    "                    'cv_scores': fold_scores,\n",
    "                    'mean_cv': mean_cv_score,\n",
    "                    'std_cv': std_cv_score\n",
    "                })\n",
    "                \n",
    "                print(f\"\\n{model_name} CV Summary: {mean_cv_score:.1f}% ± {std_cv_score:.1f}%\")\n",
    "                \n",
    "                if mean_cv_score > self.best_cv_score:\n",
    "                    self.best_cv_score = mean_cv_score\n",
    "                    self.best_cv_model_info = {\n",
    "                        'model_name': model_name,\n",
    "                        'model_config': model_config,\n",
    "                        'mean_cv': mean_cv_score,\n",
    "                        'std_cv': std_cv_score\n",
    "                    }\n",
    "        \n",
    "        self.save_cv_results()\n",
    "        return self.best_cv_model_info\n",
    "    \n",
    "    def train_final_model_on_all_training_data(self, X_train, y_train, model_info):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TRAINING FINAL MODEL ON ALL TRAINING DATA\")\n",
    "        print(f\"Best model from CV: {model_info['model_name']}\")\n",
    "        print(f\"CV Score: {model_info['mean_cv']:.1f}% ± {model_info['std_cv']:.1f}%\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        model_name = model_info['model_name']\n",
    "        model_config = model_info['model_config']\n",
    "        \n",
    "        X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        train_dataset = OptimizedDataset(X_train_final, y_train_final, \n",
    "                                       TRAINING_CONFIG['augmentation_strength'], AUGMENTATION_TARGET, True)\n",
    "        val_dataset = OptimizedDataset(X_val_final, y_val_final, 'light', 200, False)\n",
    "        \n",
    "        batch_size = model_config['batch_size']\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "        \n",
    "        model = self.create_model(model_name, model_config)\n",
    "        final_state, final_val_acc = self.train_model(model, model_name, train_loader, val_loader, 0)\n",
    "        \n",
    "        model.load_state_dict(final_state)\n",
    "        \n",
    "        print(f\"\\nFinal model training complete: {final_val_acc:.1f}%\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def evaluate_on_held_out_test_set(self, model, X_test, y_test, model_name, model_config):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"HELD-OUT TEST SET EVALUATION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        test_dataset = OptimizedDataset(X_test, y_test, 'light', 200, False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=model_config['batch_size'], shuffle=False, \n",
    "                                num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "        \n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        test_acc = 100. * test_correct / test_total\n",
    "        \n",
    "        print(f\"\\nTest Set Size: {test_total} images\")\n",
    "        print(f\"Test Accuracy: {test_acc:.1f}%\")\n",
    "        \n",
    "        save_path = os.path.join(self.save_dir, f\"{model_name}_final_test{test_acc:.1f}.pth\")\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_name': model_name,\n",
    "            'cv_mean': self.best_cv_model_info['mean_cv'],\n",
    "            'cv_std': self.best_cv_model_info['std_cv'],\n",
    "            'test_accuracy': test_acc,\n",
    "            'image_size': IMAGE_SIZE,\n",
    "            'training_config': TRAINING_CONFIG\n",
    "        }, save_path)\n",
    "        \n",
    "        print(f\"\\nFinal model saved: {save_path}\")\n",
    "        \n",
    "        return test_acc\n",
    "    \n",
    "    def save_cv_results(self):\n",
    "        results_path = os.path.join(self.save_dir, \"cv_results.json\")\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(self.cv_results, f, indent=2)\n",
    "    \n",
    "    def print_cv_summary(self):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CROSS-VALIDATION SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        sorted_results = sorted(self.cv_results, key=lambda x: x['mean_cv'], reverse=True)\n",
    "        \n",
    "        for result in sorted_results[:10]:\n",
    "            print(f\"{result['model_name']:25} | CV: {result['mean_cv']:5.1f}% ± {result['std_cv']:4.1f}%\")\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"ACADEMICALLY RIGOROUS JAWBONE MODEL EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Methodology:\")\n",
    "    print(\"1. Split data: 80% training, 20% held-out test\")\n",
    "    print(\"2. Test set is NEVER used until final evaluation\")\n",
    "    print(\"3. Cross-validation on training data to select best architecture\")\n",
    "    print(\"4. Train final model on all training data\")\n",
    "    print(\"5. Evaluate once on held-out test set\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    images, ages = load_jawbone_data()\n",
    "    \n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    y_indices = np.array([label_mapping[age] for age in ages])\n",
    "    \n",
    "    print(f\"\\nACADEMIC TRAIN/TEST SPLIT\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images, y_indices, test_size=0.2, random_state=42, stratify=y_indices\n",
    "    )\n",
    "    print(f\"Training: {len(X_train)} images\")\n",
    "    print(f\"Test (held-out): {len(X_test)} images\")\n",
    "    print(\"Test set will NOT be touched until final evaluation\")\n",
    "    \n",
    "    trainer = AcademicModelTrainer(num_classes=len(unique_ages))\n",
    "    \n",
    "    best_model_info = trainer.run_cv_on_training_data(X_train, y_train, label_mapping)\n",
    "    \n",
    "    trainer.print_cv_summary()\n",
    "    \n",
    "    if best_model_info is None:\n",
    "        print(\"\\nNo models successfully completed CV\")\n",
    "        return\n",
    "    \n",
    "    final_model = trainer.train_final_model_on_all_training_data(X_train, y_train, best_model_info)\n",
    "    \n",
    "    test_accuracy = trainer.evaluate_on_held_out_test_set(\n",
    "        final_model, X_test, y_test, \n",
    "        best_model_info['model_name'], \n",
    "        best_model_info['model_config']\n",
    "    )\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Best Architecture: {best_model_info['model_name']}\")\n",
    "    print(f\"Cross-Validation: {best_model_info['mean_cv']:.1f}% ± {best_model_info['std_cv']:.1f}%\")\n",
    "    print(f\"Held-Out Test: {test_accuracy:.1f}%\")\n",
    "    print(f\"Total Time: {elapsed:.1f} minutes\")\n",
    "    print(f\"Results saved to: {trainer.save_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96221295-b68b-41f1-a553-fd2261269c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4c189-d24a-4497-b0e5-aef0e9265db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
