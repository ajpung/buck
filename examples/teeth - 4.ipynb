{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a5b6e2-ade6-40ef-9121-92a94d2a855e",
   "metadata": {},
   "source": [
    "## What changed?\n",
    "\n",
    "This notebook uses the same method as `teeth - 3` to build the ensemble, but does so in a more academically rigorous manner. In the previous notebook, ALL images were used to train the data. In this notebook, a section of the images are quarantined so that the test data is truly never experienced until after the model is build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62247f7c-827d-4f33-99dd-bc10db9c0700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 2060\n",
      "GPU memory: 6.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"❌ CUDA not detected by PyTorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa4928b-d228-4d97-b29e-b0b0abf0737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academically Rigorous Multi-Architecture Ensemble Training\n",
      "================================================================================\n",
      "Available architectures: ['resnet18', 'resnet34', 'resnet50', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'mobilenetv3_large_100']\n",
      "Number of folds: 5\n",
      "Image size: 448x224\n",
      "Augmentation target: 1000 samples per class\n",
      "Loading data...\n",
      "Total images: 243\n",
      "Age distribution: {0.5: 39, 2.5: 33, 3.5: 29, 1.5: 62, 4.5: 20, 5.5: 60}\n",
      "\n",
      "--- ACADEMIC TRAIN/TEST SPLIT ---\n",
      "Training data: 194 images\n",
      "Test data: 49 images\n",
      "Test set will NOT be used until final evaluation\n",
      "Results will be saved to: academic_ensemble_20250721_081845\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "GPU Memory: 6.0 GB\n",
      "\n",
      "--- CROSS-VALIDATION ON TRAINING DATA ONLY ---\n",
      "Training Fold 1/5\n",
      "  Testing resnet18...\n",
      "  Testing resnet34...\n",
      "  Testing resnet50...\n",
      "  Testing efficientnet_b0...\n",
      "  Testing efficientnet_b1...\n",
      "  Testing efficientnet_b2...\n",
      "  Testing mobilenetv3_large_100...\n",
      "  Saved fold 1 to: academic_ensemble_20250721_081845\\efficientnet_b2_fold_1_92.3pct.pth\n",
      "Fold 1/5 completed: 92.3% with efficientnet_b2\n",
      "Training Fold 2/5\n",
      "  Testing resnet18...\n",
      "  Testing resnet34...\n",
      "  Testing resnet50...\n",
      "  Testing efficientnet_b0...\n",
      "  Testing efficientnet_b1...\n",
      "  Testing efficientnet_b2...\n",
      "  Testing mobilenetv3_large_100...\n",
      "  Saved fold 2 to: academic_ensemble_20250721_081845\\efficientnet_b0_fold_2_87.2pct.pth\n",
      "Fold 2/5 completed: 87.2% with efficientnet_b0\n",
      "Training Fold 3/5\n",
      "  Testing resnet18...\n",
      "  Testing resnet34...\n",
      "  Testing resnet50...\n",
      "  Testing efficientnet_b0...\n",
      "  Testing efficientnet_b1...\n",
      "  Testing efficientnet_b2...\n",
      "  Testing mobilenetv3_large_100...\n",
      "  Saved fold 3 to: academic_ensemble_20250721_081845\\efficientnet_b0_fold_3_94.9pct.pth\n",
      "Fold 3/5 completed: 94.9% with efficientnet_b0\n",
      "Training Fold 4/5\n",
      "  Testing resnet18...\n",
      "  Testing resnet34...\n",
      "  Testing resnet50...\n",
      "  Testing efficientnet_b0...\n",
      "  Testing efficientnet_b1...\n",
      "  Testing efficientnet_b2...\n",
      "  Testing mobilenetv3_large_100...\n",
      "  Saved fold 4 to: academic_ensemble_20250721_081845\\efficientnet_b2_fold_4_89.7pct.pth\n",
      "Fold 4/5 completed: 89.7% with efficientnet_b2\n",
      "Training Fold 5/5\n",
      "  Testing resnet18...\n",
      "  Testing resnet34...\n",
      "  Testing resnet50...\n",
      "  Testing efficientnet_b0...\n",
      "  Testing efficientnet_b1...\n",
      "  Testing efficientnet_b2...\n",
      "  Testing mobilenetv3_large_100...\n",
      "  Saved fold 5 to: academic_ensemble_20250721_081845\\efficientnet_b0_fold_5_89.5pct.pth\n",
      "Fold 5/5 completed: 89.5% with efficientnet_b0\n",
      "\n",
      "--- FINAL EVALUATION ON HELD-OUT TEST SET ---\n",
      "Saving final ensemble...\n",
      "Final ensemble saved to: academic_ensemble_20250721_081845\\academic_ensemble.pth\n",
      "Generating training curves...\n",
      "\n",
      "=== ACADEMIC RESULTS ===\n",
      "Cross-Validation Results (Training Data Only):\n",
      "Fold 1: 92.3% (efficientnet_b2)\n",
      "Fold 2: 87.2% (efficientnet_b0)\n",
      "Fold 3: 94.9% (efficientnet_b0)\n",
      "Fold 4: 89.7% (efficientnet_b2)\n",
      "Fold 5: 89.5% (efficientnet_b0)\n",
      "CV Mean: 90.7% ± 2.6%\n",
      "\n",
      "Final Test Accuracy (Held-Out Data): 77.6%\n",
      "Training Time: 536.3 minutes\n",
      "All results saved to: academic_ensemble_20250721_081845\n",
      "\n",
      "=== OVERFITTING ANALYSIS ===\n",
      "Check training_plots/ for loss curves to verify no overfitting\n"
     ]
    }
   ],
   "source": [
    "# Academically Rigorous Multi-Architecture Ensemble Training Script\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "AVAILABLE_ARCHITECTURES = ['resnet18', 'resnet34', 'resnet50', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'mobilenetv3_large_100']\n",
    "AUGMENTATION_TARGET = 1000\n",
    "NUM_FOLDS = 5\n",
    "IMAGE_SIZE = (224, 448)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    MIXED_PRECISION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXED_PRECISION_AVAILABLE = False\n",
    "    class autocast:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "def load_original_data():\n",
    "    try:\n",
    "        fpath = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\tooth_analysis\\\\images\\\\*.png\"\n",
    "        \n",
    "        image_paths = glob.glob(fpath)\n",
    "        if not image_paths:\n",
    "            raise FileNotFoundError(f\"No images found at {fpath}\")\n",
    "        \n",
    "        images = []\n",
    "        ages = []\n",
    "        \n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                \n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "                \n",
    "                filename = os.path.basename(img_path)\n",
    "                filename_no_ext = os.path.splitext(filename)[0]\n",
    "                parts = filename_no_ext.split('_')\n",
    "                \n",
    "                if len(parts) < 3:\n",
    "                    continue\n",
    "                \n",
    "                bbb_part = parts[1]\n",
    "                \n",
    "                if 'p' not in bbb_part:\n",
    "                    continue\n",
    "                \n",
    "                value_str = bbb_part.replace('p', '.')\n",
    "                try:\n",
    "                    age_value = float(value_str)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                \n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if not images:\n",
    "            raise ValueError(\"No valid images loaded\")\n",
    "        \n",
    "        ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "        \n",
    "        age_counts = Counter(ages_grouped)\n",
    "        valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "        \n",
    "        filtered_images = []\n",
    "        filtered_ages = []\n",
    "        \n",
    "        for img, age in zip(images, ages_grouped):\n",
    "            if age in valid_ages:\n",
    "                filtered_images.append(img)\n",
    "                filtered_ages.append(age)\n",
    "        \n",
    "        return np.array(filtered_images), filtered_ages\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def enhanced_augment_image(image):\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    if random.random() < 0.7:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    if random.random() < 0.8:\n",
    "        alpha = random.uniform(0.7, 1.3)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    if random.random() < 0.4:\n",
    "        gamma = random.uniform(0.8, 1.2)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    if random.random() < 0.3:\n",
    "        noise = np.random.normal(0, 7, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_augmented_data(X_train, y_train):\n",
    "    class_counts = Counter(y_train)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max(AUGMENTATION_TARGET, max_count)\n",
    "    \n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    \n",
    "    for class_idx in range(len(set(y_train))):\n",
    "        class_mask = y_train == class_idx\n",
    "        class_images = X_train[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        if current_count == 0:\n",
    "            continue\n",
    "        \n",
    "        for _ in range(4):\n",
    "            X_aug.extend(class_images)\n",
    "            y_aug.extend([class_idx] * current_count)\n",
    "        \n",
    "        needed = target_count - (current_count * 4)\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image(class_images[orig_idx].copy())\n",
    "            X_aug.append(aug_img)\n",
    "            y_aug.append(class_idx)\n",
    "    \n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "class OptimizedDataset(Dataset):\n",
    "    def __init__(self, X, y, test_time_aug=False):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.test_time_aug = test_time_aug\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        if image.shape[-2:] != IMAGE_SIZE:\n",
    "            image = F.interpolate(image.unsqueeze(0), size=IMAGE_SIZE, mode='bilinear', align_corners=False).squeeze(0)\n",
    "        \n",
    "        if self.test_time_aug and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class MultiArchEnsembleTrainer:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"academic_ensemble_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        print(f\"Results will be saved to: {self.save_dir}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            if MIXED_PRECISION_AVAILABLE:\n",
    "                self.scaler = GradScaler()\n",
    "                self.use_amp = True\n",
    "            else:\n",
    "                self.use_amp = False\n",
    "        else:\n",
    "            self.use_amp = False\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    def create_model(self, architecture):\n",
    "        model = timm.create_model(architecture, pretrained=True, num_classes=self.num_classes)\n",
    "        \n",
    "        if 'resnet' in architecture:\n",
    "            frozen_layers = ['conv1', 'bn1', 'layer1', 'layer2', 'layer3']\n",
    "            for name, param in model.named_parameters():\n",
    "                for frozen_layer in frozen_layers:\n",
    "                    if name.startswith(frozen_layer):\n",
    "                        param.requires_grad = False\n",
    "                        break\n",
    "        elif 'efficientnet' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'blocks.0' in name or 'blocks.1' in name or 'blocks.2' in name:\n",
    "                    param.requires_grad = False\n",
    "        elif 'mobilenet' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'features.0' in name or 'features.1' in name or 'features.2' in name:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        if hasattr(model, 'fc'):\n",
    "            in_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        elif hasattr(model, 'classifier'):\n",
    "            if hasattr(model.classifier, 'in_features'):\n",
    "                in_features = model.classifier.in_features\n",
    "                model.classifier = nn.Sequential(\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(in_features, self.num_classes)\n",
    "                )\n",
    "            else:\n",
    "                in_features = model.classifier[-1].in_features\n",
    "                model.classifier[-1] = nn.Sequential(\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(in_features, self.num_classes)\n",
    "                )\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def train_single_model(self, train_loader, val_loader, architecture):\n",
    "        model = self.create_model(architecture)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'fc' in name or 'classifier' in name:\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': 0.0003},\n",
    "            {'params': classifier_params, 'lr': 0.001}\n",
    "        ], weight_decay=0.03)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80, eta_min=1e-6)\n",
    "        \n",
    "        max_epochs = 80\n",
    "        patience = 20\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        training_history = {\n",
    "            'train_accs': [],\n",
    "            'val_accs': [],\n",
    "            'train_losses': [],\n",
    "            'val_losses': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss_total = 0.0\n",
    "            train_batches = 0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.step(optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "                train_loss_total += loss.item()\n",
    "                train_batches += 1\n",
    "                \n",
    "                if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            train_loss = train_loss_total / train_batches\n",
    "            \n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_loss_total = 0.0\n",
    "            val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    if self.use_amp:\n",
    "                        with autocast():\n",
    "                            outputs = model(images)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "                    val_loss_total += loss.item()\n",
    "                    val_batches += 1\n",
    "                    \n",
    "                    if batch_idx % 5 == 0 and torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "            \n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            val_loss = val_loss_total / val_batches\n",
    "            scheduler.step()\n",
    "            \n",
    "            training_history['train_accs'].append(train_acc)\n",
    "            training_history['val_accs'].append(val_acc)\n",
    "            training_history['train_losses'].append(train_loss)\n",
    "            training_history['val_losses'].append(val_loss)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "            \n",
    "            if epoch % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        return model, best_val_acc, training_history, architecture\n",
    "    \n",
    "    def train_single_fold(self, train_loader, val_loader, fold_idx):\n",
    "        best_model = None\n",
    "        best_acc = 0.0\n",
    "        best_history = None\n",
    "        best_arch = None\n",
    "        \n",
    "        for arch in AVAILABLE_ARCHITECTURES:\n",
    "            try:\n",
    "                print(f\"  Testing {arch}...\")\n",
    "                model, val_acc, history, architecture = self.train_single_model(train_loader, val_loader, arch)\n",
    "                \n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    best_model = model\n",
    "                    best_history = history\n",
    "                    best_arch = architecture\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Failed to train {arch}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return best_model, best_acc, best_history, best_arch\n",
    "    \n",
    "    def save_fold_immediately(self, model, fold_num, architecture, cv_score, label_mapping, history):\n",
    "        model_path = os.path.join(self.save_dir, f\"{architecture}_fold_{fold_num}_{cv_score:.1f}pct.pth\")\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_architecture': architecture,\n",
    "            'fold': fold_num,\n",
    "            'cv_score': cv_score,\n",
    "            'num_classes': self.num_classes,\n",
    "            'label_mapping': label_mapping,\n",
    "            'input_size': IMAGE_SIZE,\n",
    "            'training_history': history\n",
    "        }, model_path)\n",
    "        \n",
    "        print(f\"  Saved fold {fold_num} to: {model_path}\")\n",
    "        return model_path\n",
    "    \n",
    "    def train_ensemble_academic(self, X_train, y_train, label_mapping):\n",
    "        skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "        fold_splits = list(skf.split(X_train, y_train))\n",
    "        \n",
    "        with open(os.path.join(self.save_dir, \"label_mapping.json\"), 'w') as f:\n",
    "            json.dump(label_mapping, f, indent=2)\n",
    "        \n",
    "        with open(os.path.join(self.save_dir, \"fold_splits.pkl\"), 'wb') as f:\n",
    "            pickle.dump(fold_splits, f)\n",
    "        \n",
    "        trained_models = []\n",
    "        cv_scores = []\n",
    "        training_histories = []\n",
    "        architectures_used = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(fold_splits):\n",
    "            fold_num = fold_idx + 1\n",
    "            print(f\"Training Fold {fold_num}/{NUM_FOLDS}\")\n",
    "            \n",
    "            X_train_fold = X_train[train_idx]\n",
    "            y_train_fold = y_train[train_idx]\n",
    "            X_val_fold = X_train[val_idx]\n",
    "            y_val_fold = y_train[val_idx]\n",
    "            \n",
    "            X_train_aug, y_train_aug = create_augmented_data(X_train_fold, y_train_fold)\n",
    "            \n",
    "            train_dataset = OptimizedDataset(X_train_aug, y_train_aug)\n",
    "            val_dataset = OptimizedDataset(X_val_fold, y_val_fold, test_time_aug=True)\n",
    "            \n",
    "            batch_size = 16 if torch.cuda.is_available() else 8\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "            \n",
    "            model, val_acc, history, best_arch = self.train_single_fold(train_loader, val_loader, fold_num)\n",
    "            \n",
    "            if model is not None:\n",
    "                self.save_fold_immediately(model, fold_num, best_arch, val_acc, label_mapping, history)\n",
    "                \n",
    "                trained_models.append(model)\n",
    "                cv_scores.append(val_acc)\n",
    "                training_histories.append(history)\n",
    "                architectures_used.append(best_arch)\n",
    "                print(f\"Fold {fold_num}/{NUM_FOLDS} completed: {val_acc:.1f}% with {best_arch}\")\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return trained_models, cv_scores, training_histories, architectures_used\n",
    "    \n",
    "    def evaluate_ensemble(self, models, cv_scores, test_loader):\n",
    "        scores_array = np.array(cv_scores)\n",
    "        weights = np.exp(scores_array / 20)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        for model in models:\n",
    "            model.eval()\n",
    "        \n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        all_ensemble_probs = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                ensemble_outputs = torch.zeros(images.size(0), self.num_classes).to(self.device)\n",
    "                \n",
    "                for model, weight in zip(models, weights):\n",
    "                    outputs1 = model(images)\n",
    "                    flipped = torch.flip(images, [3])\n",
    "                    outputs2 = model(flipped)\n",
    "                    avg_outputs = (outputs1 + outputs2) / 2\n",
    "                    \n",
    "                    ensemble_outputs += weight * F.softmax(avg_outputs, dim=1)\n",
    "                \n",
    "                _, predicted = torch.max(ensemble_outputs, 1)\n",
    "                \n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_ensemble_probs.extend(ensemble_outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        ensemble_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        return ensemble_acc, np.array(all_ensemble_probs), np.array(all_labels)\n",
    "\n",
    "def save_final_ensemble(trainer, models, cv_scores, label_mapping, ensemble_acc, training_histories, architectures_used):\n",
    "    ensemble_path = os.path.join(trainer.save_dir, \"academic_ensemble.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dicts': [model.state_dict() for model in models],\n",
    "        'architectures_used': architectures_used,\n",
    "        'cv_scores': cv_scores,\n",
    "        'ensemble_score': ensemble_acc,\n",
    "        'num_classes': len(label_mapping),\n",
    "        'label_mapping': label_mapping,\n",
    "        'input_size': IMAGE_SIZE\n",
    "    }, ensemble_path)\n",
    "    \n",
    "    with open(os.path.join(trainer.save_dir, \"training_histories.pkl\"), 'wb') as f:\n",
    "        pickle.dump(training_histories, f)\n",
    "    \n",
    "    metadata = {\n",
    "        'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "        'architectures_used': architectures_used,\n",
    "        'num_folds': len(models),\n",
    "        'cv_scores': cv_scores,\n",
    "        'cv_mean': float(np.mean(cv_scores)),\n",
    "        'cv_std': float(np.std(cv_scores)),\n",
    "        'ensemble_score': ensemble_acc,\n",
    "        'label_mapping': label_mapping,\n",
    "        'input_size': f'{IMAGE_SIZE[1]}x{IMAGE_SIZE[0]}',\n",
    "        'augmentation_target': AUGMENTATION_TARGET,\n",
    "        'completed': True,\n",
    "        'academic_split': True\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(trainer.save_dir, \"metadata.json\"), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Final ensemble saved to: {ensemble_path}\")\n",
    "    return trainer.save_dir\n",
    "\n",
    "def plot_training_curves(training_histories, architectures_used, save_dir):\n",
    "    Path(os.path.join(save_dir, \"training_plots\")).mkdir(exist_ok=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, NUM_FOLDS, figsize=(4*NUM_FOLDS, 4))\n",
    "    if NUM_FOLDS == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for fold, (history, arch) in enumerate(zip(training_histories, architectures_used)):\n",
    "        ax = axes[fold]\n",
    "        epochs = range(1, len(history['train_accs']) + 1)\n",
    "        \n",
    "        ax.plot(epochs, history['train_accs'], 'b-', label='Training', linewidth=2, alpha=0.8)\n",
    "        ax.plot(epochs, history['val_accs'], 'r-', label='Validation', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title(f'Fold {fold + 1} ({arch})')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"training_plots\", \"training_curves.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, NUM_FOLDS, figsize=(4*NUM_FOLDS, 4))\n",
    "    if NUM_FOLDS == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for fold, (history, arch) in enumerate(zip(training_histories, architectures_used)):\n",
    "        ax = axes[fold]\n",
    "        epochs = range(1, len(history['train_losses']) + 1)\n",
    "        \n",
    "        ax.plot(epochs, history['train_losses'], 'b-', label='Training Loss', linewidth=2, alpha=0.8)\n",
    "        ax.plot(epochs, history['val_losses'], 'r-', label='Validation Loss', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title(f'Fold {fold + 1} Loss ({arch})')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"training_plots\", \"loss_curves.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    print(\"Academically Rigorous Multi-Architecture Ensemble Training\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Available architectures: {AVAILABLE_ARCHITECTURES}\")\n",
    "    print(f\"Number of folds: {NUM_FOLDS}\")\n",
    "    print(f\"Image size: {IMAGE_SIZE[1]}x{IMAGE_SIZE[0]}\")\n",
    "    print(f\"Augmentation target: {AUGMENTATION_TARGET} samples per class\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        images, ages = load_original_data()\n",
    "        \n",
    "        unique_ages = sorted(list(set(ages)))\n",
    "        label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "        y_indices = np.array([label_mapping[age] for age in ages])\n",
    "        \n",
    "        print(f\"Total images: {len(images)}\")\n",
    "        print(f\"Age distribution: {dict(Counter(ages))}\")\n",
    "        \n",
    "        print(\"\\n--- ACADEMIC TRAIN/TEST SPLIT ---\")\n",
    "        X_train_all, X_test_final, y_train_all, y_test_final = train_test_split(\n",
    "            images, y_indices, test_size=0.2, random_state=42, stratify=y_indices\n",
    "        )\n",
    "        \n",
    "        print(f\"Training data: {len(X_train_all)} images\")\n",
    "        print(f\"Test data: {len(X_test_final)} images\")\n",
    "        print(\"Test set will NOT be used until final evaluation\")\n",
    "        \n",
    "        trainer = MultiArchEnsembleTrainer(num_classes=len(unique_ages))\n",
    "        \n",
    "        print(\"\\n--- CROSS-VALIDATION ON TRAINING DATA ONLY ---\")\n",
    "        models, cv_scores, training_histories, architectures_used = trainer.train_ensemble_academic(\n",
    "            X_train_all, y_train_all, label_mapping\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- FINAL EVALUATION ON HELD-OUT TEST SET ---\")\n",
    "        test_dataset = OptimizedDataset(X_test_final, y_test_final, test_time_aug=True)\n",
    "        batch_size = 32 if torch.cuda.is_available() else 8\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        \n",
    "        ensemble_acc, _, _ = trainer.evaluate_ensemble(models, cv_scores, test_loader)\n",
    "        \n",
    "        print(\"Saving final ensemble...\")\n",
    "        save_dir = save_final_ensemble(trainer, models, cv_scores, label_mapping, ensemble_acc, training_histories, architectures_used)\n",
    "        \n",
    "        print(\"Generating training curves...\")\n",
    "        plot_training_curves(training_histories, architectures_used, save_dir)\n",
    "        \n",
    "        elapsed = (time.time() - start_time) / 60\n",
    "        \n",
    "        print(\"\\n=== ACADEMIC RESULTS ===\")\n",
    "        print(\"Cross-Validation Results (Training Data Only):\")\n",
    "        for i, (score, arch) in enumerate(zip(cv_scores, architectures_used)):\n",
    "            print(f\"Fold {i+1}: {score:.1f}% ({arch})\")\n",
    "        print(f\"CV Mean: {np.mean(cv_scores):.1f}% ± {np.std(cv_scores):.1f}%\")\n",
    "        print(f\"\\nFinal Test Accuracy (Held-Out Data): {ensemble_acc:.1f}%\")\n",
    "        print(f\"Training Time: {elapsed:.1f} minutes\")\n",
    "        print(f\"All results saved to: {save_dir}\")\n",
    "        \n",
    "        print(\"\\n=== OVERFITTING ANALYSIS ===\")\n",
    "        print(\"Check training_plots/ for loss curves to verify no overfitting\")\n",
    "        \n",
    "        return {\n",
    "            'models': models,\n",
    "            'cv_scores': cv_scores,\n",
    "            'test_accuracy': ensemble_acc,\n",
    "            'architectures_used': architectures_used,\n",
    "            'save_directory': save_dir\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da3401d-e17c-47d7-9cc1-6690b9bef01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ensemble with 5 models\n",
      "Architectures: ['efficientnet_b2', 'efficientnet_b0', 'efficientnet_b0', 'efficientnet_b2', 'efficientnet_b0']\n",
      "CV Scores: ['92.3%', '87.2%', '94.9%', '89.7%', '89.5%']\n",
      "============================================================\n",
      "COMPREHENSIVE ENSEMBLE MODEL EVALUATION\n",
      "============================================================\n",
      "\n",
      "1. Loading test data...\n",
      "Loaded 49 test images\n",
      "Classes: ['0.5', '1.5', '2.5', '3.5', '4.5', '5.5']\n",
      "\n",
      "2. Computing ensemble predictions...\n",
      "Computing ensemble predictions...\n",
      "Processing image 1/49\n",
      "Ensemble predictions complete!\n",
      "\n",
      "3. Generating confusion matrices...\n",
      "Confusion matrices saved!\n",
      "\n",
      "4. Computing classification metrics...\n",
      "Classification metrics saved!\n",
      "\n",
      "5. Creating class distribution plots...\n",
      "Class distribution plots saved!\n",
      "\n",
      "6. Generating attention heatmaps...\n",
      "Attention heatmaps saved!\n",
      "\n",
      "7. Analyzing ensemble performance...\n",
      "Ensemble analysis saved!\n",
      "\n",
      "8. Performing error analysis...\n",
      "Error analysis saved!\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE!\n",
      "============================================================\n",
      "Test Accuracy: 77.6%\n",
      "F1-Score (Macro): 0.760\n",
      "Results saved to: evaluation_results\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, \n",
    "    precision_score, recall_score, accuracy_score,\n",
    "    precision_recall_curve, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for publication-quality plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class ComprehensiveEvaluator:\n",
    "    def __init__(self, ensemble_path, test_data_path=None, device=None):\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.ensemble_path = ensemble_path\n",
    "        self.test_data_path = test_data_path\n",
    "        \n",
    "        # Load ensemble\n",
    "        self.ensemble_data = torch.load(ensemble_path, map_location=self.device)\n",
    "        self.models = self._load_ensemble_models()\n",
    "        self.label_mapping = self.ensemble_data['label_mapping']\n",
    "        self.cv_scores = self.ensemble_data['cv_scores']\n",
    "        self.architectures = self.ensemble_data['architectures_used']\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir = Path(\"evaluation_results\")\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"Loaded ensemble with {len(self.models)} models\")\n",
    "        print(f\"Architectures: {self.architectures}\")\n",
    "        print(f\"CV Scores: {[f'{score:.1f}%' for score in self.cv_scores]}\")\n",
    "    \n",
    "    def _load_ensemble_models(self):\n",
    "        models = []\n",
    "        state_dicts = self.ensemble_data['model_state_dicts']\n",
    "        architectures = self.ensemble_data['architectures_used']\n",
    "        num_classes = self.ensemble_data['num_classes']\n",
    "        \n",
    "        for state_dict, arch in zip(state_dicts, architectures):\n",
    "            model = timm.create_model(arch, pretrained=False, num_classes=num_classes)\n",
    "            \n",
    "            # Recreate the same classifier structure used during training\n",
    "            if hasattr(model, 'fc'):\n",
    "                in_features = model.fc.in_features\n",
    "                model.fc = nn.Sequential(\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(in_features, num_classes)\n",
    "                )\n",
    "            elif hasattr(model, 'classifier'):\n",
    "                if hasattr(model.classifier, 'in_features'):\n",
    "                    in_features = model.classifier.in_features\n",
    "                    model.classifier = nn.Sequential(\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.Linear(in_features, num_classes)\n",
    "                    )\n",
    "                else:\n",
    "                    in_features = model.classifier[-1].in_features\n",
    "                    model.classifier[-1] = nn.Sequential(\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.Linear(in_features, num_classes)\n",
    "                    )\n",
    "            \n",
    "            model.load_state_dict(state_dict)\n",
    "            model.to(self.device)\n",
    "            model.eval()\n",
    "            models.append(model)\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def load_test_data(self, data_path=None):\n",
    "        \"\"\"Load test data - modify this based on your data format\"\"\"\n",
    "        if data_path:\n",
    "            # Load from specific path\n",
    "            pass\n",
    "        else:\n",
    "            # Load the original data and get test split\n",
    "            fpath = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\tooth_analysis\\\\images\\\\*.png\"\n",
    "            image_paths = glob.glob(fpath)\n",
    "            \n",
    "            images = []\n",
    "            ages = []\n",
    "            image_names = []\n",
    "            \n",
    "            for img_path in image_paths:\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    \n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img_resized = cv2.resize(img, (448, 224))  # W, H format for cv2\n",
    "                    \n",
    "                    filename = os.path.basename(img_path)\n",
    "                    filename_no_ext = os.path.splitext(filename)[0]\n",
    "                    parts = filename_no_ext.split('_')\n",
    "                    \n",
    "                    if len(parts) < 3:\n",
    "                        continue\n",
    "                    \n",
    "                    bbb_part = parts[1]\n",
    "                    if 'p' not in bbb_part:\n",
    "                        continue\n",
    "                    \n",
    "                    value_str = bbb_part.replace('p', '.')\n",
    "                    age_value = float(value_str)\n",
    "                    age_value = 5.5 if age_value >= 5.5 else age_value\n",
    "                    \n",
    "                    images.append(img_resized)\n",
    "                    ages.append(age_value)\n",
    "                    image_names.append(filename)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Convert to test set (same split as training)\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            unique_ages = sorted(list(set(ages)))\n",
    "            age_to_idx = {age: i for i, age in enumerate(unique_ages)}\n",
    "            y_indices = [age_to_idx[age] for age in ages]\n",
    "            \n",
    "            _, X_test, _, y_test, _, names_test = train_test_split(\n",
    "                images, y_indices, image_names, test_size=0.2, random_state=42, stratify=y_indices\n",
    "            )\n",
    "            \n",
    "            self.X_test = np.array(X_test)\n",
    "            self.y_test = np.array(y_test)\n",
    "            self.test_names = names_test\n",
    "            self.class_names = [str(age) for age in unique_ages]\n",
    "            \n",
    "            print(f\"Loaded {len(self.X_test)} test images\")\n",
    "            print(f\"Classes: {self.class_names}\")\n",
    "    \n",
    "    def preprocess_image_for_model(self, image):\n",
    "        \"\"\"Preprocess single image for model input\"\"\"\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        if len(image.shape) == 3:\n",
    "            image = torch.FloatTensor(image).permute(2, 0, 1)\n",
    "        \n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        image = (image - mean) / std\n",
    "        \n",
    "        return image.unsqueeze(0).to(self.device)\n",
    "    \n",
    "    def get_ensemble_predictions(self, return_probs=True):\n",
    "        \"\"\"Get ensemble predictions for all test data\"\"\"\n",
    "        all_probs = []\n",
    "        all_preds = []\n",
    "        \n",
    "        # Calculate weights based on CV scores\n",
    "        scores_array = np.array(self.cv_scores)\n",
    "        weights = np.exp(scores_array / 20)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        print(\"Computing ensemble predictions...\")\n",
    "        \n",
    "        for i, image in enumerate(self.X_test):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Processing image {i+1}/{len(self.X_test)}\")\n",
    "            \n",
    "            # Preprocess image\n",
    "            img_tensor = self.preprocess_image_for_model(image)\n",
    "            \n",
    "            # Get predictions from each model\n",
    "            ensemble_output = torch.zeros(1, len(self.class_names)).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for model, weight in zip(self.models, weights):\n",
    "                    # Original prediction\n",
    "                    output1 = model(img_tensor)\n",
    "                    # Flipped prediction (TTA)\n",
    "                    flipped = torch.flip(img_tensor, [3])\n",
    "                    output2 = model(flipped)\n",
    "                    # Average and weight\n",
    "                    avg_output = (output1 + output2) / 2\n",
    "                    ensemble_output += weight * F.softmax(avg_output, dim=1)\n",
    "            \n",
    "            probs = ensemble_output.cpu().numpy()[0]\n",
    "            pred = np.argmax(probs)\n",
    "            \n",
    "            all_probs.append(probs)\n",
    "            all_preds.append(pred)\n",
    "        \n",
    "        self.y_pred = np.array(all_preds)\n",
    "        self.y_probs = np.array(all_probs)\n",
    "        \n",
    "        print(\"Ensemble predictions complete!\")\n",
    "        return self.y_pred, self.y_probs if return_probs else self.y_pred\n",
    "    \n",
    "    def generate_confusion_matrix(self):\n",
    "        \"\"\"Generate and save confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(self.y_test, self.y_pred)\n",
    "        \n",
    "        # Calculate percentages\n",
    "        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Raw counts\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=self.class_names, yticklabels=self.class_names, ax=ax1)\n",
    "        ax1.set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Predicted Age', fontsize=12)\n",
    "        ax1.set_ylabel('True Age', fontsize=12)\n",
    "        \n",
    "        # Percentages\n",
    "        sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Oranges',\n",
    "                   xticklabels=self.class_names, yticklabels=self.class_names, ax=ax2)\n",
    "        ax2.set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Predicted Age', fontsize=12)\n",
    "        ax2.set_ylabel('True Age', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Confusion matrices saved!\")\n",
    "    \n",
    "    def generate_classification_metrics(self):\n",
    "        \"\"\"Generate comprehensive classification metrics\"\"\"\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(self.y_test, self.y_pred)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        f1_macro = f1_score(self.y_test, self.y_pred, average='macro')\n",
    "        f1_weighted = f1_score(self.y_test, self.y_pred, average='weighted')\n",
    "        f1_per_class = f1_score(self.y_test, self.y_pred, average=None)\n",
    "        \n",
    "        precision_macro = precision_score(self.y_test, self.y_pred, average='macro')\n",
    "        precision_per_class = precision_score(self.y_test, self.y_pred, average=None)\n",
    "        \n",
    "        recall_macro = recall_score(self.y_test, self.y_pred, average='macro')\n",
    "        recall_per_class = recall_score(self.y_test, self.y_pred, average=None)\n",
    "        \n",
    "        # Create metrics DataFrame\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'Class': self.class_names,\n",
    "            'F1-Score': f1_per_class,\n",
    "            'Precision': precision_per_class,\n",
    "            'Recall': recall_per_class\n",
    "        })\n",
    "        \n",
    "        # Create bar plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # F1 Scores\n",
    "        bars1 = axes[0,0].bar(self.class_names, f1_per_class, color='skyblue', alpha=0.8)\n",
    "        axes[0,0].axhline(y=f1_macro, color='red', linestyle='--', label=f'Macro Avg: {f1_macro:.3f}')\n",
    "        axes[0,0].set_title('F1-Score per Class', fontsize=14, fontweight='bold')\n",
    "        axes[0,0].set_ylabel('F1-Score', fontsize=12)\n",
    "        axes[0,0].set_ylim(0, 1)\n",
    "        axes[0,0].legend()\n",
    "        # Add value labels on bars\n",
    "        for bar, val in zip(bars1, f1_per_class):\n",
    "            axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                          f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Precision\n",
    "        bars2 = axes[0,1].bar(self.class_names, precision_per_class, color='lightgreen', alpha=0.8)\n",
    "        axes[0,1].axhline(y=precision_macro, color='red', linestyle='--', label=f'Macro Avg: {precision_macro:.3f}')\n",
    "        axes[0,1].set_title('Precision per Class', fontsize=14, fontweight='bold')\n",
    "        axes[0,1].set_ylabel('Precision', fontsize=12)\n",
    "        axes[0,1].set_ylim(0, 1)\n",
    "        axes[0,1].legend()\n",
    "        for bar, val in zip(bars2, precision_per_class):\n",
    "            axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                          f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Recall\n",
    "        bars3 = axes[1,0].bar(self.class_names, recall_per_class, color='lightcoral', alpha=0.8)\n",
    "        axes[1,0].axhline(y=recall_macro, color='red', linestyle='--', label=f'Macro Avg: {recall_macro:.3f}')\n",
    "        axes[1,0].set_title('Recall per Class', fontsize=14, fontweight='bold')\n",
    "        axes[1,0].set_ylabel('Recall', fontsize=12)\n",
    "        axes[1,0].set_xlabel('Age Class', fontsize=12)\n",
    "        axes[1,0].set_ylim(0, 1)\n",
    "        axes[1,0].legend()\n",
    "        for bar, val in zip(bars3, recall_per_class):\n",
    "            axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                          f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Overall metrics summary\n",
    "        axes[1,1].axis('off')\n",
    "        summary_text = f\"\"\"\n",
    "        Overall Performance Summary:\n",
    "        \n",
    "        Accuracy: {accuracy:.3f}\n",
    "        \n",
    "        F1-Score (Macro): {f1_macro:.3f}\n",
    "        F1-Score (Weighted): {f1_weighted:.3f}\n",
    "        \n",
    "        Precision (Macro): {precision_macro:.3f}\n",
    "        Recall (Macro): {recall_macro:.3f}\n",
    "        \n",
    "        Best F1 Class: {self.class_names[np.argmax(f1_per_class)]} ({f1_per_class.max():.3f})\n",
    "        Worst F1 Class: {self.class_names[np.argmin(f1_per_class)]} ({f1_per_class.min():.3f})\n",
    "        \"\"\"\n",
    "        axes[1,1].text(0.1, 0.9, summary_text, transform=axes[1,1].transAxes, \n",
    "                      fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "                      bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'classification_metrics.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save metrics to CSV\n",
    "        metrics_df.to_csv(self.output_dir / 'per_class_metrics.csv', index=False)\n",
    "        \n",
    "        print(\"Classification metrics saved!\")\n",
    "        return metrics_df\n",
    "    \n",
    "    def generate_class_distribution_plots(self):\n",
    "        \"\"\"Generate age class distribution plots\"\"\"\n",
    "        # Count distributions\n",
    "        true_counts = Counter(self.y_test)\n",
    "        pred_counts = Counter(self.y_pred)\n",
    "        \n",
    "        # Convert to lists for plotting\n",
    "        true_dist = [true_counts.get(i, 0) for i in range(len(self.class_names))]\n",
    "        pred_dist = [pred_counts.get(i, 0) for i in range(len(self.class_names))]\n",
    "        \n",
    "        # Create comparison plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        x = np.arange(len(self.class_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Side by side bars\n",
    "        bars1 = ax1.bar(x - width/2, true_dist, width, label='True Distribution', \n",
    "                       color='steelblue', alpha=0.8)\n",
    "        bars2 = ax1.bar(x + width/2, pred_dist, width, label='Predicted Distribution', \n",
    "                       color='lightcoral', alpha=0.8)\n",
    "        \n",
    "        ax1.set_xlabel('Age Class', fontsize=12)\n",
    "        ax1.set_ylabel('Number of Samples', fontsize=12)\n",
    "        ax1.set_title('True vs Predicted Class Distributions', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(self.class_names)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                        f'{int(height)}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Pie chart for true distribution\n",
    "        ax2.pie(true_dist, labels=self.class_names, autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title('True Age Distribution', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'class_distributions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Class distribution plots saved!\")\n",
    "    \n",
    "    def generate_grad_cam_heatmaps(self, num_samples=9):\n",
    "        \"\"\"Generate Grad-CAM attention heatmaps for sample images (original + heatmap overlay)\"\"\"\n",
    "        \n",
    "        class GradCAM:\n",
    "            def __init__(self, model):\n",
    "                self.model = model\n",
    "                self.target_layer = None\n",
    "                self.gradients = None\n",
    "                self.activations = None\n",
    "                \n",
    "                # Find the last convolutional layer\n",
    "                for name, module in self.model.named_modules():\n",
    "                    if isinstance(module, nn.Conv2d):\n",
    "                        self.target_layer = module\n",
    "                \n",
    "                self.target_layer.register_forward_hook(self.save_activation)\n",
    "                self.target_layer.register_backward_hook(self.save_gradient)\n",
    "            \n",
    "            def save_activation(self, module, input, output):\n",
    "                self.activations = output\n",
    "            \n",
    "            def save_gradient(self, module, grad_input, grad_output):\n",
    "                self.gradients = grad_output[0]\n",
    "            \n",
    "            def generate_cam(self, input_tensor, class_idx):\n",
    "                # Forward pass\n",
    "                output = self.model(input_tensor)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.model.zero_grad()\n",
    "                output[0, class_idx].backward(retain_graph=True)\n",
    "                \n",
    "                # Generate CAM\n",
    "                gradients = self.gradients.cpu().data.numpy()[0]\n",
    "                activations = self.activations.cpu().data.numpy()[0]\n",
    "                \n",
    "                weights = np.mean(gradients, axis=(1, 2))\n",
    "                cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "                \n",
    "                for i, w in enumerate(weights):\n",
    "                    cam += w * activations[i]\n",
    "                \n",
    "                cam = np.maximum(cam, 0)\n",
    "                cam = cam / cam.max() if cam.max() > 0 else cam\n",
    "                cam = cv2.resize(cam, (448, 224))\n",
    "                \n",
    "                return cam\n",
    "        \n",
    "        # Select diverse samples (one from each class if possible)\n",
    "        selected_indices = []\n",
    "        for class_idx in range(len(self.class_names)):\n",
    "            class_indices = np.where(self.y_test == class_idx)[0]\n",
    "            if len(class_indices) > 0:\n",
    "                # Select the sample with highest confidence for this class\n",
    "                class_probs = self.y_probs[class_indices, class_idx]\n",
    "                best_idx = class_indices[np.argmax(class_probs)]\n",
    "                selected_indices.append(best_idx)\n",
    "        \n",
    "        # Fill up to num_samples with random samples if needed\n",
    "        while len(selected_indices) < num_samples and len(selected_indices) < len(self.X_test):\n",
    "            remaining_indices = set(range(len(self.X_test))) - set(selected_indices)\n",
    "            if remaining_indices:\n",
    "                selected_indices.append(np.random.choice(list(remaining_indices)))\n",
    "        \n",
    "        selected_indices = selected_indices[:num_samples]\n",
    "        \n",
    "        # Create the visualization\n",
    "        fig = plt.figure(figsize=(12, 4 * len(selected_indices)))\n",
    "        \n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            image = self.X_test[idx]\n",
    "            true_class = self.y_test[idx]\n",
    "            pred_class = self.y_pred[idx]\n",
    "            probs = self.y_probs[idx]\n",
    "            \n",
    "            # Get Grad-CAM from the best model (highest CV score)\n",
    "            best_model_idx = np.argmax(self.cv_scores)\n",
    "            best_model = self.models[best_model_idx]\n",
    "            \n",
    "            grad_cam = GradCAM(best_model)\n",
    "            img_tensor = self.preprocess_image_for_model(image)\n",
    "            \n",
    "            # Generate heatmap for predicted class\n",
    "            heatmap = grad_cam.generate_cam(img_tensor, pred_class)\n",
    "            \n",
    "            # Create subplot layout: original | heatmap (tight spacing)\n",
    "            gs = plt.GridSpec(1, 2, figure=fig, width_ratios=[1, 1],\n",
    "                            top=1-i/len(selected_indices), bottom=1-(i+1)/len(selected_indices),\n",
    "                            left=0.02, right=0.98, wspace=0.02, hspace=0.02)\n",
    "            \n",
    "            # Original image\n",
    "            ax1 = fig.add_subplot(gs[0])\n",
    "            ax1.imshow(image)\n",
    "            ax1.set_title(f'Original Image\\nTrue: {self.class_names[true_class]} | Pred: {self.class_names[pred_class]}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax1.axis('off')\n",
    "            \n",
    "            # Heatmap overlay\n",
    "            ax2 = fig.add_subplot(gs[1])\n",
    "            ax2.imshow(image)\n",
    "            heatmap_colored = plt.cm.jet(heatmap)[:, :, :3]\n",
    "            ax2.imshow(heatmap_colored, alpha=0.6)\n",
    "            ax2.set_title(f'Attention Heatmap\\n(Focus: {self.class_names[pred_class]})', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax2.axis('off')\n",
    "        \n",
    "        plt.savefig(self.output_dir / 'attention_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Attention heatmaps saved!\")\n",
    "    \n",
    "    def generate_ensemble_analysis(self):\n",
    "        \"\"\"Analyze ensemble component performance\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # CV Scores comparison\n",
    "        bars1 = axes[0,0].bar(range(len(self.architectures)), self.cv_scores, \n",
    "                             color=['skyblue', 'lightgreen', 'lightcoral', 'orange', 'purple'][:len(self.architectures)])\n",
    "        axes[0,0].set_title('Cross-Validation Scores by Architecture', fontsize=14, fontweight='bold')\n",
    "        axes[0,0].set_ylabel('CV Score (%)', fontsize=12)\n",
    "        axes[0,0].set_xticks(range(len(self.architectures)))\n",
    "        axes[0,0].set_xticklabels(self.architectures, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars1, self.cv_scores):\n",
    "            axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                          f'{score:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Ensemble weights\n",
    "        scores_array = np.array(self.cv_scores)\n",
    "        weights = np.exp(scores_array / 20)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        bars2 = axes[0,1].bar(range(len(self.architectures)), weights, \n",
    "                             color=['skyblue', 'lightgreen', 'lightcoral', 'orange', 'purple'][:len(self.architectures)])\n",
    "        axes[0,1].set_title('Ensemble Weights', fontsize=14, fontweight='bold')\n",
    "        axes[0,1].set_ylabel('Weight', fontsize=12)\n",
    "        axes[0,1].set_xticks(range(len(self.architectures)))\n",
    "        axes[0,1].set_xticklabels(self.architectures, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, weight in zip(bars2, weights):\n",
    "            axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                          f'{weight:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Model architecture distribution\n",
    "        arch_counts = Counter(self.architectures)\n",
    "        axes[1,0].pie(arch_counts.values(), labels=arch_counts.keys(), autopct='%1.0f%%', startangle=90)\n",
    "        axes[1,0].set_title('Architecture Distribution in Ensemble', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Performance summary\n",
    "        axes[1,1].axis('off')\n",
    "        ensemble_accuracy = accuracy_score(self.y_test, self.y_pred)\n",
    "        summary_text = f\"\"\"\n",
    "        Ensemble Performance Summary:\n",
    "        \n",
    "        Number of Models: {len(self.models)}\n",
    "        Architectures Used: {len(set(self.architectures))}\n",
    "        \n",
    "        Best Individual CV: {max(self.cv_scores):.1f}%\n",
    "        Worst Individual CV: {min(self.cv_scores):.1f}%\n",
    "        Mean CV Score: {np.mean(self.cv_scores):.1f}%\n",
    "        \n",
    "        Final Test Accuracy: {ensemble_accuracy:.1f}%\n",
    "        \n",
    "        Ensemble Improvement: {ensemble_accuracy*100 - np.mean(self.cv_scores):.1f} percentage points\n",
    "        \"\"\"\n",
    "        axes[1,1].text(0.1, 0.9, summary_text, transform=axes[1,1].transAxes, \n",
    "                      fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "                      bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'ensemble_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Ensemble analysis saved!\")\n",
    "    \n",
    "    def generate_error_analysis(self):\n",
    "        \"\"\"Analyze prediction errors\"\"\"\n",
    "        # Find misclassified samples\n",
    "        errors = self.y_test != self.y_pred\n",
    "        error_indices = np.where(errors)[0]\n",
    "        \n",
    "        if len(error_indices) == 0:\n",
    "            print(\"No errors found!\")\n",
    "            return\n",
    "        \n",
    "        # Error types\n",
    "        error_types = []\n",
    "        for idx in error_indices:\n",
    "            true_age = float(self.class_names[self.y_test[idx]])\n",
    "            pred_age = float(self.class_names[self.y_pred[idx]])\n",
    "            error_types.append(abs(true_age - pred_age))\n",
    "        \n",
    "        # Create error analysis plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Error magnitude distribution\n",
    "        axes[0,0].hist(error_types, bins=10, color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "        axes[0,0].set_title('Error Magnitude Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0,0].set_xlabel('Age Difference (|True - Predicted|)', fontsize=12)\n",
    "        axes[0,0].set_ylabel('Frequency', fontsize=12)\n",
    "        \n",
    "        # Error by true class\n",
    "        error_by_class = {class_name: [] for class_name in self.class_names}\n",
    "        for idx in error_indices:\n",
    "            true_class_name = self.class_names[self.y_test[idx]]\n",
    "            pred_class_name = self.class_names[self.y_pred[idx]]\n",
    "            error_by_class[true_class_name].append(pred_class_name)\n",
    "        \n",
    "        error_rates = [len(errors) / list(self.y_test).count(i) * 100 \n",
    "                      for i, errors in enumerate([error_by_class[name] for name in self.class_names])]\n",
    "        \n",
    "        bars = axes[0,1].bar(self.class_names, error_rates, color='orange', alpha=0.7)\n",
    "        axes[0,1].set_title('Error Rate by True Class', fontsize=14, fontweight='bold')\n",
    "        axes[0,1].set_ylabel('Error Rate (%)', fontsize=12)\n",
    "        axes[0,1].set_xlabel('True Age Class', fontsize=12)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, rate in zip(bars, error_rates):\n",
    "            axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                          f'{rate:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Confidence vs Accuracy\n",
    "        confidences = np.max(self.y_probs, axis=1)\n",
    "        correct = self.y_test == self.y_pred\n",
    "        \n",
    "        axes[1,0].scatter(confidences[correct], [1]*sum(correct), alpha=0.5, color='green', label='Correct')\n",
    "        axes[1,0].scatter(confidences[~correct], [0]*sum(~correct), alpha=0.5, color='red', label='Incorrect')\n",
    "        axes[1,0].set_title('Prediction Confidence vs Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[1,0].set_xlabel('Max Probability', fontsize=12)\n",
    "        axes[1,0].set_ylabel('Correct (1) / Incorrect (0)', fontsize=12)\n",
    "        axes[1,0].legend()\n",
    "        \n",
    "        # Summary statistics\n",
    "        axes[1,1].axis('off')\n",
    "        total_errors = len(error_indices)\n",
    "        avg_error = np.mean(error_types) if error_types else 0\n",
    "        max_error = max(error_types) if error_types else 0\n",
    "        avg_confidence_correct = np.mean(confidences[correct]) if sum(correct) > 0 else 0\n",
    "        avg_confidence_wrong = np.mean(confidences[~correct]) if sum(~correct) > 0 else 0\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "        Error Analysis Summary:\n",
    "        \n",
    "        Total Errors: {total_errors} / {len(self.y_test)} ({total_errors/len(self.y_test)*100:.1f}%)\n",
    "        \n",
    "        Average Error Magnitude: {avg_error:.2f} years\n",
    "        Maximum Error: {max_error:.1f} years\n",
    "        \n",
    "        Avg Confidence (Correct): {avg_confidence_correct:.3f}\n",
    "        Avg Confidence (Wrong): {avg_confidence_wrong:.3f}\n",
    "        \n",
    "        Most Confused Classes:\n",
    "        {self.class_names[np.argmax(error_rates)]} ({max(error_rates):.1f}% error rate)\n",
    "        \"\"\"\n",
    "        axes[1,1].text(0.1, 0.9, summary_text, transform=axes[1,1].transAxes, \n",
    "                      fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "                      bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"mistyrose\", alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Error analysis saved!\")\n",
    "    \n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate a comprehensive evaluation report\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"COMPREHENSIVE ENSEMBLE MODEL EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Load test data\n",
    "        print(\"\\n1. Loading test data...\")\n",
    "        self.load_test_data()\n",
    "        \n",
    "        # Get predictions\n",
    "        print(\"\\n2. Computing ensemble predictions...\")\n",
    "        self.get_ensemble_predictions()\n",
    "        \n",
    "        # Generate all visualizations\n",
    "        print(\"\\n3. Generating confusion matrices...\")\n",
    "        self.generate_confusion_matrix()\n",
    "        \n",
    "        print(\"\\n4. Computing classification metrics...\")\n",
    "        metrics_df = self.generate_classification_metrics()\n",
    "        \n",
    "        print(\"\\n5. Creating class distribution plots...\")\n",
    "        self.generate_class_distribution_plots()\n",
    "        \n",
    "        print(\"\\n6. Generating attention heatmaps...\")\n",
    "        self.generate_grad_cam_heatmaps()\n",
    "        \n",
    "        print(\"\\n7. Analyzing ensemble performance...\")\n",
    "        self.generate_ensemble_analysis()\n",
    "        \n",
    "        print(\"\\n8. Performing error analysis...\")\n",
    "        self.generate_error_analysis()\n",
    "        \n",
    "        # Save final summary\n",
    "        accuracy = accuracy_score(self.y_test, self.y_pred)\n",
    "        f1_macro = f1_score(self.y_test, self.y_pred, average='macro')\n",
    "        \n",
    "        summary = {\n",
    "            'test_accuracy': float(accuracy),\n",
    "            'f1_macro': float(f1_macro),\n",
    "            'cv_scores': self.cv_scores,\n",
    "            'architectures': self.architectures,\n",
    "            'num_test_samples': len(self.y_test),\n",
    "            'num_classes': len(self.class_names),\n",
    "            'class_names': self.class_names\n",
    "        }\n",
    "        \n",
    "        with open(self.output_dir / 'evaluation_summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"EVALUATION COMPLETE!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Test Accuracy: {accuracy:.1%}\")\n",
    "        print(f\"F1-Score (Macro): {f1_macro:.3f}\")\n",
    "        print(f\"Results saved to: {self.output_dir}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize evaluator\n",
    "    ensemble_path = \"./academic_ensemble_20250721_081845 (90.7 +- 2.6%)/academic_ensemble.pth\"  # Update this path\n",
    "    evaluator = ComprehensiveEvaluator(ensemble_path)\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    evaluator.generate_comprehensive_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca41c99-14d3-4a7a-b9e6-c8a193257a5d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 194 training images, 49 test images\n",
      "Classes: ['0.5', '1.5', '2.5', '3.5', '4.5', '5.5']\n",
      "OVERFITTING DIAGNOSTIC ANALYSIS\n",
      "==================================================\n",
      "\n",
      "=== DATA LEAKAGE ANALYSIS ===\n",
      "Filename overlap between train/test: 0\n",
      "Potential patient leaks (same base filename): 1760\n",
      "Examples:\n",
      "  Test: cpj_1p5_001.png <-> Train: cpj_1p5_000.png\n",
      "  Test: cpj_1p5_001.png <-> Train: cpj_4p5_000.png\n",
      "  Test: cpj_1p5_001.png <-> Train: cpj_7p5_000.png\n",
      "  Test: cpj_1p5_001.png <-> Train: cpj_2p5_000.png\n",
      "  Test: cpj_1p5_001.png <-> Train: cpj_0p5_000.png\n",
      "\n",
      "Filename pattern analysis:\n",
      "Train filename examples: ['vfl_6p5_004.png', 'vfl_1p5_011.png', 'qdma_3p5_012.png', 'vfl_1p5_000.png', 'cpj_1p5_000.png']\n",
      "Test filename examples: ['cpj_1p5_001.png', 'jbn_1p5_001.png', 'vfl_1p5_025.png', 'jbn_1p5_006.png', 'nda_0p5_001.png']\n",
      "\n",
      "=== CLASS DISTRIBUTION ANALYSIS ===\n",
      "  Class  Train_Count  Test_Count  Train_Pct   Test_Pct  Pct_Diff\n",
      "0   0.5           31           8  15.979381  16.326531  0.347149\n",
      "1   1.5           50          12  25.773196  24.489796  1.283400\n",
      "2   2.5           26           7  13.402062  14.285714  0.883652\n",
      "3   3.5           23           6  11.855670  12.244898  0.389228\n",
      "4   4.5           16           4   8.247423   8.163265  0.084157\n",
      "5   5.5           48          12  24.742268  24.489796  0.252472\n",
      "Maximum distribution difference: 1.3 percentage points\n",
      "\n",
      "=== DATASET SIZE ANALYSIS ===\n",
      "Total dataset size: 243\n",
      "Average samples per class: 40.5\n",
      "Test samples per class: 8.2\n",
      "Test set size: 49 (20.2% of total)\n",
      "Test set class range: 4 to 12 samples\n",
      "⚠️  WARNING: Some classes have very few test samples (<10)\n",
      "⚠️  WARNING: Very small test set - high variance expected\n",
      "\n",
      "=== INDIVIDUAL FOLD ANALYSIS ===\n",
      "Found 5 fold models\n",
      "Fold 2 (efficientnet_b0): CV=87.2%, Test=79.6%, Gap=7.6%\n",
      "Fold 3 (efficientnet_b0): CV=94.9%, Test=81.6%, Gap=13.2%\n",
      "Fold 5 (efficientnet_b0): CV=89.5%, Test=69.4%, Gap=20.1%\n",
      "Fold 1 (efficientnet_b2): CV=92.3%, Test=73.5%, Gap=18.8%\n",
      "Fold 4 (efficientnet_b2): CV=89.7%, Test=81.6%, Gap=8.1%\n",
      "Average CV-Test gap across folds: 13.6%\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC SUMMARY & RECOMMENDATIONS\n",
      "============================================================\n",
      "ISSUES FOUND:\n",
      "1. Potential patient leaks: 1760 cases\n",
      "2. Very small test set\n",
      "3. Some classes have very few test samples\n",
      "4. Consistent overfitting across folds: 13.6% avg gap\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "1. ⚠️  Check if same patients appear in both train/test\n",
      "2. 📈 Consider larger dataset or repeated train/test splits\n",
      "3. ⚖️  Use stratified sampling to ensure minimum samples per class\n",
      "4. 🔧 Reduce model complexity or increase regularization\n",
      "5. 🔧 Reduce aggressive data augmentation\n",
      "6. 🔧 Add more diverse real data\n",
      "============================================================\n",
      "\n",
      "Diagnostic results saved to: diagnostic_results\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class OverfittingDiagnostic:\n",
    "    def __init__(self, ensemble_path):\n",
    "        self.ensemble_path = ensemble_path\n",
    "        self.ensemble_data = torch.load(ensemble_path, map_location='cpu')\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load original data\n",
    "        self.load_all_data()\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir = Path(\"diagnostic_results\")\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load all original data to check for issues\"\"\"\n",
    "        fpath = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\tooth_analysis\\\\images\\\\*.png\"\n",
    "        image_paths = glob.glob(fpath)\n",
    "        \n",
    "        images = []\n",
    "        ages = []\n",
    "        filenames = []\n",
    "        \n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                \n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img_resized = cv2.resize(img, (448, 224))\n",
    "                \n",
    "                filename = os.path.basename(img_path)\n",
    "                filename_no_ext = os.path.splitext(filename)[0]\n",
    "                parts = filename_no_ext.split('_')\n",
    "                \n",
    "                if len(parts) < 3:\n",
    "                    continue\n",
    "                \n",
    "                bbb_part = parts[1]\n",
    "                if 'p' not in bbb_part:\n",
    "                    continue\n",
    "                \n",
    "                value_str = bbb_part.replace('p', '.')\n",
    "                age_value = float(value_str)\n",
    "                age_value = 5.5 if age_value >= 5.5 else age_value\n",
    "                \n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                filenames.append(filename)\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Convert ages to indices\n",
    "        unique_ages = sorted(list(set(ages)))\n",
    "        self.age_to_idx = {age: i for i, age in enumerate(unique_ages)}\n",
    "        y_indices = [self.age_to_idx[age] for age in ages]\n",
    "        \n",
    "        # Same split as used in training\n",
    "        X_train, X_test, y_train, y_test, names_train, names_test = train_test_split(\n",
    "            images, y_indices, filenames, test_size=0.2, random_state=42, stratify=y_indices\n",
    "        )\n",
    "        \n",
    "        self.X_train = np.array(X_train)\n",
    "        self.X_test = np.array(X_test)\n",
    "        self.y_train = np.array(y_train)\n",
    "        self.y_test = np.array(y_test)\n",
    "        self.names_train = names_train\n",
    "        self.names_test = names_test\n",
    "        self.class_names = [str(age) for age in unique_ages]\n",
    "        \n",
    "        print(f\"Loaded {len(self.X_train)} training images, {len(self.X_test)} test images\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "    \n",
    "    def check_data_leakage(self):\n",
    "        \"\"\"Check for potential data leakage between train and test sets\"\"\"\n",
    "        print(\"\\n=== DATA LEAKAGE ANALYSIS ===\")\n",
    "        \n",
    "        # Check for duplicate filenames (should be impossible but let's verify)\n",
    "        train_set = set(self.names_train)\n",
    "        test_set = set(self.names_test)\n",
    "        overlap = train_set.intersection(test_set)\n",
    "        \n",
    "        print(f\"Filename overlap between train/test: {len(overlap)}\")\n",
    "        if overlap:\n",
    "            print(f\"Overlapping files: {list(overlap)[:5]}...\")\n",
    "        \n",
    "        # Check for very similar filenames (potential same patient)\n",
    "        potential_leaks = []\n",
    "        for test_name in self.names_test:\n",
    "            test_base = test_name.split('_')[0] if '_' in test_name else test_name[:5]\n",
    "            for train_name in self.names_train:\n",
    "                train_base = train_name.split('_')[0] if '_' in train_name else train_name[:5]\n",
    "                if test_base == train_base and test_name != train_name:\n",
    "                    potential_leaks.append((test_name, train_name))\n",
    "        \n",
    "        print(f\"Potential patient leaks (same base filename): {len(potential_leaks)}\")\n",
    "        if potential_leaks:\n",
    "            print(\"Examples:\")\n",
    "            for i, (test, train) in enumerate(potential_leaks[:5]):\n",
    "                print(f\"  Test: {test} <-> Train: {train}\")\n",
    "        \n",
    "        # Analyze filename patterns\n",
    "        print(f\"\\nFilename pattern analysis:\")\n",
    "        print(f\"Train filename examples: {self.names_train[:5]}\")\n",
    "        print(f\"Test filename examples: {self.names_test[:5]}\")\n",
    "        \n",
    "        return len(overlap), len(potential_leaks)\n",
    "    \n",
    "    def analyze_class_distributions(self):\n",
    "        \"\"\"Compare train vs test class distributions\"\"\"\n",
    "        print(\"\\n=== CLASS DISTRIBUTION ANALYSIS ===\")\n",
    "        \n",
    "        train_dist = Counter(self.y_train)\n",
    "        test_dist = Counter(self.y_test)\n",
    "        \n",
    "        # Create comparison\n",
    "        comparison_data = []\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            train_count = train_dist.get(i, 0)\n",
    "            test_count = test_dist.get(i, 0)\n",
    "            train_pct = train_count / len(self.y_train) * 100\n",
    "            test_pct = test_count / len(self.y_test) * 100\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Class': class_name,\n",
    "                'Train_Count': train_count,\n",
    "                'Test_Count': test_count,\n",
    "                'Train_Pct': train_pct,\n",
    "                'Test_Pct': test_pct,\n",
    "                'Pct_Diff': abs(train_pct - test_pct)\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        print(df)\n",
    "        \n",
    "        # Plot distributions\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        x = np.arange(len(self.class_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1.bar(x - width/2, [train_dist.get(i, 0) for i in range(len(self.class_names))], \n",
    "                width, label='Train', alpha=0.8)\n",
    "        ax1.bar(x + width/2, [test_dist.get(i, 0) for i in range(len(self.class_names))], \n",
    "                width, label='Test', alpha=0.8)\n",
    "        ax1.set_xlabel('Age Class')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.set_title('Train vs Test Distribution (Counts)')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(self.class_names)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Percentage differences\n",
    "        pct_diffs = df['Pct_Diff'].values\n",
    "        bars = ax2.bar(self.class_names, pct_diffs, color='red', alpha=0.7)\n",
    "        ax2.set_xlabel('Age Class')\n",
    "        ax2.set_ylabel('Percentage Point Difference')\n",
    "        ax2.set_title('Train vs Test Distribution Differences')\n",
    "        ax2.axhline(y=5, color='orange', linestyle='--', label='5% threshold')\n",
    "        ax2.legend()\n",
    "        \n",
    "        for bar, diff in zip(bars, pct_diffs):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                    f'{diff:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        max_diff = df['Pct_Diff'].max()\n",
    "        print(f\"Maximum distribution difference: {max_diff:.1f} percentage points\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def test_individual_fold_performance(self):\n",
    "        \"\"\"Test each fold's performance on the test set individually\"\"\"\n",
    "        print(\"\\n=== INDIVIDUAL FOLD ANALYSIS ===\")\n",
    "        \n",
    "        # We'll need to load the individual fold models\n",
    "        ensemble_dir = Path(self.ensemble_path).parent\n",
    "        fold_files = list(ensemble_dir.glob(\"*_fold_*.pth\"))\n",
    "        \n",
    "        if not fold_files:\n",
    "            print(\"No individual fold files found. Cannot analyze individual performance.\")\n",
    "            return\n",
    "        \n",
    "        individual_results = []\n",
    "        \n",
    "        print(f\"Found {len(fold_files)} fold models\")\n",
    "        \n",
    "        for fold_file in fold_files:\n",
    "            try:\n",
    "                fold_data = torch.load(fold_file, map_location='cpu')\n",
    "                cv_score = fold_data['cv_score']\n",
    "                architecture = fold_data['model_architecture']\n",
    "                fold_num = fold_data['fold']\n",
    "                \n",
    "                # Load and test this individual model\n",
    "                import timm\n",
    "                import torch.nn as nn\n",
    "                \n",
    "                model = timm.create_model(architecture, pretrained=False, \n",
    "                                        num_classes=len(self.class_names))\n",
    "                \n",
    "                # Recreate classifier structure\n",
    "                if hasattr(model, 'fc'):\n",
    "                    in_features = model.fc.in_features\n",
    "                    model.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(in_features, len(self.class_names)))\n",
    "                elif hasattr(model, 'classifier'):\n",
    "                    if hasattr(model.classifier, 'in_features'):\n",
    "                        in_features = model.classifier.in_features\n",
    "                        model.classifier = nn.Sequential(nn.Dropout(0.3), nn.Linear(in_features, len(self.class_names)))\n",
    "                \n",
    "                model.load_state_dict(fold_data['model_state_dict'])\n",
    "                model.eval()\n",
    "                \n",
    "                # Test on test set\n",
    "                test_accuracy = self.evaluate_single_model(model)\n",
    "                \n",
    "                individual_results.append({\n",
    "                    'Fold': fold_num,\n",
    "                    'Architecture': architecture,\n",
    "                    'CV_Score': cv_score,\n",
    "                    'Test_Accuracy': test_accuracy,\n",
    "                    'Gap': cv_score - test_accuracy\n",
    "                })\n",
    "                \n",
    "                print(f\"Fold {fold_num} ({architecture}): CV={cv_score:.1f}%, Test={test_accuracy:.1f}%, Gap={cv_score-test_accuracy:.1f}%\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {fold_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if individual_results:\n",
    "            df = pd.DataFrame(individual_results)\n",
    "            \n",
    "            # Plot individual gaps\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # CV vs Test scores\n",
    "            folds = [f\"Fold {r['Fold']}\" for r in individual_results]\n",
    "            cv_scores = [r['CV_Score'] for r in individual_results]\n",
    "            test_scores = [r['Test_Accuracy'] for r in individual_results]\n",
    "            \n",
    "            x = np.arange(len(folds))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax1.bar(x - width/2, cv_scores, width, label='CV Score', alpha=0.8)\n",
    "            ax1.bar(x + width/2, test_scores, width, label='Test Score', alpha=0.8)\n",
    "            ax1.set_xlabel('Model Fold')\n",
    "            ax1.set_ylabel('Accuracy (%)')\n",
    "            ax1.set_title('CV vs Test Performance by Fold')\n",
    "            ax1.set_xticks(x)\n",
    "            ax1.set_xticklabels(folds, rotation=45)\n",
    "            ax1.legend()\n",
    "            \n",
    "            # Performance gaps\n",
    "            gaps = [r['Gap'] for r in individual_results]\n",
    "            bars = ax2.bar(folds, gaps, color='red', alpha=0.7)\n",
    "            ax2.set_xlabel('Model Fold')\n",
    "            ax2.set_ylabel('Performance Gap (%)')\n",
    "            ax2.set_title('CV - Test Performance Gap by Fold')\n",
    "            ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "            ax2.set_xticklabels(folds, rotation=45)\n",
    "            \n",
    "            for bar, gap in zip(bars, gaps):\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                        f'{gap:.1f}%', ha='center', va='bottom')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.output_dir / 'individual_fold_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            avg_gap = np.mean(gaps)\n",
    "            print(f\"Average CV-Test gap across folds: {avg_gap:.1f}%\")\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def evaluate_single_model(self, model):\n",
    "        \"\"\"Evaluate a single model on test set\"\"\"\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for image in self.X_test:\n",
    "                # Preprocess\n",
    "                if image.max() > 1.0:\n",
    "                    image = image / 255.0\n",
    "                \n",
    "                image_tensor = torch.FloatTensor(image).permute(2, 0, 1)\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "                image_tensor = (image_tensor - mean) / std\n",
    "                image_tensor = image_tensor.unsqueeze(0)\n",
    "                \n",
    "                outputs = model(image_tensor)\n",
    "                predicted = torch.argmax(outputs, dim=1).item()\n",
    "                \n",
    "                correct += 1 if predicted == self.y_test[total] else 0\n",
    "                total += 1\n",
    "        \n",
    "        return (correct / total) * 100\n",
    "    \n",
    "    def analyze_dataset_size_impact(self):\n",
    "        \"\"\"Analyze if small dataset size is causing the issue\"\"\"\n",
    "        print(\"\\n=== DATASET SIZE ANALYSIS ===\")\n",
    "        \n",
    "        total_samples = len(self.X_train) + len(self.X_test)\n",
    "        samples_per_class = total_samples / len(self.class_names)\n",
    "        test_samples_per_class = len(self.X_test) / len(self.class_names)\n",
    "        \n",
    "        print(f\"Total dataset size: {total_samples}\")\n",
    "        print(f\"Average samples per class: {samples_per_class:.1f}\")\n",
    "        print(f\"Test samples per class: {test_samples_per_class:.1f}\")\n",
    "        print(f\"Test set size: {len(self.X_test)} ({len(self.X_test)/total_samples*100:.1f}% of total)\")\n",
    "        \n",
    "        # Check class representation in test set\n",
    "        min_test_class = min(Counter(self.y_test).values())\n",
    "        max_test_class = max(Counter(self.y_test).values())\n",
    "        \n",
    "        print(f\"Test set class range: {min_test_class} to {max_test_class} samples\")\n",
    "        \n",
    "        if min_test_class < 10:\n",
    "            print(\"⚠️  WARNING: Some classes have very few test samples (<10)\")\n",
    "        if test_samples_per_class < 20:\n",
    "            print(\"⚠️  WARNING: Very small test set - high variance expected\")\n",
    "        \n",
    "        return {\n",
    "            'total_samples': total_samples,\n",
    "            'test_size': len(self.X_test),\n",
    "            'min_test_class': min_test_class,\n",
    "            'max_test_class': max_test_class,\n",
    "            'avg_test_per_class': test_samples_per_class\n",
    "        }\n",
    "    \n",
    "    def generate_recommendations(self, leakage_info, distribution_df, size_info, individual_df=None):\n",
    "        \"\"\"Generate recommendations based on analysis\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DIAGNOSTIC SUMMARY & RECOMMENDATIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        issues_found = []\n",
    "        recommendations = []\n",
    "        \n",
    "        # Check for data leakage\n",
    "        if leakage_info[0] > 0:\n",
    "            issues_found.append(\"Direct filename overlap between train/test\")\n",
    "            recommendations.append(\"🔥 CRITICAL: Remove overlapping files from train or test set\")\n",
    "        \n",
    "        if leakage_info[1] > 0:\n",
    "            issues_found.append(f\"Potential patient leaks: {leakage_info[1]} cases\")\n",
    "            recommendations.append(\"⚠️  Check if same patients appear in both train/test\")\n",
    "        \n",
    "        # Check distribution differences\n",
    "        max_dist_diff = distribution_df['Pct_Diff'].max()\n",
    "        if max_dist_diff > 10:\n",
    "            issues_found.append(f\"Large distribution differences: {max_dist_diff:.1f}%\")\n",
    "            recommendations.append(\"📊 Consider stratified sampling with more constraints\")\n",
    "        \n",
    "        # Check dataset size\n",
    "        if size_info['test_size'] < 100:\n",
    "            issues_found.append(\"Very small test set\")\n",
    "            recommendations.append(\"📈 Consider larger dataset or repeated train/test splits\")\n",
    "        \n",
    "        if size_info['min_test_class'] < 5:\n",
    "            issues_found.append(\"Some classes have very few test samples\")\n",
    "            recommendations.append(\"⚖️  Use stratified sampling to ensure minimum samples per class\")\n",
    "        \n",
    "        # Check individual model performance\n",
    "        if individual_df is not None:\n",
    "            avg_gap = individual_df['Gap'].mean()\n",
    "            if avg_gap > 10:\n",
    "                issues_found.append(f\"Consistent overfitting across folds: {avg_gap:.1f}% avg gap\")\n",
    "                recommendations.append(\"🔧 Reduce model complexity or increase regularization\")\n",
    "                recommendations.append(\"🔧 Reduce aggressive data augmentation\")\n",
    "                recommendations.append(\"🔧 Add more diverse real data\")\n",
    "        \n",
    "        # Print findings\n",
    "        if issues_found:\n",
    "            print(\"ISSUES FOUND:\")\n",
    "            for i, issue in enumerate(issues_found, 1):\n",
    "                print(f\"{i}. {issue}\")\n",
    "        else:\n",
    "            print(\"No obvious issues detected in basic checks.\")\n",
    "        \n",
    "        print(f\"\\nRECOMMENDATIONS:\")\n",
    "        if recommendations:\n",
    "            for i, rec in enumerate(recommendations, 1):\n",
    "                print(f\"{i}. {rec}\")\n",
    "        else:\n",
    "            print(\"1. 🔍 The 13% CV-Test gap suggests overfitting despite no obvious data issues\")\n",
    "            print(\"2. 🔧 Try reducing augmentation intensity\")\n",
    "            print(\"3. 🔧 Add more regularization (dropout, weight decay)\")\n",
    "            print(\"4. 📊 Consider k-fold evaluation on multiple random splits\")\n",
    "            print(\"5. 📈 Collect more diverse real data if possible\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def run_comprehensive_diagnostic(self):\n",
    "        \"\"\"Run all diagnostic tests\"\"\"\n",
    "        print(\"OVERFITTING DIAGNOSTIC ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Run all analyses\n",
    "        leakage_info = self.check_data_leakage()\n",
    "        distribution_df = self.analyze_class_distributions()\n",
    "        size_info = self.analyze_dataset_size_impact()\n",
    "        individual_df = self.test_individual_fold_performance()\n",
    "        \n",
    "        # Generate recommendations\n",
    "        self.generate_recommendations(leakage_info, distribution_df, size_info, individual_df)\n",
    "        \n",
    "        # Save summary\n",
    "        summary = {\n",
    "            'filename_overlap': leakage_info[0],\n",
    "            'potential_patient_leaks': leakage_info[1],\n",
    "            'max_distribution_difference': float(distribution_df['Pct_Diff'].max()),\n",
    "            'test_set_size': size_info['test_size'],\n",
    "            'min_test_class_size': size_info['min_test_class'],\n",
    "            'individual_fold_gaps': individual_df['Gap'].tolist() if individual_df is not None else []\n",
    "        }\n",
    "        \n",
    "        with open(self.output_dir / 'diagnostic_summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nDiagnostic results saved to: {self.output_dir}\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    ensemble_path = \"./academic_ensemble_20250721_081845 (90.7 +- 2.6%)/academic_ensemble.pth\"\n",
    "    diagnostic = OverfittingDiagnostic(ensemble_path)\n",
    "    diagnostic.run_comprehensive_diagnostic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
