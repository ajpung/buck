{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a96314-0633-40b4-b727-9ceef74d57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from buck.analysis.basics import ingest_images\n",
    "\n",
    "# Your existing ingestion\n",
    "fpath = \"..\\\\images\\\\squared\\\\*_NDA.png\"\n",
    "images, ages = ingest_images(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "572f3c3c-89db-4ef1-90f8-8a56e6890e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 288, 288, 1) (36, 288, 288, 1) (45, 288, 288, 1)\n"
     ]
    }
   ],
   "source": [
    "from buck.analysis.basics import split_data\n",
    "\n",
    "Xtr_og, ytr_og, Xval, yval, Xte, yte_onehot, ages, l_map = split_data(images, ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f2affd-a8ae-4e07-8a9c-e85e22e7cf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 0 (Age 1.5): 20 → 1360 samples\n",
      "  Class 1 (Age 2.5): 25 → 1360 samples\n",
      "  Class 2 (Age 3.5): 30 → 1360 samples\n",
      "  Class 3 (Age 4.5): 34 → 1360 samples\n",
      "  Class 4 (Age 5.5): 34 → 1360 samples\n"
     ]
    }
   ],
   "source": [
    "from buck.analysis.basics import homogenize_data\n",
    "\n",
    "augment_multiplier = 40\n",
    "X_train_pca, y_train_flat, X_test_pca, y_true, label_mapping, num_classes = homogenize_data(Xtr_og, ytr_og, Xte,yte_onehot, l_map, augment_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4e91d15-08cb-42d5-b506-1c4e7df6c199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 0 (Age 1.5): 20 → 34 samples\n",
      "  Class 1 (Age 2.5): 25 → 34 samples\n",
      "  Class 2 (Age 3.5): 30 → 34 samples\n",
      "  Class 3 (Age 4.5): 34 → 34 samples\n",
      "  Class 4 (Age 5.5): 34 → 34 samples\n",
      "1 0.5333333333333333 0.5285075433682245\n",
      "  Class 0 (Age 1.5): 20 → 68 samples\n",
      "  Class 1 (Age 2.5): 25 → 68 samples\n",
      "  Class 2 (Age 3.5): 30 → 68 samples\n",
      "  Class 3 (Age 4.5): 34 → 68 samples\n",
      "  Class 4 (Age 5.5): 34 → 68 samples\n",
      "2 0.6 0.6029614634877792\n",
      "  Class 0 (Age 1.5): 20 → 102 samples\n",
      "  Class 1 (Age 2.5): 25 → 102 samples\n",
      "  Class 2 (Age 3.5): 30 → 102 samples\n",
      "  Class 3 (Age 4.5): 34 → 102 samples\n",
      "  Class 4 (Age 5.5): 34 → 102 samples\n",
      "3 0.5777777777777777 0.5756956493798598\n",
      "  Class 0 (Age 1.5): 20 → 136 samples\n",
      "  Class 1 (Age 2.5): 25 → 136 samples\n",
      "  Class 2 (Age 3.5): 30 → 136 samples\n",
      "  Class 3 (Age 4.5): 34 → 136 samples\n",
      "  Class 4 (Age 5.5): 34 → 136 samples\n",
      "4 0.5555555555555556 0.5405539993775288\n",
      "  Class 0 (Age 1.5): 20 → 170 samples\n",
      "  Class 1 (Age 2.5): 25 → 170 samples\n",
      "  Class 2 (Age 3.5): 30 → 170 samples\n",
      "  Class 3 (Age 4.5): 34 → 170 samples\n",
      "  Class 4 (Age 5.5): 34 → 170 samples\n",
      "5 0.5777777777777777 0.582668793195109\n",
      "  Class 0 (Age 1.5): 20 → 204 samples\n",
      "  Class 1 (Age 2.5): 25 → 204 samples\n",
      "  Class 2 (Age 3.5): 30 → 204 samples\n",
      "  Class 3 (Age 4.5): 34 → 204 samples\n",
      "  Class 4 (Age 5.5): 34 → 204 samples\n",
      "6 0.5555555555555556 0.5472236430804206\n",
      "  Class 0 (Age 1.5): 20 → 238 samples\n",
      "  Class 1 (Age 2.5): 25 → 238 samples\n",
      "  Class 2 (Age 3.5): 30 → 238 samples\n",
      "  Class 3 (Age 4.5): 34 → 238 samples\n",
      "  Class 4 (Age 5.5): 34 → 238 samples\n",
      "7 0.5333333333333333 0.536672278338945\n",
      "  Class 0 (Age 1.5): 20 → 272 samples\n",
      "  Class 1 (Age 2.5): 25 → 272 samples\n",
      "  Class 2 (Age 3.5): 30 → 272 samples\n",
      "  Class 3 (Age 4.5): 34 → 272 samples\n",
      "  Class 4 (Age 5.5): 34 → 272 samples\n",
      "8 0.6 0.5975949975949976\n",
      "  Class 0 (Age 1.5): 20 → 306 samples\n",
      "  Class 1 (Age 2.5): 25 → 306 samples\n",
      "  Class 2 (Age 3.5): 30 → 306 samples\n",
      "  Class 3 (Age 4.5): 34 → 306 samples\n",
      "  Class 4 (Age 5.5): 34 → 306 samples\n",
      "9 0.5777777777777777 0.5862776599618704\n",
      "  Class 0 (Age 1.5): 20 → 340 samples\n",
      "  Class 1 (Age 2.5): 25 → 340 samples\n",
      "  Class 2 (Age 3.5): 30 → 340 samples\n",
      "  Class 3 (Age 4.5): 34 → 340 samples\n",
      "  Class 4 (Age 5.5): 34 → 340 samples\n",
      "10 0.6 0.6126495726495726\n",
      "  Class 0 (Age 1.5): 20 → 374 samples\n",
      "  Class 1 (Age 2.5): 25 → 374 samples\n",
      "  Class 2 (Age 3.5): 30 → 374 samples\n",
      "  Class 3 (Age 4.5): 34 → 374 samples\n",
      "  Class 4 (Age 5.5): 34 → 374 samples\n",
      "11 0.6222222222222222 0.6166033820935781\n",
      "  Class 0 (Age 1.5): 20 → 408 samples\n",
      "  Class 1 (Age 2.5): 25 → 408 samples\n",
      "  Class 2 (Age 3.5): 30 → 408 samples\n",
      "  Class 3 (Age 4.5): 34 → 408 samples\n",
      "  Class 4 (Age 5.5): 34 → 408 samples\n",
      "12 0.5777777777777777 0.5903013572578789\n",
      "  Class 0 (Age 1.5): 20 → 442 samples\n",
      "  Class 1 (Age 2.5): 25 → 442 samples\n",
      "  Class 2 (Age 3.5): 30 → 442 samples\n",
      "  Class 3 (Age 4.5): 34 → 442 samples\n",
      "  Class 4 (Age 5.5): 34 → 442 samples\n",
      "13 0.5555555555555556 0.5436919494890509\n",
      "  Class 0 (Age 1.5): 20 → 476 samples\n",
      "  Class 1 (Age 2.5): 25 → 476 samples\n",
      "  Class 2 (Age 3.5): 30 → 476 samples\n",
      "  Class 3 (Age 4.5): 34 → 476 samples\n",
      "  Class 4 (Age 5.5): 34 → 476 samples\n",
      "14 0.6222222222222222 0.6143106131341425\n",
      "  Class 0 (Age 1.5): 20 → 510 samples\n",
      "  Class 1 (Age 2.5): 25 → 510 samples\n",
      "  Class 2 (Age 3.5): 30 → 510 samples\n",
      "  Class 3 (Age 4.5): 34 → 510 samples\n",
      "  Class 4 (Age 5.5): 34 → 510 samples\n",
      "15 0.6222222222222222 0.6095722397781894\n",
      "  Class 0 (Age 1.5): 20 → 544 samples\n",
      "  Class 1 (Age 2.5): 25 → 544 samples\n",
      "  Class 2 (Age 3.5): 30 → 544 samples\n",
      "  Class 3 (Age 4.5): 34 → 544 samples\n",
      "  Class 4 (Age 5.5): 34 → 544 samples\n",
      "16 0.5777777777777777 0.5796165627744574\n",
      "  Class 0 (Age 1.5): 20 → 578 samples\n",
      "  Class 1 (Age 2.5): 25 → 578 samples\n",
      "  Class 2 (Age 3.5): 30 → 578 samples\n",
      "  Class 3 (Age 4.5): 34 → 578 samples\n",
      "  Class 4 (Age 5.5): 34 → 578 samples\n",
      "17 0.5777777777777777 0.5755735492577598\n",
      "  Class 0 (Age 1.5): 20 → 612 samples\n",
      "  Class 1 (Age 2.5): 25 → 612 samples\n",
      "  Class 2 (Age 3.5): 30 → 612 samples\n",
      "  Class 3 (Age 4.5): 34 → 612 samples\n",
      "  Class 4 (Age 5.5): 34 → 612 samples\n",
      "18 0.6222222222222222 0.6000265226863692\n",
      "  Class 0 (Age 1.5): 20 → 646 samples\n",
      "  Class 1 (Age 2.5): 25 → 646 samples\n",
      "  Class 2 (Age 3.5): 30 → 646 samples\n",
      "  Class 3 (Age 4.5): 34 → 646 samples\n",
      "  Class 4 (Age 5.5): 34 → 646 samples\n",
      "19 0.6444444444444445 0.6454098208484174\n",
      "  Class 0 (Age 1.5): 20 → 680 samples\n",
      "  Class 1 (Age 2.5): 25 → 680 samples\n",
      "  Class 2 (Age 3.5): 30 → 680 samples\n",
      "  Class 3 (Age 4.5): 34 → 680 samples\n",
      "  Class 4 (Age 5.5): 34 → 680 samples\n",
      "20 0.6 0.5939603911740134\n",
      "  Class 0 (Age 1.5): 20 → 714 samples\n",
      "  Class 1 (Age 2.5): 25 → 714 samples\n",
      "  Class 2 (Age 3.5): 30 → 714 samples\n",
      "  Class 3 (Age 4.5): 34 → 714 samples\n",
      "  Class 4 (Age 5.5): 34 → 714 samples\n",
      "21 0.6222222222222222 0.6096429308565531\n",
      "  Class 0 (Age 1.5): 20 → 748 samples\n",
      "  Class 1 (Age 2.5): 25 → 748 samples\n",
      "  Class 2 (Age 3.5): 30 → 748 samples\n",
      "  Class 3 (Age 4.5): 34 → 748 samples\n",
      "  Class 4 (Age 5.5): 34 → 748 samples\n",
      "22 0.6 0.5841809911985351\n",
      "  Class 0 (Age 1.5): 20 → 782 samples\n",
      "  Class 1 (Age 2.5): 25 → 782 samples\n",
      "  Class 2 (Age 3.5): 30 → 782 samples\n",
      "  Class 3 (Age 4.5): 34 → 782 samples\n",
      "  Class 4 (Age 5.5): 34 → 782 samples\n",
      "23 0.6222222222222222 0.6062338283902425\n",
      "  Class 0 (Age 1.5): 20 → 816 samples\n",
      "  Class 1 (Age 2.5): 25 → 816 samples\n",
      "  Class 2 (Age 3.5): 30 → 816 samples\n",
      "  Class 3 (Age 4.5): 34 → 816 samples\n",
      "  Class 4 (Age 5.5): 34 → 816 samples\n",
      "24 0.6 0.5881514513398571\n",
      "  Class 0 (Age 1.5): 20 → 850 samples\n",
      "  Class 1 (Age 2.5): 25 → 850 samples\n",
      "  Class 2 (Age 3.5): 30 → 850 samples\n",
      "  Class 3 (Age 4.5): 34 → 850 samples\n",
      "  Class 4 (Age 5.5): 34 → 850 samples\n",
      "25 0.5555555555555556 0.5448888888888888\n",
      "  Class 0 (Age 1.5): 20 → 884 samples\n",
      "  Class 1 (Age 2.5): 25 → 884 samples\n",
      "  Class 2 (Age 3.5): 30 → 884 samples\n",
      "  Class 3 (Age 4.5): 34 → 884 samples\n",
      "  Class 4 (Age 5.5): 34 → 884 samples\n",
      "26 0.6444444444444445 0.637085467291417\n",
      "  Class 0 (Age 1.5): 20 → 918 samples\n",
      "  Class 1 (Age 2.5): 25 → 918 samples\n",
      "  Class 2 (Age 3.5): 30 → 918 samples\n",
      "  Class 3 (Age 4.5): 34 → 918 samples\n",
      "  Class 4 (Age 5.5): 34 → 918 samples\n",
      "27 0.6222222222222222 0.6097140083410105\n",
      "  Class 0 (Age 1.5): 20 → 952 samples\n",
      "  Class 1 (Age 2.5): 25 → 952 samples\n",
      "  Class 2 (Age 3.5): 30 → 952 samples\n",
      "  Class 3 (Age 4.5): 34 → 952 samples\n",
      "  Class 4 (Age 5.5): 34 → 952 samples\n",
      "28 0.6222222222222222 0.6130894503129438\n",
      "  Class 0 (Age 1.5): 20 → 986 samples\n",
      "  Class 1 (Age 2.5): 25 → 986 samples\n",
      "  Class 2 (Age 3.5): 30 → 986 samples\n",
      "  Class 3 (Age 4.5): 34 → 986 samples\n",
      "  Class 4 (Age 5.5): 34 → 986 samples\n",
      "29 0.6222222222222222 0.6127468429527926\n",
      "  Class 0 (Age 1.5): 20 → 1020 samples\n",
      "  Class 1 (Age 2.5): 25 → 1020 samples\n",
      "  Class 2 (Age 3.5): 30 → 1020 samples\n",
      "  Class 3 (Age 4.5): 34 → 1020 samples\n",
      "  Class 4 (Age 5.5): 34 → 1020 samples\n",
      "30 0.5555555555555556 0.5519028340080971\n",
      "  Class 0 (Age 1.5): 20 → 1054 samples\n",
      "  Class 1 (Age 2.5): 25 → 1054 samples\n",
      "  Class 2 (Age 3.5): 30 → 1054 samples\n",
      "  Class 3 (Age 4.5): 34 → 1054 samples\n",
      "  Class 4 (Age 5.5): 34 → 1054 samples\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.63 GiB for an array with shape (5270, 82944) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Optimize Augmentation factor (RandFor)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m aug_mul \u001b[38;5;129;01min\u001b[39;00m np.arange(\u001b[32m1\u001b[39m,\u001b[32m65\u001b[39m,\u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     X_train_pca, y_train_flat, X_test_pca, y_true, label_mapping, num_classes = \u001b[43mhomogenize_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr_og\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr_og\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXte\u001b[49m\u001b[43m,\u001b[49m\u001b[43myte_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_mul\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Shorten parameters\u001b[39;00m\n\u001b[32m     11\u001b[39m     Xtr_pca = X_train_pca\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\Dropbox\\AI Projects\\buck\\src\\buck\\analysis\\basics.py:265\u001b[39m, in \u001b[36mhomogenize_data\u001b[39m\u001b[34m(X_train_orig, y_train_orig, X_test, y_test_onehot, label_mapping, aug_mult)\u001b[39m\n\u001b[32m    262\u001b[39m y_true = np.argmax(y_test_onehot, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# Flatten the data - using full images\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m X_train_flat = X_train_augmented.reshape(X_train_augmented.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m).astype(\n\u001b[32m    266\u001b[39m     np.float32\n\u001b[32m    267\u001b[39m )\n\u001b[32m    268\u001b[39m X_test_flat = X_test.reshape(X_test.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m).astype(np.float32)\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# Apply standardization\u001b[39;00m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.63 GiB for an array with shape (5270, 82944) and data type float32"
     ]
    }
   ],
   "source": [
    "from buck.classifiers.random_forest import (\n",
    "    _optimize_rs, _optimize_nest, _optimize_max_d, _optimize_crit, _optimize_cw, _optimize_mss, _optimize_msl, _optimize_mwfl, _optimize_mf, _optimize_mln, _optimize_mid\n",
    ")\n",
    "\n",
    "# Optimize Augmentation factor (RandFor)\n",
    "\n",
    "for aug_mul in np.arange(1,65,1):\n",
    "    X_train_pca, y_train_flat, X_test_pca, y_true, label_mapping, num_classes = homogenize_data(Xtr_og, ytr_og, Xte,yte_onehot, l_map, aug_mul)\n",
    "\n",
    "    # Shorten parameters\n",
    "    Xtr_pca = X_train_pca\n",
    "    ytr_flat = y_train_flat\n",
    "    Xte_pca = X_test_pca\n",
    "\n",
    "    opts = {\"n_estimators\": 100, \"criterion\": \"gini\", \"max_depth\": None, \"min_samples_split\": 2, \"min_samples_leaf\": 1, \"min_weight_fraction_leaf\": 0.0,\n",
    "            \"max_features\": \"sqrt\", \"max_leaf_nodes\": None, \"min_impurity_decrease\": 0.0, \"bootstrap\": True, \"oob_score\": False, \"n_jobs\": -1,\n",
    "            \"random_state\": 42, \"verbose\": 0, \"warm_start\": False, \"class_weight\": None, \"ccp_alpha\": 0.0, \"max_samples\": None, \"monotonic_cst\": None}\n",
    "    \n",
    "    # Optimize hyperparameters\n",
    "    ma_vec = []\n",
    "    f1_vec = []\n",
    "    opts, _, _ = _optimize_rs(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_nest(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_max_d(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_crit(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)  # type: ignore\n",
    "    opts, _, _ = _optimize_cw(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_mss(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_msl(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_mwfl(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_mf(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_mln(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, ma, f1 = _optimize_mid(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    ma_vec.append(ma)\n",
    "    f1_vec.append(f1)\n",
    "\n",
    "    print(aug_mul,ma,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54486b36-c48f-4b93-9069-1faf2c1f0374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing\n",
      "...adaboost\n"
     ]
    }
   ],
   "source": [
    "from buck.classifiers.autotune import optimize_all\n",
    "\n",
    "results = optimize_all(X_train_pca, y_train_flat, X_test_pca, y_true, cycles = 1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415e8a2-4981-46df-9980-e2f0c35bd3a3",
   "metadata": {},
   "source": [
    "## Homogenize data across classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c2ba6-9153-4ea4-a8f6-55201f91bb7b",
   "metadata": {},
   "source": [
    "## Optimize all classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6d11d-5762-40c8-947d-f66c4a4045f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "'''\n",
    "from buck.classifiers.ada_boost import (_optimize_rs, _optimize_nest, _optimize_lr)\n",
    "\n",
    "# Shorten parameters\n",
    "Xtr_pca = X_train_pca\n",
    "ytr_flat = y_train_flat\n",
    "Xte_pca = X_test_pca\n",
    "\n",
    "# Define optimals\n",
    "opts = {\n",
    "    \"random_state\": None,\n",
    "    \"estimator\": None,\n",
    "    \"n_estimators\": 50,\n",
    "    \"learning_rate\": 1.0,\n",
    "}\n",
    "\n",
    "# Adaboost\n",
    "opts, ma, ab_rs = _optimize_rs(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "opts, ma, ab_ne = _optimize_nest(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "opts, ma, ab_lr = _optimize_lr(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "print(ma)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8aedc-fd21-4ced-b966-16b7008eedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from buck.classifiers.random_forest import (\n",
    "    _optimize_rs, _optimize_nest, _optimize_max_d, _optimize_crit, _optimize_cw, _optimize_mss, _optimize_msl, _optimize_mwfl, _optimize_mf, _optimize_mln, _optimize_mid\n",
    ")\n",
    "\n",
    "# Shorten parameters\n",
    "Xtr_pca = X_train_pca\n",
    "ytr_flat = y_train_flat\n",
    "Xte_pca = X_test_pca\n",
    "\n",
    "opts = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"criterion\": \"gini\",\n",
    "    \"max_depth\": None,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"min_weight_fraction_leaf\": 0.0,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"max_leaf_nodes\": None,\n",
    "    \"min_impurity_decrease\": 0.0,\n",
    "    \"bootstrap\": True,\n",
    "    \"oob_score\": False,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "    \"warm_start\": False,\n",
    "    \"class_weight\": None,\n",
    "    \"ccp_alpha\": 0.0,\n",
    "    \"max_samples\": None,\n",
    "    \"monotonic_cst\": None,\n",
    "}\n",
    "\n",
    "# Optimize hyperparameters\n",
    "ma_vec = []\n",
    "f1_vec = []\n",
    "for c in np.arange(10):\n",
    "    opts, _, _ = _optimize_rs(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_nest(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_max_d(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_crit(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)  # type: ignore\n",
    "    opts, _, _ = _optimize_cw(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_mss(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_msl(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_mwfl(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_mf(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, _, _ = _optimize_mln(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    opts, ma, f1 = _optimize_mid(Xtr_pca, ytr_flat, Xte_pca, y_true, opts)\n",
    "    ma_vec.append(ma)\n",
    "    f1_vec.append(f1)\n",
    "    print(ma, f1)\n",
    "\n",
    "#Best: 0.7894736842105263\n",
    "#{'n_estimators': np.int64(127),\n",
    "# 'criterion': 'gini',\n",
    "# 'max_depth': None,\n",
    "# 'min_samples_split': np.int64(2),\n",
    "# 'min_samples_leaf': np.int64(1),\n",
    "# 'min_weight_fraction_leaf': np.float64(0.0),\n",
    "# 'max_features': 'log2',\n",
    "# 'max_leaf_nodes': None,\n",
    "# 'min_impurity_decrease': np.float64(0.0),\n",
    "# 'bootstrap': True,\n",
    "# 'oob_score': False,\n",
    "# 'n_jobs': -1,\n",
    "# 'random_state': np.int64(405),\n",
    "# 'verbose': 0,\n",
    "# 'warm_start': False,\n",
    "# 'class_weight': None,\n",
    "# 'ccp_alpha': 0.0,\n",
    "# 'max_samples': None,\n",
    "# 'monotonic_cst': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ababe7-a723-4cbd-92b3-597b6c4cde41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Neural Net\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Shorten parameters\n",
    "Xtr_pca = X_train_pca\n",
    "ytr_flat = y_train_flat\n",
    "Xte_pca = X_test_pca\n",
    "\n",
    "# Define optimals\n",
    "opts = {\n",
    "    \"hidden_layer_sizes\": (100,),\n",
    "    \"activation\": \"relu\",\n",
    "    \"solver\": \"adam\",\n",
    "    \"alpha\": 0.0001,\n",
    "    \"batch_size\": \"auto\",\n",
    "    \"learning_rate\": \"constant\",\n",
    "    \"learning_rate_init\": 0.001,\n",
    "    \"power_t\": 0.5,\n",
    "    \"max_iter\": 20000,\n",
    "    \"shuffle\": True,\n",
    "    \"random_state\": None,\n",
    "    \"tol\": 0.01,\n",
    "    \"verbose\": False,\n",
    "    \"warm_start\": False,\n",
    "    \"momentum\": 0.9,\n",
    "    \"nesterovs_momentum\": True,\n",
    "    \"early_stopping\": False,\n",
    "    \"validation_fraction\": 0.1,\n",
    "    \"beta_1\": 0.9,\n",
    "    \"beta_2\": 0.999,\n",
    "    \"epsilon\": 1e-08,\n",
    "    \"n_iter_no_change\": 10,\n",
    "    \"max_fun\": 15000,\n",
    "}\n",
    "\n",
    "# Initialize variables\n",
    "ac_vec = []\n",
    "f1_vec = []\n",
    "max_acc = -np.inf\n",
    "max_idx = -1\n",
    "variable_array = np.arange(150)\n",
    "#best_val = variable_array[0]\n",
    "for i in np.arange(len(variable_array)):\n",
    "    v = variable_array[i]\n",
    "    # Define classifiers to test\n",
    "    classifier = MLPClassifier(\n",
    "        random_state=v,\n",
    "        hidden_layer_sizes=opts[\"hidden_layer_sizes\"],\n",
    "        activation=opts[\"activation\"],\n",
    "        solver=opts[\"solver\"],\n",
    "        alpha=opts[\"alpha\"],\n",
    "        batch_size=opts[\"batch_size\"],\n",
    "        learning_rate=opts[\"learning_rate\"],\n",
    "        learning_rate_init=opts[\"learning_rate_init\"],\n",
    "        power_t=opts[\"power_t\"],\n",
    "        max_iter=opts[\"max_iter\"],\n",
    "        shuffle=opts[\"shuffle\"],\n",
    "        tol=opts[\"tol\"],\n",
    "        verbose=opts[\"verbose\"],\n",
    "        warm_start=opts[\"warm_start\"],\n",
    "        momentum=opts[\"momentum\"],\n",
    "        nesterovs_momentum=opts[\"nesterovs_momentum\"],\n",
    "        early_stopping=opts[\"early_stopping\"],\n",
    "        validation_fraction=opts[\"validation_fraction\"],\n",
    "        beta_1=opts[\"beta_1\"],\n",
    "        beta_2=opts[\"beta_2\"],\n",
    "        epsilon=opts[\"epsilon\"],\n",
    "        n_iter_no_change=opts[\"n_iter_no_change\"],\n",
    "        max_fun=opts[\"max_fun\"],\n",
    "    )\n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train_pca, y_train_flat)\n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test_pca)\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    ac_vec.append(accuracy)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1_vec.append(f1)\n",
    "    \n",
    "    # Return index\n",
    "    if accuracy >= max_acc:\n",
    "        max_acc = accuracy\n",
    "        print(max_acc)\n",
    "        best_val = v\n",
    "    \n",
    "    # Store best value\n",
    "    opts[\"random_state\"] = best_val\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8281d0-0684-46a1-9e1d-28fa603e6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from buck.classifiers.compare_models import compare_models\n",
    "#\n",
    "## Shorten parameters\n",
    "#Xtr_pca = X_train_pca\n",
    "#ytr_flat = y_train_flat\n",
    "#Xte_pca = X_test_pca\n",
    "#\n",
    "#compare_models(Xtr_pca, ytr_flat, Xte_pca, y_true, num_classes, label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51182f6d-437a-4c77-96c0-0156ca6a648a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c450d8-7f37-4727-9223-1c3855193bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
