{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2dad4-961a-479e-9619-1a78b5561d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def save_backup_files(self, completed_count):\n",
    "        \"\"\"Save JSON/pickle backup files after each architecture\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save as JSON\n",
    "        json_file = f'deer_aging_backup_{completed_count}_{timestamp}.json'\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(self.all_results, f, indent=2)\n",
    "        \n",
    "        # Save as pickle for full Python objects\n",
    "        pkl_file = f'deer_aging_backup_{completed_count}_{timestamp}.pkl'\n",
    "        with open(pkl_file, 'wb') as f:\n",
    "            pickle.dump(self.all_results, f)\n",
    "        \n",
    "        return json_file, pkl_file\n",
    "\n",
    "def load_original_data():\n",
    "    \"\"\"Load the original 357 images\"\"\"\n",
    "    print(\"📂 LOADING ORIGINAL DATA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load using the user's method\n",
    "    from buck.analysis.basics import ingest_images\n",
    "    \n",
    "    fpath = \"C:\\\\Users\\\\aaron\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*.png\"\n",
    "    images, ages = ingest_images(fpath)\n",
    "    print(f\"   ✅ Loaded {len(images)} original images\")\n",
    "    \n",
    "    # Group ages: 5.5+ all become 5.5 (creating exactly 5 classes)\n",
    "    print(\"   🔄 Grouping ages: 5.5+ → 5.5\")\n",
    "    ages_grouped = []\n",
    "    for age in ages:\n",
    "        if age >= 5.5:\n",
    "            ages_grouped.append(5.5)\n",
    "        else:\n",
    "            ages_grouped.append(age)\n",
    "    \n",
    "    # Print age distribution before and after grouping\n",
    "    print(f\"   📊 Original age distribution: {dict(Counter(ages))}\")\n",
    "    print(f\"   📊 Grouped age distribution: {dict(Counter(ages_grouped))}\")\n",
    "    \n",
    "    return images, ages_grouped\n",
    "\n",
    "def create_train_val_test_split(images, ages, test_size=0.2, val_size=0.15, random_state=42):\n",
    "    \"\"\"Create train/validation/test split\"\"\"\n",
    "    print(\"\\n🔀 CREATING TRAIN/VAL/TEST SPLIT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    if not isinstance(images, np.ndarray):\n",
    "        images = np.array(images)\n",
    "    if not isinstance(ages, np.ndarray):\n",
    "        ages = np.array(ages)\n",
    "    \n",
    "    # Check if stratified split is possible\n",
    "    age_counts = Counter(ages)\n",
    "    min_count = min(age_counts.values())\n",
    "    can_stratify = min_count >= 2\n",
    "    \n",
    "    print(f\"   📊 Age distribution: {dict(age_counts)}\")\n",
    "    print(f\"   📊 Minimum class size: {min_count}\")\n",
    "    print(f\"   🎯 Can use stratified split: {can_stratify}\")\n",
    "    \n",
    "    if can_stratify:\n",
    "        # First split: separate test set (stratified)\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            images, ages, test_size=test_size, random_state=random_state, stratify=ages\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation from remaining data\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # Adjust for remaining data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, stratify=y_temp\n",
    "        )\n",
    "    else:\n",
    "        print(\"   ⚠️ Using random split (some classes too small for stratification)\")\n",
    "        # First split: separate test set (random)\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            images, ages, test_size=test_size, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation from remaining data\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # Adjust for remaining data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, shuffle=True\n",
    "        )\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    reverse_mapping = {i: age for age, i in label_mapping.items()}\n",
    "    \n",
    "    print(f\"   📊 Train: {len(X_train)} samples\")\n",
    "    print(f\"   📊 Val: {len(X_val)} samples\") \n",
    "    print(f\"   📊 Test: {len(X_test)} samples\")\n",
    "    print(f\"   🏷️ Label mapping: {label_mapping}\")\n",
    "    print(f\"   🎯 Number of classes: {len(unique_ages)}\")\n",
    "    \n",
    "    # Convert ages to class indices\n",
    "    y_train_indices = np.array([label_mapping[age] for age in y_train])\n",
    "    y_val_indices = np.array([label_mapping[age] for age in y_val])\n",
    "    y_test_indices = np.array([label_mapping[age] for age in y_test])\n",
    "    \n",
    "    print(f\"   📈 Train distribution: {Counter(y_train_indices)}\")\n",
    "    print(f\"   📈 Val distribution: {Counter(y_val_indices)}\")\n",
    "    print(f\"   📈 Test distribution: {Counter(y_test_indices)}\")\n",
    "    \n",
    "    return (X_train, y_train_indices, X_val, y_val_indices, X_test, y_test_indices, \n",
    "            label_mapping, reverse_mapping)\n",
    "\n",
    "def augment_image(image):\n",
    "    \"\"\"Apply random augmentation to an image\"\"\"\n",
    "    # Ensure image is uint8\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Random rotation\n",
    "    if random.random() < 0.5:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Random horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Random brightness/contrast\n",
    "    if random.random() < 0.5:\n",
    "        alpha = random.uniform(0.8, 1.2)  # Contrast\n",
    "        beta = random.randint(-20, 20)    # Brightness\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Random noise (fixed data type issue)\n",
    "    if random.random() < 0.3:\n",
    "        # Create noise with same dtype as image\n",
    "        noise = np.random.normal(0, 5, image.shape).astype(np.int16)  # Use int16 to handle negative values\n",
    "        # Convert image to int16 for safe addition\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        # Add noise and clip to valid range\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        # Convert back to uint8\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def balance_and_augment_data(X_train, y_train, augment_multiplier=30, num_classes=5):\n",
    "    \"\"\"Balance classes and augment training data\"\"\"\n",
    "    print(f\"\\n🔄 BALANCING AND AUGMENTING DATA\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"   🎯 Target: {augment_multiplier}x augmentation per class\")\n",
    "    \n",
    "    # Count samples per class\n",
    "    class_counts = Counter(y_train)\n",
    "    print(f\"   📊 Original distribution: {dict(class_counts)}\")\n",
    "    \n",
    "    # Find target count (based on largest class * multiplier)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max_count * augment_multiplier\n",
    "    print(f\"   🎯 Target samples per class: {target_count}\")\n",
    "    \n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Get samples for this class\n",
    "        class_mask = y_train == class_idx\n",
    "        class_images = X_train[class_mask]\n",
    "        class_labels = y_train[class_mask]\n",
    "        \n",
    "        current_count = len(class_images)\n",
    "        needed_count = target_count\n",
    "        \n",
    "        print(f\"   📈 Class {class_idx}: {current_count} → {needed_count} samples\")\n",
    "        \n",
    "        # Add original samples\n",
    "        X_augmented.extend(class_images)\n",
    "        y_augmented.extend(class_labels)\n",
    "        \n",
    "        # Generate augmented samples\n",
    "        augmented_needed = needed_count - current_count\n",
    "        \n",
    "        for i in range(augmented_needed):\n",
    "            # Pick random original image from this class\n",
    "            original_idx = random.randint(0, current_count - 1)\n",
    "            original_image = class_images[original_idx].copy()\n",
    "            \n",
    "            # Augment it\n",
    "            augmented_image = augment_image(original_image)\n",
    "            \n",
    "            X_augmented.append(augmented_image)\n",
    "            y_augmented.append(class_idx)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    X_augmented = np.array(X_augmented)\n",
    "    y_augmented = np.array(y_augmented)\n",
    "    \n",
    "    print(f\"   ✅ Augmentation complete: {len(X_augmented)} total samples\")\n",
    "    print(f\"   📊 Final distribution: {Counter(y_augmented)}\")\n",
    "    \n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    \"\"\"Dataset for deer aging with preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, transform=True):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.FloatTensor(X)\n",
    "        else:\n",
    "            self.X = torch.FloatTensor(np.array(X))\n",
    "            \n",
    "        if isinstance(y, np.ndarray):\n",
    "            self.y = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.y = torch.LongTensor(np.array(y))\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Normalize to [0,1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Ensure CHW format (channels first)\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Resize to 224x224\n",
    "        if image.shape[-2:] != (224, 224):\n",
    "            image = image.unsqueeze(0)\n",
    "            image = F.interpolate(image, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            image = image.squeeze(0)\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        image = (image - mean) / std\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class CompleteDeerAgeTrainer:\n",
    "    \"\"\"Complete deer age trainer starting from EfficientNet-B5\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.all_results = []\n",
    "        print(f\"🔥 COMPLETE DEER AGE TRAINER\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Classes: {num_classes}\")\n",
    "    \n",
    "    def get_all_architectures_with_fallback(self):\n",
    "        \"\"\"Get ALL architectures with graceful fallback for missing pretrained weights\"\"\"\n",
    "        \n",
    "        architectures = {\n",
    "            # EfficientNet COMPLETE SERIES (B5-B7 + variants)\n",
    "            #'EfficientNet-B5': {'model_name': 'efficientnet_b5', 'family': 'EfficientNet'},\n",
    "            #'EfficientNet-B6': {'model_name': 'efficientnet_b6', 'family': 'EfficientNet'},\n",
    "            #'EfficientNet-B7': {'model_name': 'efficientnet_b7', 'family': 'EfficientNet'},\n",
    "            \n",
    "            # EfficientNetV2 (try multiple naming conventions)\n",
    "            #'EfficientNetV2-S': {'model_name': 'efficientnetv2_s', 'family': 'EfficientNetV2', 'alternatives': ['tf_efficientnetv2_s_in21ft1k', 'efficientnetv2_rw_s']},\n",
    "            #EfficientNetV2-M': {'model_name': 'efficientnetv2_m', 'family': 'EfficientNetV2', 'alternatives': ['tf_efficientnetv2_m_in21ft1k', 'efficientnetv2_rw_m']},\n",
    "            #'EfficientNetV2-L': {'model_name': 'efficientnetv2_l', 'family': 'EfficientNetV2', 'alternatives': ['tf_efficientnetv2_l_in21ft1k']},\n",
    "            \n",
    "            # DenseNet Family\n",
    "            #'DenseNet-121': {'model_name': 'densenet121', 'family': 'DenseNet'},\n",
    "            #'DenseNet-161': {'model_name': 'densenet161', 'family': 'DenseNet'},\n",
    "            #'DenseNet-169': {'model_name': 'densenet169', 'family': 'DenseNet'},\n",
    "            'DenseNet-201': {'model_name': 'densenet201', 'family': 'DenseNet'},\n",
    "            \n",
    "            # ResNet Family\n",
    "            'ResNet-18': {'model_name': 'resnet18', 'family': 'ResNet'},\n",
    "            'ResNet-34': {'model_name': 'resnet34', 'family': 'ResNet'},\n",
    "            'ResNet-50': {'model_name': 'resnet50', 'family': 'ResNet'},\n",
    "            'ResNet-101': {'model_name': 'resnet101', 'family': 'ResNet'},\n",
    "            'ResNet-152': {'model_name': 'resnet152', 'family': 'ResNet'},\n",
    "            \n",
    "            # ResNeXt\n",
    "            'ResNeXt-50': {'model_name': 'resnext50_32x4d', 'family': 'ResNeXt'},\n",
    "            'ResNeXt-101': {'model_name': 'resnext101_32x8d', 'family': 'ResNeXt'},\n",
    "            \n",
    "            # Wide ResNet\n",
    "            'Wide-ResNet-50': {'model_name': 'wide_resnet50_2', 'family': 'Wide-ResNet'},\n",
    "            'Wide-ResNet-101': {'model_name': 'wide_resnet101_2', 'family': 'Wide-ResNet'},\n",
    "            \n",
    "            # MobileNet Family\n",
    "            'MobileNetV2': {'model_name': 'mobilenetv2_100', 'family': 'MobileNet'},\n",
    "            'MobileNetV3-Small': {'model_name': 'mobilenetv3_small_100', 'family': 'MobileNet'},\n",
    "            'MobileNetV3-Large': {'model_name': 'mobilenetv3_large_100', 'family': 'MobileNet'},\n",
    "            \n",
    "            # RegNet Family (try multiple naming conventions)\n",
    "            'RegNetX-400MF': {'model_name': 'regnetx_400mf', 'family': 'RegNet', 'alternatives': ['regnetx_002', 'regnetx_004']},\n",
    "            'RegNetX-800MF': {'model_name': 'regnetx_800mf', 'family': 'RegNet', 'alternatives': ['regnetx_004', 'regnetx_006']},\n",
    "            'RegNetY-400MF': {'model_name': 'regnety_400mf', 'family': 'RegNet', 'alternatives': ['regnety_002', 'regnety_004']},\n",
    "            'RegNetY-800MF': {'model_name': 'regnety_800mf', 'family': 'RegNet', 'alternatives': ['regnety_004', 'regnety_006']},\n",
    "            \n",
    "            # ConvNeXt Family\n",
    "            'ConvNeXt-Tiny': {'model_name': 'convnext_tiny', 'family': 'ConvNeXt', 'alternatives': ['convnext_tiny_in22ft1k']},\n",
    "            'ConvNeXt-Small': {'model_name': 'convnext_small', 'family': 'ConvNeXt', 'alternatives': ['convnext_small_in22ft1k']},\n",
    "            'ConvNeXt-Base': {'model_name': 'convnext_base', 'family': 'ConvNeXt', 'alternatives': ['convnext_base_in22ft1k']},\n",
    "            \n",
    "            # Vision Transformer variants\n",
    "            'Swin-Tiny': {'model_name': 'swin_tiny_patch4_window7_224', 'family': 'Swin', 'alternatives': ['swin_tiny_patch4_window7_224_in22k']},\n",
    "            'Swin-Small': {'model_name': 'swin_small_patch4_window7_224', 'family': 'Swin', 'alternatives': ['swin_small_patch4_window7_224_in22k']},\n",
    "            \n",
    "            # VGG (classic)\n",
    "            'VGG-16': {'model_name': 'vgg16', 'family': 'VGG'},\n",
    "            'VGG-19': {'model_name': 'vgg19', 'family': 'VGG'},\n",
    "            \n",
    "            # Vision Transformers\n",
    "            'DeiT-Tiny': {'model_name': 'deit_tiny_patch16_224', 'family': 'DeiT'},\n",
    "            'DeiT-Small': {'model_name': 'deit_small_patch16_224', 'family': 'DeiT'},\n",
    "            'DeiT-Base': {'model_name': 'deit_base_patch16_224', 'family': 'DeiT'},\n",
    "            \n",
    "            # Additional EfficientNet variants\n",
    "            'EfficientNet-ES': {'model_name': 'efficientnet_es', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-EM': {'model_name': 'efficientnet_em', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-EL': {'model_name': 'efficientnet_el', 'family': 'EfficientNet'},\n",
    "            \n",
    "            # Additional strong architectures\n",
    "            'ResNet-26': {'model_name': 'resnet26', 'family': 'ResNet'},\n",
    "            'ResNet-26d': {'model_name': 'resnet26d', 'family': 'ResNet'},\n",
    "            'SEResNet-50': {'model_name': 'seresnet50', 'family': 'SEResNet'},\n",
    "            'SEResNeXt-50': {'model_name': 'seresnext50_32x4d', 'family': 'SEResNeXt'},\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n🏗️ COMPLETE ARCHITECTURE ARSENAL ({len(architectures)} models)\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"🎯 FALLBACK STRATEGY: Pretrained → Alternative Names → Random Init\")\n",
    "        print(\"📊 ALL models will be tested regardless of pretrained weight availability\")\n",
    "        \n",
    "        # Group by family and show counts\n",
    "        families = {}\n",
    "        for arch_name, arch_info in architectures.items():\n",
    "            family = arch_info['family']\n",
    "            if family not in families:\n",
    "                families[family] = []\n",
    "            families[family].append(arch_name)\n",
    "        \n",
    "        for family, models in families.items():\n",
    "            print(f\"📁 {family} ({len(models)} models): {', '.join(models)}\")\n",
    "        \n",
    "        return architectures\n",
    "    \n",
    "    def create_model_with_fallback(self, arch_name, arch_info, freeze_strategy='none'):\n",
    "        \"\"\"Create model with graceful fallback for missing pretrained weights\"\"\"\n",
    "        model_name = arch_info['model_name']\n",
    "        alternatives = arch_info.get('alternatives', [])\n",
    "        \n",
    "        print(f\"      🔧 Creating {arch_name}...\")\n",
    "        \n",
    "        # Strategy 1: Try primary model name with pretrained=True\n",
    "        try:\n",
    "            print(f\"         🎯 Trying pretrained: {model_name}\")\n",
    "            model = timm.create_model(model_name, pretrained=True, num_classes=self.num_classes)\n",
    "            initialization_type = \"pretrained\"\n",
    "            final_model_name = model_name\n",
    "            print(f\"         ✅ SUCCESS with pretrained weights!\")\n",
    "        except Exception as e1:\n",
    "            print(f\"         ❌ Pretrained failed: {str(e1)[:50]}...\")\n",
    "            \n",
    "            # Strategy 2: Try alternative names with pretrained=True\n",
    "            model = None\n",
    "            for alt_name in alternatives:\n",
    "                try:\n",
    "                    print(f\"         🎯 Trying alternative pretrained: {alt_name}\")\n",
    "                    model = timm.create_model(alt_name, pretrained=True, num_classes=self.num_classes)\n",
    "                    initialization_type = \"pretrained_alt\"\n",
    "                    final_model_name = alt_name\n",
    "                    print(f\"         ✅ SUCCESS with alternative pretrained weights!\")\n",
    "                    break\n",
    "                except Exception as e2:\n",
    "                    print(f\"         ❌ Alternative {alt_name} failed: {str(e2)[:30]}...\")\n",
    "                    continue\n",
    "            \n",
    "            # Strategy 3: Fall back to random initialization\n",
    "            if model is None:\n",
    "                try:\n",
    "                    print(f\"         🎲 Falling back to random initialization: {model_name}\")\n",
    "                    model = timm.create_model(model_name, pretrained=False, num_classes=self.num_classes)\n",
    "                    initialization_type = \"random\"\n",
    "                    final_model_name = model_name\n",
    "                    print(f\"         ✅ SUCCESS with random initialization!\")\n",
    "                except Exception as e3:\n",
    "                    # Try alternatives with random initialization\n",
    "                    for alt_name in alternatives:\n",
    "                        try:\n",
    "                            print(f\"         🎲 Trying alternative random: {alt_name}\")\n",
    "                            model = timm.create_model(alt_name, pretrained=False, num_classes=self.num_classes)\n",
    "                            initialization_type = \"random_alt\"\n",
    "                            final_model_name = alt_name\n",
    "                            print(f\"         ✅ SUCCESS with alternative random initialization!\")\n",
    "                            break\n",
    "                        except Exception as e4:\n",
    "                            continue\n",
    "                    \n",
    "                    # If still failed, return None\n",
    "                    if model is None:\n",
    "                        print(f\"         ❌ COMPLETE FAILURE: All strategies failed\")\n",
    "                        return None, None, None\n",
    "        \n",
    "        # Apply freezing strategy\n",
    "        if freeze_strategy == 'backbone':\n",
    "            print(f\"         🧊 Freezing backbone layers...\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'classifier' not in name and 'head' not in name and 'fc' not in name:\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"         📊 Loaded: {total_params:,} total, {trainable_params:,} trainable ({initialization_type})\")\n",
    "        \n",
    "        elif freeze_strategy == 'partial':\n",
    "            print(f\"         ❄️ Partial freeze (last 30% unfrozen)...\")\n",
    "            all_params = list(model.named_parameters())\n",
    "            total_layers = len(all_params)\n",
    "            freeze_until = int(total_layers * 0.7)\n",
    "            \n",
    "            for i, (name, param) in enumerate(all_params):\n",
    "                if i < freeze_until and 'classifier' not in name and 'head' not in name and 'fc' not in name:\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"         📊 Loaded: {total_params:,} total, {trainable_params:,} trainable ({initialization_type})\")\n",
    "        \n",
    "        else:  # no freezing\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"         📊 Loaded: {total_params:,} parameters (all trainable, {initialization_type})\")\n",
    "        \n",
    "        model = model.to(self.device)\n",
    "        return model, initialization_type, final_model_name\n",
    "    \n",
    "    def ultra_aggressive_training(self, model, arch_name, train_loader, val_loader, test_loader, strategy='unfrozen'):\n",
    "        \"\"\"Ultra aggressive training with minimal early stopping\"\"\"\n",
    "        print(f\"      🔥 ULTRA AGGRESSIVE TRAINING: {arch_name} ({strategy})...\")\n",
    "        \n",
    "        # More aggressive setup\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Strategy-specific hyperparameters\n",
    "        if strategy == 'frozen':\n",
    "            lr = 0.01\n",
    "            max_epochs = 100\n",
    "            patience = 50\n",
    "        elif strategy == 'partial':\n",
    "            lr = 0.005\n",
    "            max_epochs = 100\n",
    "            patience = 50\n",
    "        else:  # unfrozen\n",
    "            lr = 0.001\n",
    "            max_epochs = 100\n",
    "            patience = 50\n",
    "        \n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=0.01,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        # Simple step scheduler\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"         📊 ULTRA SETUP: {max_epochs} epochs, LR={lr}, patience={patience}\")\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            # Very lenient early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                improvement = \"🔥\"\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                improvement = \"\"\n",
    "            \n",
    "            # More frequent progress updates\n",
    "            if epoch % 5 == 0 or epoch < 10 or improvement or epoch > max_epochs - 10:\n",
    "                gap = train_acc - val_acc\n",
    "                print(f\"         Epoch {epoch:3d}: Train {train_acc:.1f}%, Val {val_acc:.1f}% (gap: {gap:+.1f}%), LR: {current_lr:.2e} {improvement}\")\n",
    "        \n",
    "        # Restore best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Test evaluation\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        print(f\"         🎯 {arch_name} ({strategy}) FINAL: Val {best_val_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "        \n",
    "        return best_val_acc, test_acc\n",
    "    \n",
    "    def test_architecture_with_multiple_strategies(self, arch_name, arch_info, train_loader, val_loader, test_loader):\n",
    "        \"\"\"Test architecture with multiple training strategies and fallback support\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Strategy 1: Frozen backbone (fast warmup)\n",
    "        print(f\"      🧊 FROZEN BACKBONE STRATEGY:\")\n",
    "        model_frozen, init_type_frozen, final_name_frozen = self.create_model_with_fallback(arch_name, arch_info, freeze_strategy='backbone')\n",
    "        if model_frozen is not None:\n",
    "            try:\n",
    "                val_acc_frozen, test_acc_frozen = self.ultra_aggressive_training(\n",
    "                    model_frozen, arch_name, train_loader, val_loader, test_loader, strategy='frozen'\n",
    "                )\n",
    "                results.append({\n",
    "                    'name': f\"{arch_name}-Frozen\",\n",
    "                    'strategy': 'frozen',\n",
    "                    'val_accuracy': val_acc_frozen,\n",
    "                    'test_accuracy': test_acc_frozen,\n",
    "                    'family': arch_info['family'],\n",
    "                    'initialization': init_type_frozen,\n",
    "                    'final_model_name': final_name_frozen,\n",
    "                    'original_model_name': arch_info['model_name']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"         ❌ Frozen strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        # Strategy 2: Partial freeze (if frozen worked reasonably)\n",
    "        if results and results[-1]['val_accuracy'] > 35:\n",
    "            print(f\"      ❄️ PARTIAL FREEZE STRATEGY:\")\n",
    "            model_partial, init_type_partial, final_name_partial = self.create_model_with_fallback(arch_name, arch_info, freeze_strategy='partial')\n",
    "            if model_partial is not None:\n",
    "                try:\n",
    "                    val_acc_partial, test_acc_partial = self.ultra_aggressive_training(\n",
    "                        model_partial, arch_name, train_loader, val_loader, test_loader, strategy='partial'\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'name': f\"{arch_name}-Partial\",\n",
    "                        'strategy': 'partial',\n",
    "                        'val_accuracy': val_acc_partial,\n",
    "                        'test_accuracy': test_acc_partial,\n",
    "                        'family': arch_info['family'],\n",
    "                        'initialization': init_type_partial,\n",
    "                        'final_model_name': final_name_partial,\n",
    "                        'original_model_name': arch_info['model_name']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"         ❌ Partial strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        # Strategy 3: Full unfrozen (if partial worked well)\n",
    "        if results and max(r['val_accuracy'] for r in results) > 45:\n",
    "            print(f\"      🔥 FULL UNFROZEN STRATEGY:\")\n",
    "            model_unfrozen, init_type_unfrozen, final_name_unfrozen = self.create_model_with_fallback(arch_name, arch_info, freeze_strategy='none')\n",
    "            if model_unfrozen is not None:\n",
    "                try:\n",
    "                    val_acc_unfrozen, test_acc_unfrozen = self.ultra_aggressive_training(\n",
    "                        model_unfrozen, arch_name, train_loader, val_loader, test_loader, strategy='unfrozen'\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'name': f\"{arch_name}-Unfrozen\",\n",
    "                        'strategy': 'unfrozen',\n",
    "                        'val_accuracy': val_acc_unfrozen,\n",
    "                        'test_accuracy': test_acc_unfrozen,\n",
    "                        'family': arch_info['family'],\n",
    "                        'initialization': init_type_unfrozen,\n",
    "                        'final_model_name': final_name_unfrozen,\n",
    "                        'original_model_name': arch_info['model_name']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"         ❌ Unfrozen strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_complete_pipeline(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        \"\"\"Run the complete pipeline with result storage\"\"\"\n",
    "        print(\"🔥 COMPLETE DEER AGING PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"🎯 Starting from EfficientNet-B5 onwards\")\n",
    "        print(\"🎯 All results will be saved automatically\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = DeerDataset(X_train, y_train)\n",
    "        val_dataset = DeerDataset(X_val, y_val)\n",
    "        test_dataset = DeerDataset(X_test, y_test)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "        \n",
    "        print(f\"📊 Data ready: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n",
    "        \n",
    "        # Get all architectures\n",
    "        architectures = self.get_all_architectures_with_fallback()\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\n🔥 ULTRA AGGRESSIVE TESTING: {len(architectures)} ARCHITECTURES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, (arch_name, arch_info) in enumerate(architectures.items(), 1):\n",
    "            print(f\"\\n[{i}/{len(architectures)}] 🔥 ULTRA AGGRESSIVE {arch_name}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Test with multiple strategies\n",
    "            arch_results = self.test_architecture_with_multiple_strategies(\n",
    "                arch_name, arch_info, train_loader, val_loader, test_loader\n",
    "            )\n",
    "            \n",
    "            # Add metadata and timing\n",
    "            for result in arch_results:\n",
    "                result['architecture_family'] = arch_info['family']\n",
    "                result['training_time'] = time.time() - start_time\n",
    "                result['timestamp'] = datetime.now().isoformat()\n",
    "                self.all_results.append(result)\n",
    "            \n",
    "            if arch_results:\n",
    "                best_arch_result = max(arch_results, key=lambda x: x['test_accuracy'])\n",
    "                print(f\"      🏆 Best {arch_name}: {best_arch_result['name']} ({best_arch_result['test_accuracy']:.1f}%)\")\n",
    "            \n",
    "            print(f\"      ⏱️ Total time for {arch_name}: {time.time() - start_time:.1f}s\")\n",
    "            \n",
    "            # Save progress text file and backup files after each architecture\n",
    "            elapsed_time = time.time() - total_start_time\n",
    "            progress_file = self.save_progress_text_file(i, len(architectures), arch_name, arch_results, elapsed_time)\n",
    "            json_file, pkl_file = self.save_backup_files(i)\n",
    "            \n",
    "            print(f\"      💾 Backups: {json_file}, {pkl_file}\")\n",
    "            \n",
    "            # Intermediate leaderboard every 3 architectures\n",
    "            if i % 3 == 0:\n",
    "                self.show_intermediate_leaderboard(i)\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        \n",
    "        # Save final results\n",
    "        self.save_final_results(total_time)\n",
    "        \n",
    "        # Display final leaderboard\n",
    "        self.show_final_leaderboard(total_time)\n",
    "        \n",
    "        return self.all_results\n",
    "    \n",
    "    def save_progress_text_file(self, completed_count, total_count, arch_name, arch_results, total_time_so_far):\n",
    "        \"\"\"Save human-readable progress text file after each architecture\n",
    "        \n",
    "        Creates files like:\n",
    "        🔥 DEER AGING PIPELINE - PROGRESS REPORT\n",
    "        ================================================================================\n",
    "        📅 Timestamp: 2025-06-11 14:23:45\n",
    "        ⏱️  Runtime so far: 2.34 hours\n",
    "        📊 Progress: 5/32 architectures completed\n",
    "        🔧 Device: cuda\n",
    "        🎯 Classes: 5\n",
    "        \n",
    "        🏁 JUST COMPLETED: EfficientNet-B5\n",
    "        --------------------------------------------------\n",
    "        EfficientNet-B5-Frozen        | frozen   | Pretrained   | Val:  52.1% | Test:  48.3%\n",
    "        EfficientNet-B5-Partial       | partial  | Pretrained   | Val:  58.7% | Test:  55.2%\n",
    "        🏆 Best: EfficientNet-B5-Partial (55.2%)\n",
    "        \n",
    "        🏆 CURRENT TOP 10 LEADERBOARD\n",
    "        --------------------------------------------------------------------------------\n",
    "        Rank Model                          Strategy   Init         Val%     Test%\n",
    "        --------------------------------------------------------------------------------\n",
    "        1    EfficientNet-B4-Unfrozen       unfrozen   Pretrained   61.2     58.9\n",
    "        2    EfficientNet-B5-Partial        partial    Pretrained   58.7     55.2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'deer_aging_progress_{timestamp}.txt'\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"🔥 DEER AGING PIPELINE - PROGRESS REPORT\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"📅 Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"⏱️  Runtime so far: {total_time_so_far/3600:.2f} hours\\n\")\n",
    "            f.write(f\"📊 Progress: {completed_count}/{total_count} architectures completed\\n\")\n",
    "            f.write(f\"🔧 Device: {self.device}\\n\")\n",
    "            f.write(f\"🎯 Classes: {self.num_classes}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            # Just completed architecture results\n",
    "            f.write(f\"🏁 JUST COMPLETED: {arch_name}\\n\")\n",
    "            f.write(\"-\"*50 + \"\\n\")\n",
    "            if arch_results:\n",
    "                for result in arch_results:\n",
    "                    init_type = result.get('initialization', 'unknown')\n",
    "                    f.write(f\"   {result['name']:30} | {result['strategy']:8} | {init_type:12} | Val: {result['val_accuracy']:5.1f}% | Test: {result['test_accuracy']:5.1f}%\\n\")\n",
    "                \n",
    "                best_arch = max(arch_results, key=lambda x: x['test_accuracy'])\n",
    "                f.write(f\"   🏆 Best: {best_arch['name']} ({best_arch['test_accuracy']:.1f}%)\\n\")\n",
    "            else:\n",
    "                f.write(\"   ❌ No successful results for this architecture\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Current overall leaderboard (top 10)\n",
    "            sorted_results = sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "            f.write(f\"🏆 CURRENT TOP 10 LEADERBOARD\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(f\"{'Rank':<4} {'Model':<30} {'Strategy':<10} {'Init':<12} {'Val%':<8} {'Test%':<8}\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            \n",
    "            for i, result in enumerate(sorted_results[:10], 1):\n",
    "                init_type = result.get('initialization', 'unknown')\n",
    "                if init_type == 'pretrained':\n",
    "                    init_display = \"Pretrained\"\n",
    "                elif init_type == 'pretrained_alt':\n",
    "                    init_display = \"Alt-Pre\"\n",
    "                elif init_type == 'random':\n",
    "                    init_display = \"Random\"\n",
    "                elif init_type == 'random_alt':\n",
    "                    init_display = \"Alt-Rand\"\n",
    "                else:\n",
    "                    init_display = \"Unknown\"\n",
    "                \n",
    "                f.write(f\"{i:<4} {result['name']:<30} {result['strategy']:<10} {init_display:<12} {result['val_accuracy']:<7.1f} {result['test_accuracy']:<7.1f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            if sorted_results:\n",
    "                best_overall = sorted_results[0]\n",
    "                breakthrough_count = sum(1 for r in sorted_results if r['test_accuracy'] > 54.2)\n",
    "                excellent_count = sum(1 for r in sorted_results if r['test_accuracy'] >= 65.0)\n",
    "                \n",
    "                f.write(f\"📊 SUMMARY STATISTICS\\n\")\n",
    "                f.write(\"-\"*50 + \"\\n\")\n",
    "                f.write(f\"🏆 Current Best: {best_overall['name']} ({best_overall['test_accuracy']:.1f}%)\\n\")\n",
    "                f.write(f\"🚀 Models beating 54.2% baseline: {breakthrough_count}/{len(sorted_results)}\\n\")\n",
    "                f.write(f\"🎉 Models achieving 65%+: {excellent_count}\\n\")\n",
    "                f.write(f\"📈 Total models tested: {len(sorted_results)}\\n\")\n",
    "                \n",
    "                # Analysis by initialization type\n",
    "                f.write(f\"\\n📊 BY INITIALIZATION TYPE:\\n\")\n",
    "                init_groups = {}\n",
    "                for result in sorted_results:\n",
    "                    init_type = result.get('initialization', 'unknown')\n",
    "                    if init_type not in init_groups:\n",
    "                        init_groups[init_type] = []\n",
    "                    init_groups[init_type].append(result)\n",
    "                \n",
    "                for init_type, group in init_groups.items():\n",
    "                    avg_test = sum(r['test_accuracy'] for r in group) / len(group)\n",
    "                    best_test = max(r['test_accuracy'] for r in group)\n",
    "                    f.write(f\"   {init_type:15}: {len(group):2d} models, avg: {avg_test:.1f}%, best: {best_test:.1f}%\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"🔄 REMAINING: {total_count - completed_count} architectures to test\\n\")\n",
    "            estimated_time_remaining = (total_time_so_far / completed_count) * (total_count - completed_count) if completed_count > 0 else 0\n",
    "            f.write(f\"⏰ Estimated time remaining: {estimated_time_remaining/3600:.1f} hours\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"📁 This file: {filename}\\n\")\n",
    "            f.write(f\"💾 Auto-saved at: {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
    "        \n",
    "        print(f\"      📝 Progress saved: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def save_final_results(self, total_time):\n",
    "        \"\"\"Save comprehensive final results\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Prepare comprehensive results\n",
    "        final_data = {\n",
    "            'experiment_info': {\n",
    "                'timestamp': timestamp,\n",
    "                'total_runtime_hours': total_time / 3600,\n",
    "                'total_models_tested': len(self.all_results),\n",
    "                'device': str(self.device),\n",
    "                'num_classes': self.num_classes\n",
    "            },\n",
    "            'results': self.all_results,\n",
    "            'leaderboard': sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        }\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open(f'deer_aging_final_results_{timestamp}.json', 'w') as f:\n",
    "            json.dump(final_data, f, indent=2)\n",
    "        \n",
    "        # Save as pickle\n",
    "        with open(f'deer_aging_final_results_{timestamp}.pkl', 'wb') as f:\n",
    "            pickle.dump(final_data, f)\n",
    "        \n",
    "        print(f\"\\n💾 RESULTS SAVED:\")\n",
    "        print(f\"   📁 deer_aging_final_results_{timestamp}.json\")\n",
    "        print(f\"   📁 deer_aging_final_results_{timestamp}.pkl\")\n",
    "    \n",
    "    def show_intermediate_leaderboard(self, completed_count):\n",
    "        \"\"\"Show intermediate leaderboard\"\"\"\n",
    "        current_best = sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)[:5]\n",
    "        print(f\"\\n📊 CURRENT TOP 5 (after {completed_count} architectures):\")\n",
    "        for j, result in enumerate(current_best, 1):\n",
    "            print(f\"   {j}. {result['name']}: {result['test_accuracy']:.1f}%\")\n",
    "        print()\n",
    "    \n",
    "    def show_final_leaderboard(self, total_time):\n",
    "        \"\"\"Show comprehensive final leaderboard\"\"\"\n",
    "        # Sort all results\n",
    "        sorted_results = sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        \n",
    "        print(f\"\\n🏆 FINAL COMPREHENSIVE RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"⏰ Total testing time: {total_time/3600:.1f} hours\")\n",
    "        print(f\"🎯 Models tested: {len(self.all_results)}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Rank':<4} {'Model':<30} {'Strategy':<10} {'Init':<12} {'Val%':<8} {'Test%':<8} {'Status'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(sorted_results, 1):\n",
    "            val_acc = result['val_accuracy']\n",
    "            test_acc = result['test_accuracy']\n",
    "            strategy = result['strategy']\n",
    "            init_type = result.get('initialization', 'unknown')\n",
    "            \n",
    "            # Format initialization type for display\n",
    "            if init_type == 'pretrained':\n",
    "                init_display = \"🎯 Pretrained\"\n",
    "            elif init_type == 'pretrained_alt':\n",
    "                init_display = \"🎯 Alt-Pre\"\n",
    "            elif init_type == 'random':\n",
    "                init_display = \"🎲 Random\"\n",
    "            elif init_type == 'random_alt':\n",
    "                init_display = \"🎲 Alt-Rand\"\n",
    "            else:\n",
    "                init_display = \"❓ Unknown\"\n",
    "            \n",
    "            if test_acc >= 75.0:\n",
    "                status = \"🎉 BREAKTHROUGH!\"\n",
    "            elif test_acc >= 65.0:\n",
    "                status = \"🔥 EXCELLENT!\"\n",
    "            elif test_acc > 54.2:\n",
    "                status = \"🚀 NEW BEST!\"\n",
    "            elif test_acc > 45.0:\n",
    "                status = \"📈 Good\"\n",
    "            else:\n",
    "                status = \"📉 Weak\"\n",
    "            \n",
    "            print(f\"{i:<4} {result['name']:<30} {strategy:<10} {init_display:<12} {val_acc:<7.1f} {test_acc:<7.1f} {status}\")\n",
    "        \n",
    "        # Additional analysis by initialization type\n",
    "        print(f\"\\n📊 ANALYSIS BY INITIALIZATION:\")\n",
    "        init_groups = {}\n",
    "        for result in sorted_results:\n",
    "            init_type = result.get('initialization', 'unknown')\n",
    "            if init_type not in init_groups:\n",
    "                init_groups[init_type] = []\n",
    "            init_groups[init_type].append(result)\n",
    "        \n",
    "        for init_type, group in init_groups.items():\n",
    "            avg_test = sum(r['test_accuracy'] for r in group) / len(group)\n",
    "            best_test = max(r['test_accuracy'] for r in group)\n",
    "            print(f\"   {init_type:15}: {len(group):2d} models, avg: {avg_test:.1f}%, best: {best_test:.1f}%\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        if sorted_results:\n",
    "            best = sorted_results[0]\n",
    "            breakthrough_count = sum(1 for r in sorted_results if r['test_accuracy'] > 54.2)\n",
    "            excellent_count = sum(1 for r in sorted_results if r['test_accuracy'] >= 65.0)\n",
    "            \n",
    "            print(f\"\\n🎊 FINAL SUMMARY:\")\n",
    "            print(f\"   🏆 ULTIMATE CHAMPION: {best['name']} ({best['test_accuracy']:.1f}%)\")\n",
    "            print(f\"   🚀 Beat 54.2% baseline: {breakthrough_count}/{len(sorted_results)} models\")\n",
    "            print(f\"   🎉 Achieved 65%+: {excellent_count} models\")\n",
    "            \n",
    "            if best['test_accuracy'] >= 75.0:\n",
    "                print(f\"   🎉 MISSION ACCOMPLISHED! Achieved 75%+ accuracy!\")\n",
    "            elif best['test_accuracy'] >= 65.0:\n",
    "                print(f\"   🎊 EXCELLENT! Found 65%+ architecture!\")\n",
    "            elif best['test_accuracy'] > 54.2:\n",
    "                improvement = best['test_accuracy'] - 54.2\n",
    "                print(f\"   🚀 SUCCESS! Improved by +{improvement:.1f}% over baseline!\")\n",
    "            \n",
    "            # Analysis of initialization types\n",
    "            best_pretrained = max([r for r in sorted_results if r.get('initialization', '').startswith('pretrained')], \n",
    "                                key=lambda x: x['test_accuracy'], default=None)\n",
    "            best_random = max([r for r in sorted_results if r.get('initialization', '').startswith('random')], \n",
    "                            key=lambda x: x['test_accuracy'], default=None)\n",
    "            \n",
    "            if best_pretrained and best_random:\n",
    "                print(f\"   🎯 Best Pretrained: {best_pretrained['name']} ({best_pretrained['test_accuracy']:.1f}%)\")\n",
    "                print(f\"   🎲 Best Random Init: {best_random['name']} ({best_random['test_accuracy']:.1f}%)\")\n",
    "                if best_random['test_accuracy'] > best_pretrained['test_accuracy']:\n",
    "                    print(f\"   🔥 SURPRISE! Random initialization outperformed pretrained!\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "\n",
    "def run_complete_deer_aging_pipeline():\n",
    "    \"\"\"Run the complete deer aging pipeline from start to finish\"\"\"\n",
    "    print(\"🚀 LAUNCHING COMPLETE DEER AGING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"📋 PIPELINE STEPS:\")\n",
    "    print(\"   1. Load original 357 images\")\n",
    "    print(\"   2. Create train/val/test splits\")\n",
    "    print(\"   3. Balance and augment training data\")\n",
    "    print(\"   4. Test all architectures (starting from EfficientNet-B5)\")\n",
    "    print(\"   5. Save results and create leaderboard\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"💾 CRASH RECOVERY: Progress saved after each model!\")\n",
    "    print(\"📝 Look for 'deer_aging_progress_*.txt' files for latest results\")\n",
    "    print(\"💼 JSON/pickle backups: 'deer_aging_backup_*.json/.pkl'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        images, ages = load_original_data()\n",
    "        \n",
    "        # Step 2: Create splits\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, label_mapping, reverse_mapping = create_train_val_test_split(images, ages)\n",
    "        \n",
    "        # Step 3: Augment data\n",
    "        X_train_aug, y_train_aug = balance_and_augment_data(X_train, y_train, augment_multiplier=30, num_classes=len(label_mapping))\n",
    "        \n",
    "        # Step 4: Run complete testing\n",
    "        trainer = CompleteDeerAgeTrainer(num_classes=len(label_mapping))\n",
    "        results = trainer.run_complete_pipeline(X_train_aug, y_train_aug, X_val, y_val, X_test, y_test)\n",
    "        \n",
    "        print(\"\\n🎉 PIPELINE COMPLETE!\")\n",
    "        print(\"📁 All results saved with timestamps\")\n",
    "        print(\"🏆 Check the final leaderboard above\")\n",
    "        \n",
    "        return results, label_mapping, reverse_mapping\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️  INTERRUPTED BY USER\")\n",
    "        print(\"📝 Check latest 'deer_aging_progress_*.txt' file for current results\")\n",
    "        print(\"💾 Backup files saved as 'deer_aging_backup_*.json/.pkl'\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ PIPELINE CRASHED: {str(e)}\")\n",
    "        print(\"📝 Check latest 'deer_aging_progress_*.txt' file for results up to crash\")\n",
    "        print(\"💾 Backup files saved as 'deer_aging_backup_*.json/.pkl'\")\n",
    "        print(\"🔄 You can manually load the pickle files to recover results\")\n",
    "        raise\n",
    "\n",
    "# 🔄 CRASH RECOVERY FUNCTION\n",
    "def load_latest_results():\n",
    "    \"\"\"Load the most recent backup results (for crash recovery)\"\"\"\n",
    "    import glob\n",
    "    import os\n",
    "    \n",
    "    print(\"🔄 CRASH RECOVERY MODE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Find latest backup files\n",
    "    pickle_files = glob.glob('deer_aging_backup_*.pkl')\n",
    "    if not pickle_files:\n",
    "        print(\"❌ No backup files found!\")\n",
    "        return None\n",
    "    \n",
    "    # Get the most recent file\n",
    "    latest_pickle = max(pickle_files, key=os.path.getctime)\n",
    "    print(f\"📁 Loading latest backup: {latest_pickle}\")\n",
    "    \n",
    "    # Load the results\n",
    "    with open(latest_pickle, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    \n",
    "    print(f\"✅ Loaded {len(results)} results\")\n",
    "    \n",
    "    # Show quick summary\n",
    "    if results:\n",
    "        sorted_results = sorted(results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        print(f\"🏆 Best model: {sorted_results[0]['name']} ({sorted_results[0]['test_accuracy']:.1f}%)\")\n",
    "        print(f\"📊 Models tested: {len(results)}\")\n",
    "        \n",
    "        # Show latest progress file\n",
    "        progress_files = glob.glob('deer_aging_progress_*.txt')\n",
    "        if progress_files:\n",
    "            latest_progress = max(progress_files, key=os.path.getctime)\n",
    "            print(f\"📝 Latest progress report: {latest_progress}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 🔥 RUN COMPLETE PIPELINE\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔥 LAUNCHING COMPLETE DEER AGING PIPELINE...\")\n",
    "    print(\"⚠️  Starting from EfficientNet-B5 (as requested)\")\n",
    "    print(\"🎯 TESTING ALL MODELS: Pretrained → Alternatives → Random Init\")\n",
    "    print(\"💾 All results will be automatically saved\")\n",
    "    print(\"📊 Will show which models used pretrained vs random initialization\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"💡 CRASH RECOVERY TIPS:\")\n",
    "    print(\"   📝 Progress files: 'deer_aging_progress_YYYYMMDD_HHMMSS.txt'\")\n",
    "    print(\"   💾 Backup files: 'deer_aging_backup_*.pkl'\") \n",
    "    print(\"   🔄 To recover: results = load_latest_results()\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    final_results, final_label_mapping, final_reverse_mapping = run_complete_deer_aging_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a316f97-c111-4262-8bef-f33af9d68e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
