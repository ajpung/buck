{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1384c-dbe8-43d5-91f0-46fda71fa2e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CUDA Diagnosis Script\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def diagnose_cuda():\n",
    "    print(\"CUDA DIAGNOSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check PyTorch version\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(\"✅ CUDA is working!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ CUDA not available\")\n",
    "        \n",
    "        # Check if CUDA is compiled into PyTorch\n",
    "        print(f\"CUDA compiled into PyTorch: {torch.backends.cudnn.enabled if hasattr(torch.backends, 'cudnn') else 'Unknown'}\")\n",
    "        \n",
    "        # Try to detect NVIDIA GPU\n",
    "        try:\n",
    "            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(\"✅ NVIDIA GPU detected via nvidia-smi\")\n",
    "                print(\"❌ But PyTorch can't access it - need CUDA-enabled PyTorch\")\n",
    "            else:\n",
    "                print(\"❌ No NVIDIA GPU detected\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"❌ nvidia-smi not found - CUDA drivers may not be installed\")\n",
    "        \n",
    "        return False\n",
    "\n",
    "def get_fix_commands():\n",
    "    \"\"\"Get the right PyTorch installation commands\"\"\"\n",
    "    print(\"\\nFIX COMMANDS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check current environment\n",
    "    print(\"1. UNINSTALL CURRENT PYTORCH:\")\n",
    "    print(\"   pip uninstall torch torchvision torchaudio -y\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. INSTALL CUDA-ENABLED PYTORCH:\")\n",
    "    print(\"   For RTX 40-series, RTX 30-series, or newer:\")\n",
    "    print(\"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
    "    print()\n",
    "    print(\"   For older GPUs (GTX 10-series, RTX 20-series):\")\n",
    "    print(\"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. VERIFY INSTALLATION:\")\n",
    "    print(\"   python -c \\\"import torch; print(f'CUDA: {torch.cuda.is_available()}')\\\"\")\n",
    "    print()\n",
    "    \n",
    "    print(\"4. IF STILL ISSUES:\")\n",
    "    print(\"   - Check NVIDIA drivers: nvidia-smi\")\n",
    "    print(\"   - Update drivers from NVIDIA website\")\n",
    "    print(\"   - Restart after driver update\")\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"Quick test if CUDA works\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            # Test basic CUDA operations\n",
    "            x = torch.randn(100, 100).cuda()\n",
    "            y = torch.randn(100, 100).cuda()\n",
    "            z = torch.mm(x, y)\n",
    "            print(\"✅ CUDA tensor operations working!\")\n",
    "            \n",
    "            # Test model creation\n",
    "            import timm\n",
    "            model = timm.create_model('resnet18', pretrained=True, num_classes=5)\n",
    "            model = model.cuda()\n",
    "            \n",
    "            # Test forward pass\n",
    "            test_input = torch.randn(1, 3, 224, 224).cuda()\n",
    "            with torch.no_grad():\n",
    "                output = model(test_input)\n",
    "            \n",
    "            print(\"✅ CUDA model inference working!\")\n",
    "            print(f\"🚀 Ready for GPU training!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ CUDA test failed: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"❌ Cannot test - CUDA not available\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cuda_works = diagnose_cuda()\n",
    "    \n",
    "    if not cuda_works:\n",
    "        get_fix_commands()\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"IMPORTANT: After installing CUDA PyTorch:\")\n",
    "        print(\"1. Restart your Python kernel/notebook\")\n",
    "        print(\"2. Re-run your training script\")\n",
    "        print(\"3. Should see 'Device: cuda' instead of 'Device: cpu'\")\n",
    "        print(\"4. Training will be 5-10x faster!\")\n",
    "    else:\n",
    "        quick_test()\n",
    "        print(\"\\n🎯 CUDA is ready - your training should be using GPU!\")\n",
    "        print(\"If your script still shows CPU, restart your Python kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995fdd53-1881-4491-9bc0-0d58eb5df549",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 95% accuracy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mixed precision imports\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    MIXED_PRECISION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXED_PRECISION_AVAILABLE = False\n",
    "    class autocast:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "def load_original_data():\n",
    "    \"\"\"Load the original images\"\"\"\n",
    "    try:\n",
    "        from buck.analysis.basics import ingest_images\n",
    "        fpath = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "        images, ages = ingest_images(fpath)\n",
    "        ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "        print(f\"Loaded {len(images)} images\")\n",
    "        print(f\"Distribution: {dict(Counter(ages_grouped))}\")\n",
    "        return images, ages_grouped\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        raise\n",
    "\n",
    "def enhanced_augment_image(image):\n",
    "    \"\"\"Optimized augmentation for deer aging\"\"\"\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Core augmentations that preserve antler features\n",
    "    if random.random() < 0.7:\n",
    "        angle = random.uniform(-12, 12)  # Reduced rotation to preserve antler orientation\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Enhanced lighting (important for outdoor deer photos)\n",
    "    if random.random() < 0.8:\n",
    "        alpha = random.uniform(0.75, 1.25)\n",
    "        beta = random.randint(-20, 20)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Gamma correction for lighting conditions\n",
    "    if random.random() < 0.4:\n",
    "        gamma = random.uniform(0.8, 1.2)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    # Realistic noise\n",
    "    if random.random() < 0.3:\n",
    "        noise = np.random.normal(0, 6, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_optimized_augmented_data(X_train, y_train, multiplier=40):\n",
    "    \"\"\"Create balanced augmented data\"\"\"\n",
    "    print(f\"OPTIMIZED AUGMENTATION ({multiplier}x)\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    class_counts = Counter(y_train)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max_count * multiplier\n",
    "    \n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    \n",
    "    for class_idx in range(len(set(y_train))):\n",
    "        class_mask = y_train == class_idx\n",
    "        class_images = X_train[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        print(f\"   Class {class_idx}: {current_count} -> {target_count}\")\n",
    "        \n",
    "        # Add originals 4 times to preserve signal\n",
    "        for _ in range(4):\n",
    "            X_aug.extend(class_images)\n",
    "            y_aug.extend([class_idx] * current_count)\n",
    "        \n",
    "        # Generate remaining augmented samples\n",
    "        needed = target_count - (current_count * 4)\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image(class_images[orig_idx].copy())\n",
    "            X_aug.append(aug_img)\n",
    "            y_aug.append(class_idx)\n",
    "    \n",
    "    print(f\"   Total: {len(X_aug)} samples\")\n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "class OptimizedDeerDataset(Dataset):\n",
    "    \"\"\"Optimized dataset for deer aging\"\"\"\n",
    "    def __init__(self, X, y, test_time_aug=False):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.test_time_aug = test_time_aug\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Normalize\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # CHW format and resize\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        if image.shape[-2:] != (224, 224):\n",
    "            image = F.interpolate(image.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "        \n",
    "        # Test-time augmentation\n",
    "        if self.test_time_aug and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        # Normalize\n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class CrossValidationTrainer:\n",
    "    \"\"\"Cross-validation trainer for reliable 70% test accuracy\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(f\"CROSS-VALIDATION TRAINER - TARGET 70%\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            if MIXED_PRECISION_AVAILABLE:\n",
    "                self.scaler = GradScaler()\n",
    "                self.use_amp = True\n",
    "                print(f\"   Mixed Precision: Enabled\")\n",
    "            else:\n",
    "                self.use_amp = False\n",
    "        else:\n",
    "            self.use_amp = False\n",
    "    \n",
    "    def create_optimal_model(self):\n",
    "        \"\"\"Create your best-performing ResNet-18 configuration\"\"\"\n",
    "        model = timm.create_model('resnet18', pretrained=True, num_classes=self.num_classes)\n",
    "        \n",
    "        # Your optimal freezing strategy (75% trainable)\n",
    "        frozen_layers = ['conv1', 'bn1', 'layer1', 'layer2', 'layer3']\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            for frozen_layer in frozen_layers:\n",
    "                if name.startswith(frozen_layer):\n",
    "                    param.requires_grad = False\n",
    "                    break\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def train_single_model(self, train_loader, val_loader, model_name=\"model\"):\n",
    "        \"\"\"Train a single model with optimal hyperparameters\"\"\"\n",
    "        model = self.create_optimal_model()\n",
    "        \n",
    "        # Optimized hyperparameters based on your best results\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Differential learning rates\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'fc' in name:\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': 0.0003},\n",
    "            {'params': classifier_params, 'lr': 0.001}\n",
    "        ], weight_decay=0.015)\n",
    "        \n",
    "        # Cosine annealing\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=70, eta_min=1e-6)\n",
    "        \n",
    "        max_epochs = 70\n",
    "        patience = 20\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"   Training {model_name}: {max_epochs} epochs, patience={patience}\")\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.step(optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    if self.use_amp:\n",
    "                        with autocast():\n",
    "                            outputs = model(images)\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track best\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "                status = \"BEST\"\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                status = \"\"\n",
    "            \n",
    "            # Progress (less frequent printing)\n",
    "            if epoch % 10 == 0 or epoch < 3 or status or epoch > max_epochs - 3:\n",
    "                print(f\"     Epoch {epoch:2d}: Train {train_acc:.1f}%, Val {val_acc:.1f}% {status}\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"     Early stop at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Restore best model\n",
    "        if 'best_state' in locals():\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        print(f\"   {model_name} complete: Best val {best_val_acc:.1f}%\")\n",
    "        return model, best_val_acc\n",
    "    \n",
    "    def evaluate_with_tta(self, model, test_loader):\n",
    "        \"\"\"Evaluate model with test-time augmentation\"\"\"\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Original prediction\n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs1 = model(images)\n",
    "                else:\n",
    "                    outputs1 = model(images)\n",
    "                \n",
    "                # Flipped prediction\n",
    "                flipped = torch.flip(images, [3])\n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs2 = model(flipped)\n",
    "                else:\n",
    "                    outputs2 = model(flipped)\n",
    "                \n",
    "                # Average (TTA)\n",
    "                avg_outputs = (outputs1 + outputs2) / 2\n",
    "                _, predicted = torch.max(avg_outputs, 1)\n",
    "                \n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return 100 * test_correct / test_total\n",
    "    \n",
    "    def run_cross_validation(self, images, ages, n_splits=5):\n",
    "        \"\"\"Run k-fold cross-validation to find best approach\"\"\"\n",
    "        print(f\"\\n{n_splits}-FOLD CROSS-VALIDATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if not isinstance(images, np.ndarray):\n",
    "            images = np.array(images)\n",
    "        if not isinstance(ages, np.ndarray):\n",
    "            ages = np.array(ages)\n",
    "        \n",
    "        # Create label mapping\n",
    "        unique_ages = sorted(list(set(ages)))\n",
    "        label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "        y_indices = np.array([label_mapping[age] for age in ages])\n",
    "        \n",
    "        # Stratified K-Fold\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        cv_scores = []\n",
    "        best_models = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(images, y_indices)):\n",
    "            print(f\"\\nFOLD {fold + 1}/{n_splits}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Split data\n",
    "            X_train_fold = images[train_idx]\n",
    "            y_train_fold = y_indices[train_idx]\n",
    "            X_val_fold = images[val_idx]\n",
    "            y_val_fold = y_indices[val_idx]\n",
    "            \n",
    "            print(f\"   Train: {len(X_train_fold)}, Val: {len(X_val_fold)}\")\n",
    "            \n",
    "            # Augment training data\n",
    "            X_train_aug, y_train_aug = create_optimized_augmented_data(X_train_fold, y_train_fold, multiplier=40)\n",
    "            \n",
    "            # Create datasets and loaders\n",
    "            train_dataset = OptimizedDeerDataset(X_train_aug, y_train_aug)\n",
    "            val_dataset = OptimizedDeerDataset(X_val_fold, y_val_fold, test_time_aug=True)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "            \n",
    "            # Train model\n",
    "            model, val_acc = self.train_single_model(train_loader, val_loader, f\"fold-{fold+1}\")\n",
    "            \n",
    "            cv_scores.append(val_acc)\n",
    "            best_models.append(model)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return cv_scores, best_models, label_mapping\n",
    "    \n",
    "    def final_test_evaluation(self, images, ages, trained_models, label_mapping):\n",
    "        \"\"\"Final evaluation on held-out test set\"\"\"\n",
    "        print(f\"\\nFINAL TEST EVALUATION\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        if not isinstance(images, np.ndarray):\n",
    "            images = np.array(images)\n",
    "        if not isinstance(ages, np.ndarray):\n",
    "            ages = np.array(ages)\n",
    "        \n",
    "        # Convert to indices\n",
    "        y_indices = np.array([label_mapping[age] for age in ages])\n",
    "        \n",
    "        # Create train/test split (80/20)\n",
    "        X_train_all, X_test, y_train_all, y_test = train_test_split(\n",
    "            images, y_indices, test_size=0.2, random_state=42, stratify=y_indices\n",
    "        )\n",
    "        \n",
    "        print(f\"   Final test set: {len(X_test)} samples\")\n",
    "        print(f\"   Test distribution: {Counter(y_test)}\")\n",
    "        \n",
    "        # Create test dataset\n",
    "        test_dataset = OptimizedDeerDataset(X_test, y_test, test_time_aug=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "        \n",
    "        # Evaluate each model\n",
    "        individual_scores = []\n",
    "        for i, model in enumerate(trained_models):\n",
    "            test_acc = self.evaluate_with_tta(model, test_loader)\n",
    "            individual_scores.append(test_acc)\n",
    "            print(f\"   Model {i+1}: {test_acc:.1f}%\")\n",
    "        \n",
    "        # Ensemble evaluation\n",
    "        print(f\"\\n   ENSEMBLE EVALUATION:\")\n",
    "        ensemble_acc = self.evaluate_ensemble_with_tta(trained_models, test_loader)\n",
    "        print(f\"   Ensemble + TTA: {ensemble_acc:.1f}%\")\n",
    "        \n",
    "        return individual_scores, ensemble_acc\n",
    "    \n",
    "    def evaluate_ensemble_with_tta(self, models, test_loader):\n",
    "        \"\"\"Ensemble evaluation with test-time augmentation\"\"\"\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "        \n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                ensemble_outputs = torch.zeros(images.size(0), self.num_classes).to(self.device)\n",
    "                \n",
    "                for model in models:\n",
    "                    # Original\n",
    "                    if self.use_amp:\n",
    "                        with autocast():\n",
    "                            outputs1 = model(images)\n",
    "                    else:\n",
    "                        outputs1 = model(images)\n",
    "                    \n",
    "                    # Flipped\n",
    "                    flipped = torch.flip(images, [3])\n",
    "                    if self.use_amp:\n",
    "                        with autocast():\n",
    "                            outputs2 = model(flipped)\n",
    "                    else:\n",
    "                        outputs2 = model(flipped)\n",
    "                    \n",
    "                    # Average and add to ensemble\n",
    "                    avg_outputs = (outputs1 + outputs2) / 2\n",
    "                    ensemble_outputs += F.softmax(avg_outputs, dim=1)\n",
    "                \n",
    "                ensemble_outputs /= len(models)\n",
    "                _, predicted = torch.max(ensemble_outputs, 1)\n",
    "                \n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return 100 * test_correct / test_total\n",
    "\n",
    "def cross_validation_pipeline():\n",
    "    \"\"\"Cross-validation pipeline to reliably reach 70%\"\"\"\n",
    "    print(\"CROSS-VALIDATION PIPELINE - RELIABLE 70%\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Strategy: K-fold CV + Best model selection + TTA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        images, ages = load_original_data()\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = CrossValidationTrainer(num_classes=len(set(ages)))\n",
    "        \n",
    "        # Run cross-validation\n",
    "        cv_scores, trained_models, label_mapping = trainer.run_cross_validation(images, ages, n_splits=5)\n",
    "        \n",
    "        # Final test evaluation\n",
    "        individual_scores, ensemble_score = trainer.final_test_evaluation(images, ages, trained_models, label_mapping)\n",
    "        \n",
    "        # Results\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nCROSS-VALIDATION RESULTS\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"CV Scores: {[f'{score:.1f}%' for score in cv_scores]}\")\n",
    "        print(f\"CV Mean: {np.mean(cv_scores):.1f}% ± {np.std(cv_scores):.1f}%\")\n",
    "        print(f\"CV Best: {max(cv_scores):.1f}%\")\n",
    "        \n",
    "        print(f\"\\nFINAL TEST RESULTS\")\n",
    "        print(\"=\"*40)\n",
    "        for i, score in enumerate(individual_scores):\n",
    "            print(f\"Model {i+1}: {score:.1f}%\")\n",
    "        \n",
    "        best_individual = max(individual_scores)\n",
    "        print(f\"\\nBest Individual: {best_individual:.1f}%\")\n",
    "        print(f\"Ensemble + TTA:  {ensemble_score:.1f}%\")\n",
    "        print(f\"Time: {elapsed/60:.1f} minutes\")\n",
    "        \n",
    "        # Goal assessment\n",
    "        final_score = max(best_individual, ensemble_score)\n",
    "        \n",
    "        if final_score >= 70:\n",
    "            print(f\"\\n🎉 SUCCESS: REACHED 70% GOAL! ({final_score:.1f}%)\")\n",
    "        elif final_score >= 68:\n",
    "            print(f\"\\n🔥 SO CLOSE: {70 - final_score:.1f}% away from 70%\")\n",
    "            print(\"Recommendation: Try ensemble of different architectures\")\n",
    "        elif final_score >= 65:\n",
    "            print(f\"\\n📈 VERY GOOD: {70 - final_score:.1f}% away from 70%\")\n",
    "            print(\"Recommendation: Hyperparameter optimization or more data\")\n",
    "        else:\n",
    "            print(f\"\\n💪 GOOD PROGRESS: {70 - final_score:.1f}% away from 70%\")\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results = {\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': float(np.mean(cv_scores)),\n",
    "            'cv_std': float(np.std(cv_scores)),\n",
    "            'individual_test_scores': individual_scores,\n",
    "            'ensemble_test_score': ensemble_score,\n",
    "            'best_score': final_score,\n",
    "            'goal_reached': final_score >= 70,\n",
    "            'gap_to_goal': max(0, 70 - final_score),\n",
    "            'elapsed_minutes': elapsed/60,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(f'cross_validation_results_{timestamp}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults saved: cross_validation_results_{timestamp}.json\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cross_validation_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f794df-cc66-4a8c-b637-50c168276c89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENHANCED CROSS-VALIDATION PIPELINE WITH MODEL SAVING\n",
      "======================================================================\n",
      "Strategy: K-fold CV + Model saving + Comprehensive data tracking\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Full model with save\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mixed precision imports\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    MIXED_PRECISION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXED_PRECISION_AVAILABLE = False\n",
    "    class autocast:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "def load_original_data():\n",
    "    \"\"\"Load the original images\"\"\"\n",
    "    try:\n",
    "        from buck.analysis.basics import ingest_images\n",
    "        fpath = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "        images, ages = ingest_images(fpath)\n",
    "        ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "        print(f\"Loaded {len(images)} images\")\n",
    "        print(f\"Distribution: {dict(Counter(ages_grouped))}\")\n",
    "        return images, ages_grouped\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        raise\n",
    "\n",
    "def enhanced_augment_image(image):\n",
    "    \"\"\"Optimized augmentation for deer aging\"\"\"\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Core augmentations that preserve antler features\n",
    "    if random.random() < 0.7:\n",
    "        angle = random.uniform(-12, 12)  # Reduced rotation to preserve antler orientation\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Enhanced lighting (important for outdoor deer photos)\n",
    "    if random.random() < 0.8:\n",
    "        alpha = random.uniform(0.75, 1.25)\n",
    "        beta = random.randint(-20, 20)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Gamma correction for lighting conditions\n",
    "    if random.random() < 0.4:\n",
    "        gamma = random.uniform(0.8, 1.2)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    # Realistic noise\n",
    "    if random.random() < 0.3:\n",
    "        noise = np.random.normal(0, 6, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_optimized_augmented_data(X_train, y_train, multiplier=40):\n",
    "    \"\"\"Create balanced augmented data\"\"\"\n",
    "    print(f\"OPTIMIZED AUGMENTATION ({multiplier}x)\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    class_counts = Counter(y_train)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max_count * multiplier\n",
    "    \n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    \n",
    "    for class_idx in range(len(set(y_train))):\n",
    "        class_mask = y_train == class_idx\n",
    "        class_images = X_train[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        print(f\"   Class {class_idx}: {current_count} -> {target_count}\")\n",
    "        \n",
    "        # Add originals 4 times to preserve signal\n",
    "        for _ in range(4):\n",
    "            X_aug.extend(class_images)\n",
    "            y_aug.extend([class_idx] * current_count)\n",
    "        \n",
    "        # Generate remaining augmented samples\n",
    "        needed = target_count - (current_count * 4)\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image(class_images[orig_idx].copy())\n",
    "            X_aug.append(aug_img)\n",
    "            y_aug.append(class_idx)\n",
    "    \n",
    "    print(f\"   Total: {len(X_aug)} samples\")\n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "class OptimizedDeerDataset(Dataset):\n",
    "    \"\"\"Optimized dataset for deer aging\"\"\"\n",
    "    def __init__(self, X, y, test_time_aug=False):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.test_time_aug = test_time_aug\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Normalize\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # CHW format and resize\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        if image.shape[-2:] != (224, 224):\n",
    "            image = F.interpolate(image.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "        \n",
    "        # Test-time augmentation\n",
    "        if self.test_time_aug and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        # Normalize\n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class CrossValidationTrainerWithSaving:\n",
    "    \"\"\"Enhanced trainer that saves models and comprehensive training data\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5, save_dir=\"saved_models\"):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        # Create save directory\n",
    "        import os\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"ENHANCED CROSS-VALIDATION TRAINER WITH MODEL SAVING\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Save directory: {save_dir}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            if MIXED_PRECISION_AVAILABLE:\n",
    "                self.scaler = GradScaler()\n",
    "                self.use_amp = True\n",
    "                print(f\"   Mixed Precision: Enabled\")\n",
    "            else:\n",
    "                self.use_amp = False\n",
    "        else:\n",
    "            self.use_amp = False\n",
    "    \n",
    "    def create_optimal_model(self):\n",
    "        \"\"\"Create your best-performing ResNet-18 configuration\"\"\"\n",
    "        model = timm.create_model('resnet18', pretrained=True, num_classes=self.num_classes)\n",
    "        \n",
    "        # Your optimal freezing strategy (75% trainable)\n",
    "        frozen_layers = ['conv1', 'bn1', 'layer1', 'layer2', 'layer3']\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            for frozen_layer in frozen_layers:\n",
    "                if name.startswith(frozen_layer):\n",
    "                    param.requires_grad = False\n",
    "                    break\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def train_single_model(self, train_loader, val_loader, fold_num):\n",
    "        \"\"\"Train a single model with comprehensive tracking\"\"\"\n",
    "        model = self.create_optimal_model()\n",
    "        \n",
    "        # Optimized hyperparameters based on your best results\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Differential learning rates\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'fc' in name:\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': 0.0003},\n",
    "            {'params': classifier_params, 'lr': 0.001}\n",
    "        ], weight_decay=0.015)\n",
    "        \n",
    "        # Cosine annealing\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=70, eta_min=1e-6)\n",
    "        \n",
    "        max_epochs = 70\n",
    "        patience = 20\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Track training history\n",
    "        training_history = {\n",
    "            'train_accs': [],\n",
    "            'val_accs': [],\n",
    "            'train_losses': [],\n",
    "            'val_losses': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        print(f\"   Training fold-{fold_num}: {max_epochs} epochs, patience={patience}\")\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss_total = 0.0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.step(optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                train_loss_total += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            train_loss = train_loss_total / len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_loss_total = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    if self.use_amp:\n",
    "                        with autocast():\n",
    "                            outputs = model(images)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss_total += loss.item()\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            val_loss = val_loss_total / len(val_loader)\n",
    "            \n",
    "            # Record history\n",
    "            training_history['train_accs'].append(train_acc)\n",
    "            training_history['val_accs'].append(val_acc)\n",
    "            training_history['train_losses'].append(train_loss)\n",
    "            training_history['val_losses'].append(val_loss)\n",
    "            training_history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track best\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "                status = \"BEST\"\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                status = \"\"\n",
    "            \n",
    "            # Progress (less frequent printing)\n",
    "            if epoch % 10 == 0 or epoch < 3 or status or epoch > max_epochs - 3:\n",
    "                print(f\"     Epoch {epoch:2d}: Train {train_acc:.1f}%, Val {val_acc:.1f}% {status}\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"     Early stop at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Restore best model\n",
    "        if 'best_state' in locals():\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        print(f\"   fold-{fold_num} complete: Best val {best_val_acc:.1f}%\")\n",
    "        \n",
    "        # Save model and training history\n",
    "        model_save_path = f\"{self.save_dir}/model_fold_{fold_num}.pth\"\n",
    "        history_save_path = f\"{self.save_dir}/history_fold_{fold_num}.pkl\"\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_config': {\n",
    "                'architecture': 'resnet18',\n",
    "                'num_classes': self.num_classes,\n",
    "                'frozen_layers': ['conv1', 'bn1', 'layer1', 'layer2', 'layer3']\n",
    "            },\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'fold_num': fold_num\n",
    "        }, model_save_path)\n",
    "        \n",
    "        with open(history_save_path, 'wb') as f:\n",
    "            pickle.dump(training_history, f)\n",
    "        \n",
    "        print(f\"   ✓ Saved: {model_save_path}\")\n",
    "        \n",
    "        return model, best_val_acc, training_history\n",
    "    \n",
    "    def evaluate_with_tta(self, model, test_loader):\n",
    "        \"\"\"Evaluate model with test-time augmentation\"\"\"\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Original prediction\n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs1 = model(images)\n",
    "                else:\n",
    "                    outputs1 = model(images)\n",
    "                \n",
    "                # Flipped prediction\n",
    "                flipped = torch.flip(images, [3])\n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs2 = model(flipped)\n",
    "                else:\n",
    "                    outputs2 = model(flipped)\n",
    "                \n",
    "                # Average (TTA)\n",
    "                avg_outputs = (outputs1 + outputs2) / 2\n",
    "                probs = F.softmax(avg_outputs, dim=1)\n",
    "                _, predicted = torch.max(avg_outputs, 1)\n",
    "                \n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Store for detailed analysis\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_probabilities.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = 100 * test_correct / test_total\n",
    "        return accuracy, all_predictions, all_probabilities, all_labels\n",
    "    \n",
    "    def run_cross_validation_with_saving(self, images, ages, n_splits=5):\n",
    "        \"\"\"Run k-fold cross-validation with comprehensive saving\"\"\"\n",
    "        print(f\"\\n{n_splits}-FOLD CROSS-VALIDATION WITH MODEL SAVING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not isinstance(images, np.ndarray):\n",
    "            images = np.array(images)\n",
    "        if not isinstance(ages, np.ndarray):\n",
    "            ages = np.array(ages)\n",
    "        \n",
    "        # Create label mapping\n",
    "        unique_ages = sorted(list(set(ages)))\n",
    "        label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "        y_indices = np.array([label_mapping[age] for age in ages])\n",
    "        \n",
    "        # Create train/test split (same as original)\n",
    "        X_train_all, X_test, y_train_all, y_test = train_test_split(\n",
    "            images, y_indices, test_size=0.2, random_state=42, stratify=y_indices\n",
    "        )\n",
    "        \n",
    "        # Save data splits\n",
    "        data_splits = {\n",
    "            'X_train_all': X_train_all,\n",
    "            'X_test': X_test,\n",
    "            'y_train_all': y_train_all,\n",
    "            'y_test': y_test,\n",
    "            'label_mapping': label_mapping,\n",
    "            'unique_ages': unique_ages\n",
    "        }\n",
    "        \n",
    "        with open(f\"{self.save_dir}/data_splits.pkl\", 'wb') as f:\n",
    "            pickle.dump(data_splits, f)\n",
    "        \n",
    "        print(f\"✓ Saved data splits to {self.save_dir}/data_splits.pkl\")\n",
    "        \n",
    "        # Stratified K-Fold\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        cv_scores = []\n",
    "        all_training_histories = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_all, y_train_all)):\n",
    "            print(f\"\\nFOLD {fold + 1}/{n_splits}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Split data\n",
    "            X_train_fold = X_train_all[train_idx]\n",
    "            y_train_fold = y_train_all[train_idx]\n",
    "            X_val_fold = X_train_all[val_idx]\n",
    "            y_val_fold = y_train_all[val_idx]\n",
    "            \n",
    "            print(f\"   Train: {len(X_train_fold)}, Val: {len(X_val_fold)}\")\n",
    "            \n",
    "            # Augment training data\n",
    "            X_train_aug, y_train_aug = create_optimized_augmented_data(X_train_fold, y_train_fold, multiplier=40)\n",
    "            \n",
    "            # Create datasets and loaders\n",
    "            train_dataset = OptimizedDeerDataset(X_train_aug, y_train_aug)\n",
    "            val_dataset = OptimizedDeerDataset(X_val_fold, y_val_fold, test_time_aug=True)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "            \n",
    "            # Train model with saving\n",
    "            model, val_acc, training_history = self.train_single_model(train_loader, val_loader, fold + 1)\n",
    "            \n",
    "            cv_scores.append(val_acc)\n",
    "            all_training_histories.append(training_history)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return cv_scores, all_training_histories, data_splits\n",
    "    \n",
    "    def final_test_evaluation_with_saving(self, data_splits):\n",
    "        \"\"\"Final evaluation with detailed saving\"\"\"\n",
    "        print(f\"\\nFINAL TEST EVALUATION WITH COMPREHENSIVE SAVING\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        X_test = data_splits['X_test']\n",
    "        y_test = data_splits['y_test']\n",
    "        \n",
    "        print(f\"   Test set: {len(X_test)} samples\")\n",
    "        print(f\"   Test distribution: {Counter(y_test)}\")\n",
    "        \n",
    "        # Create test dataset\n",
    "        test_dataset = OptimizedDeerDataset(X_test, y_test, test_time_aug=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "        \n",
    "        # Load and evaluate each model\n",
    "        individual_scores = []\n",
    "        all_individual_predictions = []\n",
    "        all_individual_probabilities = []\n",
    "        \n",
    "        for fold in range(1, 6):  # 5 folds\n",
    "            # Load model\n",
    "            model_path = f\"{self.save_dir}/model_fold_{fold}.pth\"\n",
    "            checkpoint = torch.load(model_path)\n",
    "            \n",
    "            model = self.create_optimal_model()\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            # Evaluate\n",
    "            test_acc, preds, probs, labels = self.evaluate_with_tta(model, test_loader)\n",
    "            individual_scores.append(test_acc)\n",
    "            all_individual_predictions.append(preds)\n",
    "            all_individual_probabilities.append(probs)\n",
    "            \n",
    "            print(f\"   Model {fold}: {test_acc:.1f}%\")\n",
    "        \n",
    "        # Ensemble evaluation\n",
    "        print(f\"\\n   ENSEMBLE EVALUATION:\")\n",
    "        ensemble_probs = np.mean(all_individual_probabilities, axis=0)\n",
    "        ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
    "        ensemble_acc = np.mean(ensemble_preds == labels) * 100\n",
    "        \n",
    "        print(f\"   Ensemble + TTA: {ensemble_acc:.1f}%\")\n",
    "        \n",
    "        # Save comprehensive test results\n",
    "        test_results = {\n",
    "            'individual_scores': individual_scores,\n",
    "            'ensemble_score': ensemble_acc,\n",
    "            'individual_predictions': all_individual_predictions,\n",
    "            'individual_probabilities': all_individual_probabilities,\n",
    "            'ensemble_predictions': ensemble_preds.tolist(),\n",
    "            'ensemble_probabilities': ensemble_probs.tolist(),\n",
    "            'true_labels': labels,\n",
    "            'test_distribution': dict(Counter(y_test))\n",
    "        }\n",
    "        \n",
    "        with open(f\"{self.save_dir}/test_results.pkl\", 'wb') as f:\n",
    "            pickle.dump(test_results, f)\n",
    "        \n",
    "        print(f\"✓ Saved comprehensive test results to {self.save_dir}/test_results.pkl\")\n",
    "        \n",
    "        return individual_scores, ensemble_acc, test_results\n",
    "\n",
    "def enhanced_cross_validation_pipeline():\n",
    "    \"\"\"Enhanced pipeline with comprehensive model and data saving\"\"\"\n",
    "    print(\"ENHANCED CROSS-VALIDATION PIPELINE WITH MODEL SAVING\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Strategy: K-fold CV + Model saving + Comprehensive data tracking\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = f\"saved_models_{timestamp}\"\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        images, ages = load_original_data()\n",
    "        \n",
    "        # Initialize trainer with saving\n",
    "        trainer = CrossValidationTrainerWithSaving(\n",
    "            num_classes=len(set(ages)),\n",
    "            save_dir=save_dir\n",
    "        )\n",
    "        \n",
    "        # Run cross-validation with saving\n",
    "        cv_scores, training_histories, data_splits = trainer.run_cross_validation_with_saving(images, ages, n_splits=5)\n",
    "        \n",
    "        # Final test evaluation with saving\n",
    "        individual_scores, ensemble_score, test_results = trainer.final_test_evaluation_with_saving(data_splits)\n",
    "        \n",
    "        # Results\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nCROSS-VALIDATION RESULTS\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"CV Scores: {[f'{score:.1f}%' for score in cv_scores]}\")\n",
    "        print(f\"CV Mean: {np.mean(cv_scores):.1f}% ± {np.std(cv_scores):.1f}%\")\n",
    "        print(f\"CV Best: {max(cv_scores):.1f}%\")\n",
    "        \n",
    "        print(f\"\\nFINAL TEST RESULTS\")\n",
    "        print(\"=\"*40)\n",
    "        for i, score in enumerate(individual_scores):\n",
    "            print(f\"Model {i+1}: {score:.1f}%\")\n",
    "        \n",
    "        best_individual = max(individual_scores)\n",
    "        print(f\"\\nBest Individual: {best_individual:.1f}%\")\n",
    "        print(f\"Ensemble + TTA:  {ensemble_score:.1f}%\")\n",
    "        print(f\"Time: {elapsed/60:.1f} minutes\")\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        final_results = {\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': float(np.mean(cv_scores)),\n",
    "            'cv_std': float(np.std(cv_scores)),\n",
    "            'individual_test_scores': individual_scores,\n",
    "            'ensemble_test_score': ensemble_score,\n",
    "            'best_score': max(best_individual, ensemble_score),\n",
    "            'goal_reached': max(best_individual, ensemble_score) >= 70,\n",
    "            'gap_to_goal': max(0, 70 - max(best_individual, ensemble_score)),\n",
    "            'elapsed_minutes': elapsed/60,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'save_directory': save_dir\n",
    "        }\n",
    "        \n",
    "        with open(f'{save_dir}/comprehensive_results.json', 'w') as f:\n",
    "            json.dump(final_results, f, indent=2)\n",
    "        \n",
    "        # Save training histories\n",
    "        with open(f'{save_dir}/all_training_histories.pkl', 'wb') as f:\n",
    "            pickle.dump(training_histories, f)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*70)\n",
    "        print(f\"🎉 TRAINING COMPLETE WITH COMPREHENSIVE SAVING!\")\n",
    "        print(f\"🎉 All models and data saved to: {save_dir}/\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nSaved files:\")\n",
    "        print(f\"• 5 trained models: model_fold_1.pth through model_fold_5.pth\")\n",
    "        print(f\"• Training histories: history_fold_1.pkl through history_fold_5.pkl\")\n",
    "        print(f\"• Data splits: data_splits.pkl\")\n",
    "        print(f\"• Test results: test_results.pkl\")\n",
    "        print(f\"• Final results: comprehensive_results.json\")\n",
    "        print(f\"• All training histories: all_training_histories.pkl\")\n",
    "        print(f\"\\nNow you can run analysis scripts without any training!\")\n",
    "        \n",
    "        return final_results, save_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    enhanced_cross_validation_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93be29-9f92-40fd-9669-2e7bc23d00df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Analysis script\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class OptimizedDeerDataset(Dataset):\n",
    "    \"\"\"Same dataset class as training\"\"\"\n",
    "    def __init__(self, X, y, test_time_aug=False):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.test_time_aug = test_time_aug\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        if image.shape[-2:] != (224, 224):\n",
    "            image = F.interpolate(image.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "        \n",
    "        if self.test_time_aug and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class PureModelAnalyzer:\n",
    "    \"\"\"Pure analysis class - loads saved models, NO TRAINING\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = save_dir\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(f\"PURE MODEL ANALYZER - NO TRAINING\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Loading from: {save_dir}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        \n",
    "        # Verify directory exists\n",
    "        if not Path(save_dir).exists():\n",
    "            raise FileNotFoundError(f\"Save directory not found: {save_dir}\")\n",
    "        \n",
    "        # Load all saved data\n",
    "        self.load_saved_data()\n",
    "        \n",
    "        print(\"[PASS] All data loaded successfully\")\n",
    "        print(\"[PASS] Ready for analysis\")\n",
    "    \n",
    "    def load_saved_data(self):\n",
    "        \"\"\"Load all saved models and data\"\"\"\n",
    "        print(\"\\nLoading saved data...\")\n",
    "        \n",
    "        # Load comprehensive results\n",
    "        with open(f\"{self.save_dir}/comprehensive_results.json\", 'r') as f:\n",
    "            self.results = json.load(f)\n",
    "        print(\"[PASS] Loaded comprehensive results\")\n",
    "        \n",
    "        # Load data splits\n",
    "        with open(f\"{self.save_dir}/data_splits.pkl\", 'rb') as f:\n",
    "            self.data_splits = pickle.load(f)\n",
    "        print(\"[PASS] Loaded data splits\")\n",
    "        \n",
    "        # Load test results\n",
    "        with open(f\"{self.save_dir}/test_results.pkl\", 'rb') as f:\n",
    "            self.test_results = pickle.load(f)\n",
    "        print(\"[PASS] Loaded test results\")\n",
    "        \n",
    "        # Load training histories\n",
    "        with open(f\"{self.save_dir}/all_training_histories.pkl\", 'rb') as f:\n",
    "            self.training_histories = pickle.load(f)\n",
    "        print(\"[PASS] Loaded training histories\")\n",
    "        \n",
    "        # Model configuration\n",
    "        self.num_classes = len(self.data_splits['unique_ages'])\n",
    "        \n",
    "        print(f\"[PASS] Configuration: {self.num_classes} classes, {len(self.data_splits['X_test'])} test samples\")\n",
    "    \n",
    "    def create_model_architecture(self):\n",
    "        \"\"\"Create the same model architecture (for loading weights)\"\"\"\n",
    "        model = timm.create_model('resnet18', pretrained=False, num_classes=self.num_classes)\n",
    "        \n",
    "        # Apply same freezing (though not needed for inference)\n",
    "        frozen_layers = ['conv1', 'bn1', 'layer1', 'layer2', 'layer3']\n",
    "        for name, param in model.named_parameters():\n",
    "            for frozen_layer in frozen_layers:\n",
    "                if name.startswith(frozen_layer):\n",
    "                    param.requires_grad = False\n",
    "                    break\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def load_trained_models(self):\n",
    "        \"\"\"Load all 5 trained models\"\"\"\n",
    "        print(\"\\nLoading trained models...\")\n",
    "        models = []\n",
    "        \n",
    "        for fold in range(1, 6):\n",
    "            model_path = f\"{self.save_dir}/model_fold_{fold}.pth\"\n",
    "            \n",
    "            # Load checkpoint\n",
    "            checkpoint = torch.load(model_path, map_location=self.device)\n",
    "            \n",
    "            # Create model and load weights\n",
    "            model = self.create_model_architecture()\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()  # Set to evaluation mode\n",
    "            \n",
    "            models.append(model)\n",
    "            print(f\"[PASS] Loaded model fold {fold} (Val acc: {checkpoint['best_val_acc']:.1f}%)\")\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def calculate_all_metrics(self):\n",
    "        \"\"\"Calculate comprehensive metrics from saved results\"\"\"\n",
    "        print(\"\\nCalculating comprehensive metrics...\")\n",
    "        \n",
    "        # Get data from saved results\n",
    "        true_labels = np.array(self.test_results['true_labels'])\n",
    "        ensemble_preds = np.array(self.test_results['ensemble_predictions'])\n",
    "        individual_preds = self.test_results['individual_predictions']\n",
    "        ensemble_probs = np.array(self.test_results['ensemble_probabilities'])\n",
    "        individual_probs = self.test_results['individual_probabilities']\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Individual model metrics\n",
    "        for i, preds in enumerate(individual_preds):\n",
    "            preds = np.array(preds)\n",
    "            accuracy = np.mean(preds == true_labels) * 100\n",
    "            f1_macro = f1_score(true_labels, preds, average='macro') * 100\n",
    "            f1_weighted = f1_score(true_labels, preds, average='weighted') * 100\n",
    "            precision = precision_score(true_labels, preds, average='macro') * 100\n",
    "            recall = recall_score(true_labels, preds, average='macro') * 100\n",
    "            \n",
    "            metrics[f'model_{i+1}'] = {\n",
    "                'accuracy': accuracy,\n",
    "                'f1_macro': f1_macro,\n",
    "                'f1_weighted': f1_weighted,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "        \n",
    "        # Ensemble metrics\n",
    "        ensemble_accuracy = np.mean(ensemble_preds == true_labels) * 100\n",
    "        ensemble_f1_macro = f1_score(true_labels, ensemble_preds, average='macro') * 100\n",
    "        ensemble_f1_weighted = f1_score(true_labels, ensemble_preds, average='weighted') * 100\n",
    "        ensemble_precision = precision_score(true_labels, ensemble_preds, average='macro') * 100\n",
    "        ensemble_recall = recall_score(true_labels, ensemble_preds, average='macro') * 100\n",
    "        \n",
    "        metrics['ensemble'] = {\n",
    "            'accuracy': ensemble_accuracy,\n",
    "            'f1_macro': ensemble_f1_macro,\n",
    "            'f1_weighted': ensemble_f1_weighted,\n",
    "            'precision': ensemble_precision,\n",
    "            'recall': ensemble_recall\n",
    "        }\n",
    "        \n",
    "        # Class-wise metrics (without target_names to get numeric keys)\n",
    "        class_names = [f'Age {age}' for age in self.data_splits['unique_ages']]\n",
    "        metrics['classification_report'] = classification_report(\n",
    "            true_labels, ensemble_preds,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Debug: Print classification report keys\n",
    "        print(f\"Classification report keys: {list(metrics['classification_report'].keys())}\")\n",
    "        print(f\"Number of classes: {len(class_names)}\")\n",
    "        print(f\"Class names: {class_names}\")\n",
    "        \n",
    "        # Also store the class names for plotting\n",
    "        metrics['class_names'] = class_names\n",
    "        \n",
    "        print(\"[PASS] All metrics calculated\")\n",
    "        return metrics\n",
    "    \n",
    "    def create_all_plots(self, metrics):\n",
    "        \"\"\"Create all academic plots\"\"\"\n",
    "        print(\"\\nCreating comprehensive plots...\")\n",
    "        \n",
    "        # Create output directory\n",
    "        Path(\"analysis_plots\").mkdir(exist_ok=True)\n",
    "        \n",
    "        # 1. Cross-validation and test scores\n",
    "        self.plot_cv_and_test_scores()\n",
    "        \n",
    "        # 2. Training curves from saved histories\n",
    "        self.plot_training_curves_from_history()\n",
    "        \n",
    "        # 3. Confusion matrices\n",
    "        self.plot_confusion_matrices()\n",
    "        \n",
    "        # 4. Model comparison\n",
    "        self.plot_model_comparison(metrics)\n",
    "        \n",
    "        # 5. Class-wise performance\n",
    "        self.plot_class_performance(metrics)\n",
    "        \n",
    "        # 6. ROC curves\n",
    "        self.plot_roc_curves()\n",
    "        \n",
    "        # 7. Loss curves\n",
    "        self.plot_loss_curves()\n",
    "        \n",
    "        print(\"[PASS] All plots saved to 'analysis_plots/' directory\")\n",
    "    \n",
    "    def plot_cv_and_test_scores(self):\n",
    "        \"\"\"Plot CV and test scores\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # CV scores\n",
    "        cv_scores = self.results['cv_scores']\n",
    "        folds = range(1, len(cv_scores) + 1)\n",
    "        \n",
    "        bars1 = ax1.bar(folds, cv_scores, alpha=0.8, color='skyblue', edgecolor='navy', linewidth=2)\n",
    "        ax1.axhline(y=self.results['cv_mean'], color='red', linestyle='--', linewidth=2,\n",
    "                   label=f\"Mean: {self.results['cv_mean']:.1f}%\")\n",
    "        ax1.axhline(y=70, color='green', linestyle='--', alpha=0.7, linewidth=2, label=\"Target: 70%\")\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars1, cv_scores):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{score:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax1.set_xlabel('Fold', fontsize=12)\n",
    "        ax1.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "        ax1.set_title('Cross-Validation Scores', fontsize=14, fontweight='bold')\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim(0, max(cv_scores) * 1.1)\n",
    "        \n",
    "        # Test scores\n",
    "        test_scores = self.results['individual_test_scores'] + [self.results['ensemble_test_score']]\n",
    "        model_names = [f'Model {i+1}' for i in range(len(self.results['individual_test_scores']))] + ['Ensemble']\n",
    "        colors = ['lightcoral', 'lightgreen', 'lightsalmon', 'lightblue', 'plum', 'gold']\n",
    "        \n",
    "        bars2 = ax2.bar(model_names, test_scores, alpha=0.8, color=colors, edgecolor='black', linewidth=2)\n",
    "        ax2.axhline(y=70, color='red', linestyle='--', alpha=0.7, linewidth=2, label=\"Target: 70%\")\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars2, test_scores):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{score:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax2.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "        ax2.set_title('Final Test Performance', fontsize=14, fontweight='bold')\n",
    "        ax2.legend(fontsize=11)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.set_ylim(0, max(test_scores) * 1.1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('analysis_plots/cv_and_test_scores.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_training_curves_from_history(self):\n",
    "        \"\"\"Plot training curves from saved histories\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for fold, history in enumerate(self.training_histories):\n",
    "            ax = axes[fold]\n",
    "            epochs = range(1, len(history['train_accs']) + 1)\n",
    "            \n",
    "            # Plot accuracy curves\n",
    "            ax.plot(epochs, history['train_accs'], 'b-', label='Training Accuracy', linewidth=2, marker='o', markersize=3)\n",
    "            ax.plot(epochs, history['val_accs'], 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=3)\n",
    "            \n",
    "            ax.set_xlabel('Epoch', fontsize=11)\n",
    "            ax.set_ylabel('Accuracy (%)', fontsize=11)\n",
    "            ax.set_title(f'Fold {fold + 1} - Training Curves', fontsize=12, fontweight='bold')\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Find best epoch\n",
    "            best_epoch = np.argmax(history['val_accs']) + 1\n",
    "            best_val_acc = max(history['val_accs'])\n",
    "            ax.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7, label=f'Best: Epoch {best_epoch}')\n",
    "            \n",
    "            # Add final accuracy text\n",
    "            final_train = history['train_accs'][-1]\n",
    "            final_val = history['val_accs'][-1]\n",
    "            ax.text(0.02, 0.98, f'Final: Train {final_train:.1f}%, Val {final_val:.1f}%\\nBest Val: {best_val_acc:.1f}%', \n",
    "                   transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.8),\n",
    "                   verticalalignment='top', fontsize=9)\n",
    "        \n",
    "        # Summary plot\n",
    "        ax_summary = axes[5]\n",
    "        all_train_accs = [history['train_accs'][-1] for history in self.training_histories]\n",
    "        all_val_accs = [history['val_accs'][-1] for history in self.training_histories]\n",
    "        all_best_val = [max(history['val_accs']) for history in self.training_histories]\n",
    "        folds = range(1, 6)\n",
    "        \n",
    "        x = np.arange(len(folds))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax_summary.bar(x - width, all_train_accs, width, label='Final Training', alpha=0.8, color='lightblue')\n",
    "        ax_summary.bar(x, all_val_accs, width, label='Final Validation', alpha=0.8, color='lightcoral')\n",
    "        ax_summary.bar(x + width, all_best_val, width, label='Best Validation', alpha=0.8, color='lightgreen')\n",
    "        \n",
    "        ax_summary.set_xlabel('Fold', fontsize=11)\n",
    "        ax_summary.set_ylabel('Accuracy (%)', fontsize=11)\n",
    "        ax_summary.set_title('Summary: Final vs Best Accuracies', fontsize=12, fontweight='bold')\n",
    "        ax_summary.set_xticks(x)\n",
    "        ax_summary.set_xticklabels([f'Fold {i}' for i in folds])\n",
    "        ax_summary.legend(fontsize=10)\n",
    "        ax_summary.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('analysis_plots/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrices(self):\n",
    "        \"\"\"Plot confusion matrices\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        true_labels = np.array(self.test_results['true_labels'])\n",
    "        ensemble_preds = np.array(self.test_results['ensemble_predictions'])\n",
    "        class_names = [f'Age {age}' for age in self.data_splits['unique_ages']]\n",
    "        \n",
    "        # Raw confusion matrix\n",
    "        cm = confusion_matrix(true_labels, ensemble_preds)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                   xticklabels=class_names, yticklabels=class_names,\n",
    "                   cbar_kws={'label': 'Count'})\n",
    "        ax1.set_title('Confusion Matrix (Raw Counts)', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Predicted', fontsize=12)\n",
    "        ax1.set_ylabel('True', fontsize=12)\n",
    "        \n",
    "        # Normalized confusion matrix\n",
    "        cm_norm = confusion_matrix(true_labels, ensemble_preds, normalize='true')\n",
    "        sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=ax2,\n",
    "                   xticklabels=class_names, yticklabels=class_names,\n",
    "                   cbar_kws={'label': 'Proportion'})\n",
    "        ax2.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Predicted', fontsize=12)\n",
    "        ax2.set_ylabel('True', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('analysis_plots/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_model_comparison(self, metrics):\n",
    "        \"\"\"Plot comprehensive model comparison\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        model_names = [f'Model {i+1}' for i in range(5)] + ['Ensemble']\n",
    "        colors = ['lightcoral', 'lightgreen', 'lightsalmon', 'lightblue', 'plum', 'gold']\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        accuracies = [metrics[f'model_{i+1}']['accuracy'] for i in range(5)] + [metrics['ensemble']['accuracy']]\n",
    "        bars1 = axes[0,0].bar(model_names, accuracies, alpha=0.8, color=colors, edgecolor='black')\n",
    "        axes[0,0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[0,0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        self.add_value_labels(axes[0,0], bars1, accuracies)\n",
    "        \n",
    "        # F1 Score comparison\n",
    "        f1_scores = [metrics[f'model_{i+1}']['f1_macro'] for i in range(5)] + [metrics['ensemble']['f1_macro']]\n",
    "        bars2 = axes[0,1].bar(model_names, f1_scores, alpha=0.8, color=colors, edgecolor='black')\n",
    "        axes[0,1].set_title('F1 Score (Macro) Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[0,1].set_ylabel('F1 Score (%)', fontsize=12)\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        self.add_value_labels(axes[0,1], bars2, f1_scores)\n",
    "        \n",
    "        # Precision comparison\n",
    "        precisions = [metrics[f'model_{i+1}']['precision'] for i in range(5)] + [metrics['ensemble']['precision']]\n",
    "        bars3 = axes[1,0].bar(model_names, precisions, alpha=0.8, color=colors, edgecolor='black')\n",
    "        axes[1,0].set_title('Precision Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[1,0].set_ylabel('Precision (%)', fontsize=12)\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        self.add_value_labels(axes[1,0], bars3, precisions)\n",
    "        \n",
    "        # Recall comparison\n",
    "        recalls = [metrics[f'model_{i+1}']['recall'] for i in range(5)] + [metrics['ensemble']['recall']]\n",
    "        bars4 = axes[1,1].bar(model_names, recalls, alpha=0.8, color=colors, edgecolor='black')\n",
    "        axes[1,1].set_title('Recall Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[1,1].set_ylabel('Recall (%)', fontsize=12)\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        self.add_value_labels(axes[1,1], bars4, recalls)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('analysis_plots/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def add_value_labels(self, ax, bars, values):\n",
    "        \"\"\"Add value labels on top of bars\"\"\"\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    def plot_class_performance(self, metrics):\n",
    "        \"\"\"Plot class-wise performance\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        class_names = metrics['class_names']\n",
    "        report = metrics['classification_report']\n",
    "        \n",
    "        try:\n",
    "            # Extract class-wise metrics using numeric string keys\n",
    "            f1_scores = [report[str(i)]['f1-score'] * 100 for i in range(len(class_names))]\n",
    "            precisions = [report[str(i)]['precision'] * 100 for i in range(len(class_names))]\n",
    "            recalls = [report[str(i)]['recall'] * 100 for i in range(len(class_names))]\n",
    "            supports = [report[str(i)]['support'] for i in range(len(class_names))]\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError in class performance: {e}\")\n",
    "            print(f\"Available keys in classification report: {list(report.keys())}\")\n",
    "            print(f\"Looking for keys: {[str(i) for i in range(len(class_names))]}\")\n",
    "            # Try alternative key format if numeric keys don't work\n",
    "            if 'macro avg' in report:\n",
    "                print(\"Using macro averages as fallback...\")\n",
    "                macro_avg = report['macro avg']\n",
    "                f1_scores = [macro_avg['f1-score'] * 100] * len(class_names)\n",
    "                precisions = [macro_avg['precision'] * 100] * len(class_names)\n",
    "                recalls = [macro_avg['recall'] * 100] * len(class_names)\n",
    "                supports = [1] * len(class_names)  # Dummy values\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Class-wise metrics\n",
    "        x = np.arange(len(class_names))\n",
    "        width = 0.25\n",
    "        \n",
    "        bars1 = ax1.bar(x - width, f1_scores, width, label='F1-Score', alpha=0.8, color='lightcoral')\n",
    "        bars2 = ax1.bar(x, precisions, width, label='Precision', alpha=0.8, color='lightgreen')\n",
    "        bars3 = ax1.bar(x + width, recalls, width, label='Recall', alpha=0.8, color='lightblue')\n",
    "        \n",
    "        ax1.set_xlabel('Age Class', fontsize=12)\n",
    "        ax1.set_ylabel('Score (%)', fontsize=12)\n",
    "        ax1.set_title('Class-wise Performance Metrics', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(class_names)\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Support (number of samples)\n",
    "        bars4 = ax2.bar(class_names, supports, alpha=0.8, color='mediumpurple', edgecolor='indigo')\n",
    "        ax2.set_xlabel('Age Class', fontsize=12)\n",
    "        ax2.set_ylabel('Number of Test Samples', fontsize=12)\n",
    "        ax2.set_title('Test Set Distribution', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, support in zip(bars4, supports):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    str(support), ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('analysis_plots/class_performance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_roc_curves(self):\n",
    "        \"\"\"Plot ROC curves\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        true_labels = np.array(self.test_results['true_labels'])\n",
    "        ensemble_probs = np.array(self.test_results['ensemble_probabilities'])\n",
    "        class_names = [f'Age {age}' for age in self.data_splits['unique_ages']]\n",
    "        n_classes = len(class_names)\n",
    "        \n",
    "        # Binarize the output\n",
    "        y_test_bin = label_binarize(true_labels, classes=range(n_classes))\n",
    "        \n",
    "        # If binary classification, reshape\n",
    "        if n_classes == 2:\n",
    "            y_test_bin = y_test_bin.ravel()\n",
    "        \n",
    "        # Plot ROC curve for each class\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, n_classes))\n",
    "        \n",
    "        for i, (class_name, color) in enumerate(zip(class_names, colors)):\n",
    "            if n_classes == 2:\n",
    "                fpr, tpr, _ = roc_curve(y_test_bin, ensemble_probs[:, 1])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                ax.plot(fpr, tpr, color=color, lw=3, \n",
    "                       label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
    "                break\n",
    "            else:\n",
    "                fpr, tpr, _ = roc_curve(y_test_bin[:, i], ensemble_probs[:, i])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                ax.plot(fpr, tpr, color=color, lw=3, \n",
    "                       label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
    "        \n",
    "        # Plot diagonal line\n",
    "        ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.5, label='Random (AUC = 0.500)')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "        ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "        ax.set_title('ROC Curves - Multi-class Classification', fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc=\"lower right\", fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('analysis_plots/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_loss_curves(self):\n",
    "        \"\"\"Plot loss curves from training histories\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for fold, history in enumerate(self.training_histories):\n",
    "            ax = axes[fold]\n",
    "            epochs = range(1, len(history['train_losses']) + 1)\n",
    "            \n",
    "            # Plot loss curves\n",
    "            ax.plot(epochs, history['train_losses'], 'b-', label='Training Loss', linewidth=2, marker='o', markersize=3)\n",
    "            ax.plot(epochs, history['val_losses'], 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=3)\n",
    "            \n",
    "            ax.set_xlabel('Epoch', fontsize=11)\n",
    "            ax.set_ylabel('Loss', fontsize=11)\n",
    "            ax.set_title(f'Fold {fold + 1} - Loss Curves', fontsize=12, fontweight='bold')\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Find best epoch (min val loss)\n",
    "            best_epoch = np.argmin(history['val_losses']) + 1\n",
    "            min_val_loss = min(history['val_losses'])\n",
    "            ax.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Add final loss text\n",
    "            final_train_loss = history['train_losses'][-1]\n",
    "            final_val_loss = history['val_losses'][-1]\n",
    "            ax.text(0.02, 0.98, f'Final: Train {final_train_loss:.3f}, Val {final_val_loss:.3f}\\nMin Val: {min_val_loss:.3f}', \n",
    "                   transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.8),\n",
    "                   verticalalignment='top', fontsize=9)\n",
    "        \n",
    "        # Summary plot - Learning rates\n",
    "        ax_summary = axes[5]\n",
    "        for fold, history in enumerate(self.training_histories):\n",
    "            epochs = range(1, len(history['learning_rates']) + 1)\n",
    "            ax_summary.plot(epochs, history['learning_rates'], label=f'Fold {fold+1}', linewidth=2)\n",
    "        \n",
    "        ax_summary.set_xlabel('Epoch', fontsize=11)\n",
    "        ax_summary.set_ylabel('Learning Rate', fontsize=11)\n",
    "        ax_summary.set_title('Learning Rate Schedule (All Folds)', fontsize=12, fontweight='bold')\n",
    "        ax_summary.legend(fontsize=10)\n",
    "        ax_summary.grid(True, alpha=0.3)\n",
    "        ax_summary.set_yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('analysis_plots/loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_comprehensive_report(self, metrics):\n",
    "        \"\"\"Generate comprehensive academic report\"\"\"\n",
    "        print(\"\\nGenerating comprehensive report...\")\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"=\"*80)\n",
    "        report.append(\"COMPREHENSIVE MODEL ANALYSIS REPORT\")\n",
    "        report.append(\"=\"*80)\n",
    "        report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report.append(f\"Results from: {self.results['timestamp']}\")\n",
    "        report.append(f\"Models loaded from: {self.save_dir}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Dataset overview\n",
    "        report.append(\"DATASET OVERVIEW\")\n",
    "        report.append(\"-\"*40)\n",
    "        total_samples = len(self.data_splits['X_train_all']) + len(self.data_splits['X_test'])\n",
    "        report.append(f\"Total samples: {total_samples}\")\n",
    "        report.append(f\"Training samples: {len(self.data_splits['X_train_all'])}\")\n",
    "        report.append(f\"Test samples: {len(self.data_splits['X_test'])}\")\n",
    "        report.append(f\"Classes: {self.num_classes} age groups ({', '.join([str(age) for age in self.data_splits['unique_ages']])})\")\n",
    "        report.append(f\"Train/Test split: 80/20\")\n",
    "        report.append(f\"Cross-validation: 5-fold stratified\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Model architecture\n",
    "        report.append(\"MODEL ARCHITECTURE\")\n",
    "        report.append(\"-\"*40)\n",
    "        report.append(\"Base model: ResNet-18 (pretrained on ImageNet)\")\n",
    "        report.append(\"Frozen layers: conv1, bn1, layer1, layer2, layer3 (75% frozen)\")\n",
    "        report.append(\"Trainable layers: layer4, fc\")\n",
    "        report.append(\"Input size: 224x224 RGB images\")\n",
    "        report.append(\"Data augmentation: 40x multiplier with rotation, flip, lighting\")\n",
    "        report.append(\"Test-time augmentation: Horizontal flip averaging\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Training details\n",
    "        report.append(\"TRAINING CONFIGURATION\")\n",
    "        report.append(\"-\"*40)\n",
    "        report.append(\"Optimizer: AdamW with differential learning rates\")\n",
    "        report.append(\"  - Backbone: 0.0003\")\n",
    "        report.append(\"  - Classifier: 0.001\")\n",
    "        report.append(\"Weight decay: 0.015\")\n",
    "        report.append(\"Loss function: CrossEntropyLoss with label smoothing (0.1)\")\n",
    "        report.append(\"Scheduler: CosineAnnealingLR (T_max=70, eta_min=1e-6)\")\n",
    "        report.append(\"Max epochs: 70, Early stopping patience: 20\")\n",
    "        report.append(\"Mixed precision training: Enabled\")\n",
    "        report.append(f\"Total training time: {self.results['elapsed_minutes']:.1f} minutes\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Cross-validation results\n",
    "        report.append(\"CROSS-VALIDATION RESULTS\")\n",
    "        report.append(\"-\"*40)\n",
    "        for i, score in enumerate(self.results['cv_scores']):\n",
    "            report.append(f\"Fold {i+1}: {score:.1f}%\")\n",
    "        report.append(f\"Mean CV accuracy: {self.results['cv_mean']:.1f} ± {self.results['cv_std']:.1f}%\")\n",
    "        report.append(f\"Best CV accuracy: {max(self.results['cv_scores']):.1f}%\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Test performance\n",
    "        report.append(\"TEST SET PERFORMANCE\")\n",
    "        report.append(\"-\"*40)\n",
    "        for i, score in enumerate(self.results['individual_test_scores']):\n",
    "            report.append(f\"Model {i+1}: {score:.1f}%\")\n",
    "        report.append(f\"Ensemble (5 models + TTA): {self.results['ensemble_test_score']:.1f}%\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Detailed ensemble metrics\n",
    "        report.append(\"DETAILED ENSEMBLE METRICS\")\n",
    "        report.append(\"-\"*40)\n",
    "        ensemble_metrics = metrics['ensemble']\n",
    "        report.append(f\"Accuracy: {ensemble_metrics['accuracy']:.2f}%\")\n",
    "        report.append(f\"F1-Score (Macro): {ensemble_metrics['f1_macro']:.2f}%\")\n",
    "        report.append(f\"F1-Score (Weighted): {ensemble_metrics['f1_weighted']:.2f}%\")\n",
    "        report.append(f\"Precision (Macro): {ensemble_metrics['precision']:.2f}%\")\n",
    "        report.append(f\"Recall (Macro): {ensemble_metrics['recall']:.2f}%\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Class-wise detailed performance\n",
    "        report.append(\"CLASS-WISE PERFORMANCE (ENSEMBLE)\")\n",
    "        report.append(\"-\"*40)\n",
    "        class_report = metrics['classification_report']\n",
    "        \n",
    "        try:\n",
    "            for i, age in enumerate(self.data_splits['unique_ages']):\n",
    "                class_metrics = class_report[str(i)]  # Use numeric string key\n",
    "                report.append(f\"Age {age}:\")\n",
    "                report.append(f\"  Precision: {class_metrics['precision']*100:.2f}%\")\n",
    "                report.append(f\"  Recall: {class_metrics['recall']*100:.2f}%\")\n",
    "                report.append(f\"  F1-Score: {class_metrics['f1-score']*100:.2f}%\")\n",
    "                report.append(f\"  Support: {class_metrics['support']} samples\")\n",
    "                report.append(\"\")\n",
    "        except KeyError as e:\n",
    "            report.append(f\"Error accessing class-wise metrics: {e}\")\n",
    "            report.append(f\"Available keys: {list(class_report.keys())}\")\n",
    "            if 'macro avg' in class_report:\n",
    "                macro_avg = class_report['macro avg']\n",
    "                report.append(f\"Macro averages:\")\n",
    "                report.append(f\"  Precision: {macro_avg['precision']*100:.2f}%\")\n",
    "                report.append(f\"  Recall: {macro_avg['recall']*100:.2f}%\")\n",
    "                report.append(f\"  F1-Score: {macro_avg['f1-score']*100:.2f}%\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Statistical significance\n",
    "        report.append(\"STATISTICAL ANALYSIS\")\n",
    "        report.append(\"-\"*40)\n",
    "        cv_scores = self.results['cv_scores']\n",
    "        cv_mean = np.mean(cv_scores)\n",
    "        cv_std = np.std(cv_scores)\n",
    "        cv_sem = cv_std / np.sqrt(len(cv_scores))  # Standard error of mean\n",
    "        confidence_95 = 1.96 * cv_sem  # 95% confidence interval\n",
    "        \n",
    "        report.append(f\"Cross-validation mean: {cv_mean:.2f}%\")\n",
    "        report.append(f\"Cross-validation std: {cv_std:.2f}%\")\n",
    "        report.append(f\"Standard error of mean: {cv_sem:.2f}%\")\n",
    "        report.append(f\"95% Confidence interval: [{cv_mean-confidence_95:.2f}%, {cv_mean+confidence_95:.2f}%]\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Model generalization assessment\n",
    "        report.append(\"GENERALIZATION ASSESSMENT\")\n",
    "        report.append(\"-\"*40)\n",
    "        \n",
    "        # Check for overfitting by comparing train vs val\n",
    "        final_train_accs = [h['train_accs'][-1] for h in self.training_histories]\n",
    "        final_val_accs = [h['val_accs'][-1] for h in self.training_histories]\n",
    "        best_val_accs = [max(h['val_accs']) for h in self.training_histories]\n",
    "        \n",
    "        mean_train = np.mean(final_train_accs)\n",
    "        mean_val = np.mean(final_val_accs)\n",
    "        mean_best_val = np.mean(best_val_accs)\n",
    "        \n",
    "        overfitting_gap = mean_train - mean_val\n",
    "        generalization_gap = mean_best_val - self.results['ensemble_test_score']\n",
    "        \n",
    "        report.append(f\"Average final training accuracy: {mean_train:.2f}%\")\n",
    "        report.append(f\"Average final validation accuracy: {mean_val:.2f}%\")\n",
    "        report.append(f\"Average best validation accuracy: {mean_best_val:.2f}%\")\n",
    "        report.append(f\"Final test accuracy (ensemble): {self.results['ensemble_test_score']:.2f}%\")\n",
    "        report.append(f\"Overfitting gap (train - val): {overfitting_gap:.2f}%\")\n",
    "        report.append(f\"Generalization gap (val - test): {generalization_gap:.2f}%\")\n",
    "        \n",
    "        if overfitting_gap < 5:\n",
    "            report.append(\"✓ No significant overfitting detected\".replace(\"✓\", \"[PASS]\"))\n",
    "        elif overfitting_gap < 10:\n",
    "            report.append(\"⚠ Mild overfitting detected\".replace(\"⚠\", \"[WARN]\"))\n",
    "        else:\n",
    "            report.append(\"❌ Significant overfitting detected\".replace(\"❌\", \"[FAIL]\"))\n",
    "        \n",
    "        if abs(generalization_gap) < 5:\n",
    "            report.append(\"✓ Good generalization to test set\".replace(\"✓\", \"[PASS]\"))\n",
    "        else:\n",
    "            report.append(\"⚠ Some generalization gap observed\".replace(\"⚠\", \"[WARN]\"))\n",
    "        \n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Goal achievement\n",
    "        report.append(\"GOAL ACHIEVEMENT\")\n",
    "        report.append(\"-\"*40)\n",
    "        target = 70\n",
    "        best_score = self.results['ensemble_test_score']\n",
    "        \n",
    "        if best_score >= target:\n",
    "            report.append(f\"[SUCCESS] Target achieved!\")\n",
    "            report.append(f\"   Target: {target}%\")\n",
    "            report.append(f\"   Achieved: {best_score:.1f}%\")\n",
    "            report.append(f\"   Margin: +{best_score - target:.1f}%\")\n",
    "        else:\n",
    "            report.append(f\"[MISS] Target not reached\")\n",
    "            report.append(f\"   Target: {target}%\")\n",
    "            report.append(f\"   Achieved: {best_score:.1f}%\")\n",
    "            report.append(f\"   Gap: -{target - best_score:.1f}%\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Technical notes\n",
    "        report.append(\"TECHNICAL IMPLEMENTATION\")\n",
    "        report.append(\"-\"*40)\n",
    "        report.append(\"- Framework: PyTorch with timm (transformers)\")\n",
    "        report.append(\"- Hardware: NVIDIA RTX 2060 with CUDA\")\n",
    "        report.append(\"- Mixed precision training for efficiency\")\n",
    "        report.append(\"- Reproducible results (fixed random seeds)\")\n",
    "        report.append(\"- Ensemble method: Simple averaging of softmax outputs\")\n",
    "        report.append(\"- Cross-validation: Stratified to maintain class balance\")\n",
    "        report.append(\"- Data augmentation: Domain-specific for deer images\")\n",
    "        report.append(\"- Early stopping to prevent overfitting\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Files generated\n",
    "        report.append(\"ANALYSIS OUTPUTS\")\n",
    "        report.append(\"-\"*40)\n",
    "        report.append(\"Generated plots:\")\n",
    "        report.append(\"- cv_and_test_scores.png - Cross-validation and test performance\")\n",
    "        report.append(\"- training_curves.png - Training/validation accuracy curves\")\n",
    "        report.append(\"- confusion_matrices.png - Prediction confusion analysis\")\n",
    "        report.append(\"- model_comparison.png - Individual vs ensemble metrics\")\n",
    "        report.append(\"- class_performance.png - Per-class performance breakdown\")\n",
    "        report.append(\"- roc_curves.png - ROC curves for each class\")\n",
    "        report.append(\"- loss_curves.png - Training/validation loss progression\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"Data files:\")\n",
    "        report.append(f\"- Models: {self.save_dir}/model_fold_*.pth\")\n",
    "        report.append(f\"- Results: {self.save_dir}/comprehensive_results.json\")\n",
    "        report.append(f\"- Test predictions: {self.save_dir}/test_results.pkl\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        report_text = \"\\n\".join(report)\n",
    "        \n",
    "        # Save report with UTF-8 encoding\n",
    "        with open('analysis_plots/academic_report.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(report_text)\n",
    "        \n",
    "        print(report_text)\n",
    "        print(\"\\n[PASS] Academic report saved to 'analysis_plots/academic_report.txt'\")\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Run complete analysis pipeline - NO TRAINING\"\"\"\n",
    "        print(\"STARTING PURE ANALYSIS (NO TRAINING)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Calculate metrics from saved results\n",
    "            metrics = self.calculate_all_metrics()\n",
    "            \n",
    "            # Create all plots\n",
    "            self.create_all_plots(metrics)\n",
    "            \n",
    "            # Generate academic report\n",
    "            self.generate_comprehensive_report(metrics)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUCCESS! PURE ANALYSIS COMPLETE!\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"✓ NO TRAINING WAS PERFORMED\".replace(\"✓\", \"[PASS]\"))\n",
    "            print(\"✓ All metrics calculated from saved models\".replace(\"✓\", \"[PASS]\"))\n",
    "            print(\"✓ All plots generated and saved\".replace(\"✓\", \"[PASS]\"))\n",
    "            print(\"✓ Academic report created\".replace(\"✓\", \"[PASS]\"))\n",
    "            print(\"\")\n",
    "            print(\"Check 'analysis_plots/' folder for:\")\n",
    "            print(\"   - All visualization plots\")\n",
    "            print(\"   - Academic report (academic_report.txt)\")\n",
    "            print(\"\")\n",
    "            print(f\"Key Results:\")\n",
    "            print(f\"   - CV Mean: {self.results['cv_mean']:.1f}% ± {self.results['cv_std']:.1f}%\")\n",
    "            print(f\"   - Best Individual: {max(self.results['individual_test_scores']):.1f}%\")\n",
    "            print(f\"   - Ensemble: {self.results['ensemble_test_score']:.1f}%\")\n",
    "            success_status = \"ACHIEVED\" if self.results['ensemble_test_score'] >= 70 else \"NOT REACHED\"\n",
    "            print(f\"   - Goal (70%): {success_status}\")\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in analysis: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Main execution function\n",
    "def analyze_saved_models(save_dir):\n",
    "    \"\"\"Analyze saved models without any training\"\"\"\n",
    "    print(\"🔍 PURE MODEL ANALYSIS - ZERO TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = PureModelAnalyzer(save_dir)\n",
    "    \n",
    "    # Run complete analysis\n",
    "    metrics = analyzer.run_complete_analysis()\n",
    "    \n",
    "    return analyzer, metrics\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual save directory\n",
    "    save_dir = \"saved_models_20250619_171443\"  # Updated to match your training output\n",
    "    \n",
    "    # If you don't know the exact directory name, uncomment this:\n",
    "    # import glob\n",
    "    # save_dirs = glob.glob(\"saved_models_*\")\n",
    "    # if save_dirs:\n",
    "    #     save_dir = save_dirs[-1]  # Use the most recent\n",
    "    #     print(f\"Found save directory: {save_dir}\")\n",
    "    # else:\n",
    "    #     print(\"No saved model directories found!\")\n",
    "    #     print(\"Please run the enhanced training script first.\")\n",
    "    #     exit()\n",
    "    \n",
    "    try:\n",
    "        analyzer, metrics = analyze_saved_models(save_dir)\n",
    "        print(\"\\nSUCCESS! All analysis complete with NO TRAINING!\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nSave directory not found: {save_dir}\")\n",
    "        print(\"Please:\")\n",
    "        print(\"1. Run the enhanced training script first, OR\")\n",
    "        print(\"2. Update the save_dir variable to point to your saved models\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(\"Please check that all required files are in the save directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf276e2-9709-4439-b7cb-e1dc55cbd509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
