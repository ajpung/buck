{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2dad4-961a-479e-9619-1a78b5561d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_18884\\1301233008.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ LAUNCHING COMPLETE DEER AGING PIPELINE...\n",
      "âš ï¸  Starting from EfficientNet-B5 (as requested)\n",
      "ğŸ¯ TESTING ALL MODELS: Pretrained â†’ Alternatives â†’ Random Init\n",
      "ğŸ’¾ All results will be automatically saved\n",
      "ğŸ“Š Will show which models used pretrained vs random initialization\n",
      "ğŸš€ LAUNCHING COMPLETE DEER AGING PIPELINE\n",
      "================================================================================\n",
      "ğŸ“‹ PIPELINE STEPS:\n",
      "   1. Load original 357 images\n",
      "   2. Create train/val/test splits\n",
      "   3. Balance and augment training data\n",
      "   4. Test all architectures (starting from EfficientNet-B5)\n",
      "   5. Save results and create leaderboard\n",
      "================================================================================\n",
      "ğŸ“‚ LOADING ORIGINAL DATA\n",
      "==================================================\n",
      "   âœ… Loaded 357 original images\n",
      "   ğŸ”„ Grouping ages: 5.5+ â†’ 5.5\n",
      "   ğŸ“Š Original age distribution: {5.5: 63, 4.5: 74, 2.5: 65, 3.5: 73, 1.5: 57, 12.5: 1, 6.5: 19, 8.5: 4, 7.5: 1}\n",
      "   ğŸ“Š Grouped age distribution: {5.5: 88, 4.5: 74, 2.5: 65, 3.5: 73, 1.5: 57}\n",
      "\n",
      "ğŸ”€ CREATING TRAIN/VAL/TEST SPLIT\n",
      "==================================================\n",
      "   ğŸ“Š Age distribution: {np.float64(5.5): 88, np.float64(4.5): 74, np.float64(2.5): 65, np.float64(3.5): 73, np.float64(1.5): 57}\n",
      "   ğŸ“Š Minimum class size: 57\n",
      "   ğŸ¯ Can use stratified split: True\n",
      "   ğŸ“Š Train: 231 samples\n",
      "   ğŸ“Š Val: 54 samples\n",
      "   ğŸ“Š Test: 72 samples\n",
      "   ğŸ·ï¸ Label mapping: {np.float64(1.5): 0, np.float64(2.5): 1, np.float64(3.5): 2, np.float64(4.5): 3, np.float64(5.5): 4}\n",
      "   ğŸ¯ Number of classes: 5\n",
      "   ğŸ“ˆ Train distribution: Counter({np.int64(4): 57, np.int64(3): 48, np.int64(2): 47, np.int64(1): 42, np.int64(0): 37})\n",
      "   ğŸ“ˆ Val distribution: Counter({np.int64(4): 13, np.int64(3): 11, np.int64(2): 11, np.int64(1): 10, np.int64(0): 9})\n",
      "   ğŸ“ˆ Test distribution: Counter({np.int64(4): 18, np.int64(3): 15, np.int64(2): 15, np.int64(1): 13, np.int64(0): 11})\n",
      "\n",
      "ğŸ”„ BALANCING AND AUGMENTING DATA\n",
      "==================================================\n",
      "   ğŸ¯ Target: 30x augmentation per class\n",
      "   ğŸ“Š Original distribution: {np.int64(2): 47, np.int64(1): 42, np.int64(0): 37, np.int64(4): 57, np.int64(3): 48}\n",
      "   ğŸ¯ Target samples per class: 1710\n",
      "   ğŸ“ˆ Class 0: 37 â†’ 1710 samples\n",
      "   ğŸ“ˆ Class 1: 42 â†’ 1710 samples\n",
      "   ğŸ“ˆ Class 2: 47 â†’ 1710 samples\n",
      "   ğŸ“ˆ Class 3: 48 â†’ 1710 samples\n",
      "   ğŸ“ˆ Class 4: 57 â†’ 1710 samples\n",
      "   âœ… Augmentation complete: 8550 total samples\n",
      "   ğŸ“Š Final distribution: Counter({np.int64(0): 1710, np.int64(1): 1710, np.int64(2): 1710, np.int64(3): 1710, np.int64(4): 1710})\n",
      "ğŸ”¥ COMPLETE DEER AGE TRAINER\n",
      "   Device: cuda\n",
      "   Classes: 5\n",
      "ğŸ”¥ COMPLETE DEER AGING PIPELINE\n",
      "================================================================================\n",
      "ğŸ¯ Starting from EfficientNet-B5 onwards\n",
      "ğŸ¯ All results will be saved automatically\n",
      "================================================================================\n",
      "ğŸ“Š Data ready: 8550 train, 54 val, 72 test\n",
      "\n",
      "ğŸ—ï¸ COMPLETE ARCHITECTURE ARSENAL (43 models)\n",
      "================================================================================\n",
      "ğŸ¯ FALLBACK STRATEGY: Pretrained â†’ Alternative Names â†’ Random Init\n",
      "ğŸ“Š ALL models will be tested regardless of pretrained weight availability\n",
      "ğŸ“ EfficientNet (6 models): EfficientNet-B5, EfficientNet-B6, EfficientNet-B7, EfficientNet-ES, EfficientNet-EM, EfficientNet-EL\n",
      "ğŸ“ EfficientNetV2 (3 models): EfficientNetV2-S, EfficientNetV2-M, EfficientNetV2-L\n",
      "ğŸ“ DenseNet (4 models): DenseNet-121, DenseNet-161, DenseNet-169, DenseNet-201\n",
      "ğŸ“ ResNet (7 models): ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152, ResNet-26, ResNet-26d\n",
      "ğŸ“ ResNeXt (2 models): ResNeXt-50, ResNeXt-101\n",
      "ğŸ“ Wide-ResNet (2 models): Wide-ResNet-50, Wide-ResNet-101\n",
      "ğŸ“ MobileNet (3 models): MobileNetV2, MobileNetV3-Small, MobileNetV3-Large\n",
      "ğŸ“ RegNet (4 models): RegNetX-400MF, RegNetX-800MF, RegNetY-400MF, RegNetY-800MF\n",
      "ğŸ“ ConvNeXt (3 models): ConvNeXt-Tiny, ConvNeXt-Small, ConvNeXt-Base\n",
      "ğŸ“ Swin (2 models): Swin-Tiny, Swin-Small\n",
      "ğŸ“ VGG (2 models): VGG-16, VGG-19\n",
      "ğŸ“ DeiT (3 models): DeiT-Tiny, DeiT-Small, DeiT-Base\n",
      "ğŸ“ SEResNet (1 models): SEResNet-50\n",
      "ğŸ“ SEResNeXt (1 models): SEResNeXt-50\n",
      "\n",
      "ğŸ”¥ ULTRA AGGRESSIVE TESTING: 43 ARCHITECTURES\n",
      "================================================================================\n",
      "\n",
      "[1/43] ğŸ”¥ ULTRA AGGRESSIVE EfficientNet-B5\n",
      "----------------------------------------------------------------------\n",
      "      ğŸ§Š FROZEN BACKBONE STRATEGY:\n",
      "      ğŸ”§ Creating EfficientNet-B5...\n",
      "         ğŸ¯ Trying pretrained: efficientnet_b5\n",
      "         âœ… SUCCESS with pretrained weights!\n",
      "         ğŸ§Š Freezing backbone layers...\n",
      "         ğŸ“Š Loaded: 28,351,029 total, 1,058,821 trainable (pretrained)\n",
      "      ğŸ”¥ ULTRA AGGRESSIVE TRAINING: EfficientNet-B5 (frozen)...\n",
      "         ğŸ“Š ULTRA SETUP: 100 epochs, LR=0.01, patience=50\n",
      "         Epoch   0: Train 22.1%, Val 18.5% (gap: +3.6%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   1: Train 22.9%, Val 16.7% (gap: +6.2%), LR: 1.00e-02 \n",
      "         Epoch   2: Train 23.0%, Val 20.4% (gap: +2.6%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   3: Train 25.0%, Val 18.5% (gap: +6.5%), LR: 1.00e-02 \n",
      "         Epoch   4: Train 25.9%, Val 20.4% (gap: +5.6%), LR: 1.00e-02 \n",
      "         Epoch   5: Train 26.3%, Val 20.4% (gap: +5.9%), LR: 1.00e-02 \n",
      "         Epoch   6: Train 25.9%, Val 16.7% (gap: +9.3%), LR: 1.00e-02 \n",
      "         Epoch   7: Train 27.4%, Val 22.2% (gap: +5.2%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   8: Train 27.7%, Val 25.9% (gap: +1.8%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   9: Train 28.6%, Val 22.2% (gap: +6.3%), LR: 1.00e-02 \n",
      "         Epoch  10: Train 28.0%, Val 33.3% (gap: -5.4%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch  14: Train 28.7%, Val 37.0% (gap: -8.4%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch  15: Train 28.6%, Val 33.3% (gap: -4.7%), LR: 1.00e-02 \n",
      "         Epoch  20: Train 30.8%, Val 31.5% (gap: -0.7%), LR: 5.00e-03 \n",
      "         Epoch  25: Train 32.3%, Val 18.5% (gap: +13.8%), LR: 5.00e-03 \n",
      "         Epoch  26: Train 32.1%, Val 40.7% (gap: -8.6%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch  30: Train 33.3%, Val 22.2% (gap: +11.0%), LR: 5.00e-03 \n",
      "         Epoch  35: Train 33.1%, Val 27.8% (gap: +5.3%), LR: 5.00e-03 \n",
      "         Epoch  40: Train 34.6%, Val 33.3% (gap: +1.2%), LR: 2.50e-03 \n",
      "         Epoch  45: Train 35.1%, Val 29.6% (gap: +5.5%), LR: 2.50e-03 \n",
      "         Epoch  50: Train 36.0%, Val 38.9% (gap: -2.9%), LR: 2.50e-03 \n",
      "         Epoch  55: Train 36.4%, Val 31.5% (gap: +4.9%), LR: 2.50e-03 \n",
      "         Epoch  60: Train 37.3%, Val 38.9% (gap: -1.6%), LR: 1.25e-03 \n",
      "         Epoch  65: Train 39.0%, Val 35.2% (gap: +3.8%), LR: 1.25e-03 \n",
      "         Epoch  70: Train 38.6%, Val 27.8% (gap: +10.9%), LR: 1.25e-03 \n",
      "         Epoch  75: Train 38.5%, Val 33.3% (gap: +5.2%), LR: 1.25e-03 \n",
      "         Epoch  80: Train 39.6%, Val 35.2% (gap: +4.4%), LR: 6.25e-04 \n",
      "         Epoch  85: Train 40.3%, Val 31.5% (gap: +8.8%), LR: 6.25e-04 \n",
      "         Epoch  90: Train 41.0%, Val 40.7% (gap: +0.2%), LR: 6.25e-04 \n",
      "         Epoch  91: Train 40.5%, Val 31.5% (gap: +9.0%), LR: 6.25e-04 \n",
      "         Epoch  92: Train 40.6%, Val 37.0% (gap: +3.6%), LR: 6.25e-04 \n",
      "         Epoch  93: Train 40.6%, Val 22.2% (gap: +18.4%), LR: 6.25e-04 \n",
      "         Epoch  94: Train 40.6%, Val 35.2% (gap: +5.4%), LR: 6.25e-04 \n",
      "         Epoch  95: Train 41.0%, Val 31.5% (gap: +9.5%), LR: 6.25e-04 \n",
      "         Epoch  96: Train 41.0%, Val 20.4% (gap: +20.6%), LR: 6.25e-04 \n",
      "         Epoch  97: Train 41.8%, Val 35.2% (gap: +6.6%), LR: 6.25e-04 \n",
      "         Epoch  98: Train 40.7%, Val 29.6% (gap: +11.1%), LR: 6.25e-04 \n",
      "         Epoch  99: Train 40.9%, Val 33.3% (gap: +7.6%), LR: 3.13e-04 \n",
      "         ğŸ¯ EfficientNet-B5 (frozen) FINAL: Val 40.7%, Test 37.5%\n",
      "      â„ï¸ PARTIAL FREEZE STRATEGY:\n",
      "      ğŸ”§ Creating EfficientNet-B5...\n",
      "         ğŸ¯ Trying pretrained: efficientnet_b5\n",
      "         âœ… SUCCESS with pretrained weights!\n",
      "         â„ï¸ Partial freeze (last 30% unfrozen)...\n",
      "         ğŸ“Š Loaded: 28,351,029 total, 22,663,665 trainable (pretrained)\n",
      "      ğŸ”¥ ULTRA AGGRESSIVE TRAINING: EfficientNet-B5 (partial)...\n",
      "         ğŸ“Š ULTRA SETUP: 100 epochs, LR=0.005, patience=50\n",
      "         Epoch   0: Train 19.6%, Val 16.7% (gap: +2.9%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch   1: Train 20.3%, Val 22.2% (gap: -2.0%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch   2: Train 20.9%, Val 20.4% (gap: +0.5%), LR: 5.00e-03 \n",
      "         Epoch   3: Train 19.8%, Val 22.2% (gap: -2.4%), LR: 5.00e-03 \n",
      "         Epoch   4: Train 20.6%, Val 24.1% (gap: -3.5%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch   5: Train 21.4%, Val 18.5% (gap: +2.9%), LR: 5.00e-03 \n",
      "         Epoch   6: Train 22.4%, Val 18.5% (gap: +3.9%), LR: 5.00e-03 \n",
      "         Epoch   7: Train 24.9%, Val 16.7% (gap: +8.2%), LR: 5.00e-03 \n",
      "         Epoch   8: Train 27.0%, Val 20.4% (gap: +6.6%), LR: 5.00e-03 \n",
      "         Epoch   9: Train 28.9%, Val 18.5% (gap: +10.4%), LR: 5.00e-03 \n",
      "         Epoch  10: Train 30.9%, Val 22.2% (gap: +8.6%), LR: 5.00e-03 \n",
      "         Epoch  13: Train 34.9%, Val 29.6% (gap: +5.3%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch  15: Train 37.4%, Val 25.9% (gap: +11.4%), LR: 5.00e-03 \n",
      "         Epoch  20: Train 42.0%, Val 22.2% (gap: +19.8%), LR: 2.50e-03 \n",
      "         Epoch  24: Train 44.9%, Val 31.5% (gap: +13.4%), LR: 2.50e-03 ğŸ”¥\n",
      "         Epoch  25: Train 44.2%, Val 24.1% (gap: +20.1%), LR: 2.50e-03 \n",
      "         Epoch  30: Train 47.3%, Val 22.2% (gap: +25.1%), LR: 2.50e-03 \n",
      "         Epoch  35: Train 49.0%, Val 22.2% (gap: +26.7%), LR: 2.50e-03 \n",
      "         Epoch  40: Train 52.9%, Val 29.6% (gap: +23.2%), LR: 1.25e-03 \n",
      "         Epoch  45: Train 55.2%, Val 24.1% (gap: +31.1%), LR: 1.25e-03 \n",
      "         Epoch  50: Train 56.9%, Val 31.5% (gap: +25.4%), LR: 1.25e-03 \n",
      "         Epoch  52: Train 57.4%, Val 33.3% (gap: +24.0%), LR: 1.25e-03 ğŸ”¥\n",
      "         Epoch  53: Train 57.7%, Val 35.2% (gap: +22.5%), LR: 1.25e-03 ğŸ”¥\n",
      "         Epoch  55: Train 58.0%, Val 25.9% (gap: +32.1%), LR: 1.25e-03 \n",
      "         Epoch  60: Train 60.3%, Val 29.6% (gap: +30.7%), LR: 6.25e-04 \n",
      "         Epoch  65: Train 60.2%, Val 24.1% (gap: +36.2%), LR: 6.25e-04 \n",
      "         Epoch  70: Train 61.2%, Val 29.6% (gap: +31.6%), LR: 6.25e-04 \n",
      "         Epoch  75: Train 61.4%, Val 29.6% (gap: +31.8%), LR: 6.25e-04 \n",
      "         Epoch  80: Train 62.8%, Val 33.3% (gap: +29.5%), LR: 3.13e-04 \n",
      "         Epoch  85: Train 63.0%, Val 29.6% (gap: +33.3%), LR: 3.13e-04 \n",
      "         Epoch  90: Train 62.8%, Val 25.9% (gap: +36.8%), LR: 3.13e-04 \n",
      "         Epoch  91: Train 63.6%, Val 27.8% (gap: +35.8%), LR: 3.13e-04 \n",
      "         Epoch  92: Train 64.0%, Val 27.8% (gap: +36.2%), LR: 3.13e-04 \n",
      "         Epoch  93: Train 63.2%, Val 20.4% (gap: +42.8%), LR: 3.13e-04 \n",
      "         Epoch  94: Train 63.8%, Val 22.2% (gap: +41.5%), LR: 3.13e-04 \n",
      "         Epoch  95: Train 62.9%, Val 35.2% (gap: +27.8%), LR: 3.13e-04 \n",
      "         Epoch  96: Train 63.4%, Val 24.1% (gap: +39.4%), LR: 3.13e-04 \n",
      "         Epoch  97: Train 63.4%, Val 27.8% (gap: +35.6%), LR: 3.13e-04 \n",
      "         Epoch  98: Train 63.5%, Val 20.4% (gap: +43.2%), LR: 3.13e-04 \n",
      "         Epoch  99: Train 63.8%, Val 24.1% (gap: +39.8%), LR: 1.56e-04 \n",
      "         ğŸ¯ EfficientNet-B5 (partial) FINAL: Val 35.2%, Test 29.2%\n",
      "      ğŸ† Best EfficientNet-B5: EfficientNet-B5-Frozen (37.5%)\n",
      "      â±ï¸ Total time for EfficientNet-B5: 11759.2s\n",
      "\n",
      "[2/43] ğŸ”¥ ULTRA AGGRESSIVE EfficientNet-B6\n",
      "----------------------------------------------------------------------\n",
      "      ğŸ§Š FROZEN BACKBONE STRATEGY:\n",
      "      ğŸ”§ Creating EfficientNet-B6...\n",
      "         ğŸ¯ Trying pretrained: efficientnet_b6\n",
      "         âŒ Pretrained failed: No pretrained weights exist for efficientnet_b6. U...\n",
      "         ğŸ² Falling back to random initialization: efficientnet_b6\n",
      "         âœ… SUCCESS with random initialization!\n",
      "         ğŸ§Š Freezing backbone layers...\n",
      "         ğŸ“Š Loaded: 40,747,229 total, 1,338,629 trainable (random)\n",
      "      ğŸ”¥ ULTRA AGGRESSIVE TRAINING: EfficientNet-B6 (frozen)...\n",
      "         ğŸ“Š ULTRA SETUP: 100 epochs, LR=0.01, patience=50\n",
      "         Epoch   0: Train 20.2%, Val 24.1% (gap: -3.8%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   1: Train 20.3%, Val 13.0% (gap: +7.3%), LR: 1.00e-02 \n",
      "         Epoch   2: Train 19.5%, Val 27.8% (gap: -8.3%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   3: Train 21.2%, Val 13.0% (gap: +8.3%), LR: 1.00e-02 \n",
      "         Epoch   4: Train 20.0%, Val 22.2% (gap: -2.3%), LR: 1.00e-02 \n",
      "         Epoch   5: Train 20.4%, Val 22.2% (gap: -1.9%), LR: 1.00e-02 \n",
      "         Epoch   6: Train 20.2%, Val 16.7% (gap: +3.5%), LR: 1.00e-02 \n",
      "         Epoch   7: Train 19.7%, Val 20.4% (gap: -0.6%), LR: 1.00e-02 \n",
      "         Epoch   8: Train 20.6%, Val 25.9% (gap: -5.3%), LR: 1.00e-02 \n",
      "         Epoch   9: Train 20.0%, Val 24.1% (gap: -4.0%), LR: 1.00e-02 \n",
      "         Epoch  10: Train 20.7%, Val 27.8% (gap: -7.1%), LR: 1.00e-02 \n",
      "         Epoch  15: Train 19.8%, Val 27.8% (gap: -7.9%), LR: 1.00e-02 \n",
      "         Epoch  20: Train 20.7%, Val 27.8% (gap: -7.1%), LR: 5.00e-03 \n",
      "         Epoch  25: Train 20.7%, Val 18.5% (gap: +2.2%), LR: 5.00e-03 \n",
      "         Epoch  30: Train 20.3%, Val 13.0% (gap: +7.3%), LR: 5.00e-03 \n",
      "         Epoch  35: Train 20.3%, Val 16.7% (gap: +3.6%), LR: 5.00e-03 \n",
      "         Epoch  40: Train 21.1%, Val 22.2% (gap: -1.1%), LR: 2.50e-03 \n",
      "         Epoch  45: Train 21.2%, Val 24.1% (gap: -2.8%), LR: 2.50e-03 \n",
      "         Epoch  50: Train 21.3%, Val 20.4% (gap: +0.9%), LR: 2.50e-03 \n",
      "         Epoch  55: Train 21.4%, Val 22.2% (gap: -0.8%), LR: 2.50e-03 \n",
      "         Epoch  60: Train 21.0%, Val 20.4% (gap: +0.7%), LR: 1.25e-03 \n",
      "         Epoch  61: Train 21.4%, Val 29.6% (gap: -8.3%), LR: 1.25e-03 ğŸ”¥\n",
      "         Epoch  65: Train 21.3%, Val 18.5% (gap: +2.7%), LR: 1.25e-03 \n",
      "         Epoch  70: Train 20.4%, Val 18.5% (gap: +1.8%), LR: 1.25e-03 \n",
      "         Epoch  75: Train 21.1%, Val 14.8% (gap: +6.3%), LR: 1.25e-03 \n",
      "         Epoch  78: Train 20.6%, Val 31.5% (gap: -10.8%), LR: 1.25e-03 ğŸ”¥\n",
      "         Epoch  80: Train 20.2%, Val 22.2% (gap: -2.0%), LR: 6.25e-04 \n",
      "         Epoch  85: Train 22.0%, Val 18.5% (gap: +3.5%), LR: 6.25e-04 \n",
      "         Epoch  90: Train 22.1%, Val 14.8% (gap: +7.3%), LR: 6.25e-04 \n",
      "         Epoch  91: Train 21.0%, Val 14.8% (gap: +6.2%), LR: 6.25e-04 \n",
      "         Epoch  92: Train 21.3%, Val 20.4% (gap: +0.9%), LR: 6.25e-04 \n",
      "         Epoch  93: Train 21.3%, Val 27.8% (gap: -6.5%), LR: 6.25e-04 \n",
      "         Epoch  94: Train 21.7%, Val 24.1% (gap: -2.4%), LR: 6.25e-04 \n",
      "         Epoch  95: Train 21.9%, Val 31.5% (gap: -9.6%), LR: 6.25e-04 \n",
      "         Epoch  96: Train 20.6%, Val 18.5% (gap: +2.1%), LR: 6.25e-04 \n",
      "         Epoch  97: Train 20.9%, Val 27.8% (gap: -6.9%), LR: 6.25e-04 \n",
      "         Epoch  98: Train 21.6%, Val 18.5% (gap: +3.1%), LR: 6.25e-04 \n",
      "         Epoch  99: Train 20.7%, Val 20.4% (gap: +0.4%), LR: 3.13e-04 \n",
      "         ğŸ¯ EfficientNet-B6 (frozen) FINAL: Val 31.5%, Test 18.1%\n",
      "      ğŸ† Best EfficientNet-B6: EfficientNet-B6-Frozen (18.1%)\n",
      "      â±ï¸ Total time for EfficientNet-B6: 5738.8s\n",
      "\n",
      "[3/43] ğŸ”¥ ULTRA AGGRESSIVE EfficientNet-B7\n",
      "----------------------------------------------------------------------\n",
      "      ğŸ§Š FROZEN BACKBONE STRATEGY:\n",
      "      ğŸ”§ Creating EfficientNet-B7...\n",
      "         ğŸ¯ Trying pretrained: efficientnet_b7\n",
      "         âŒ Pretrained failed: No pretrained weights exist for efficientnet_b7. U...\n",
      "         ğŸ² Falling back to random initialization: efficientnet_b7\n",
      "         âœ… SUCCESS with random initialization!\n",
      "         ğŸ§Š Freezing backbone layers...\n",
      "         ğŸ“Š Loaded: 63,799,765 total, 1,651,205 trainable (random)\n",
      "      ğŸ”¥ ULTRA AGGRESSIVE TRAINING: EfficientNet-B7 (frozen)...\n",
      "         ğŸ“Š ULTRA SETUP: 100 epochs, LR=0.01, patience=50\n",
      "         Epoch   0: Train 20.3%, Val 16.7% (gap: +3.6%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   1: Train 20.0%, Val 13.0% (gap: +7.0%), LR: 1.00e-02 \n",
      "         Epoch   2: Train 20.1%, Val 14.8% (gap: +5.3%), LR: 1.00e-02 \n",
      "         Epoch   3: Train 20.4%, Val 16.7% (gap: +3.7%), LR: 1.00e-02 \n",
      "         Epoch   4: Train 20.6%, Val 22.2% (gap: -1.6%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   5: Train 19.9%, Val 18.5% (gap: +1.4%), LR: 1.00e-02 \n",
      "         Epoch   6: Train 21.1%, Val 22.2% (gap: -1.1%), LR: 1.00e-02 \n",
      "         Epoch   7: Train 20.5%, Val 20.4% (gap: +0.1%), LR: 1.00e-02 \n",
      "         Epoch   8: Train 20.2%, Val 24.1% (gap: -3.9%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   9: Train 20.4%, Val 20.4% (gap: +0.0%), LR: 1.00e-02 \n",
      "         Epoch  10: Train 20.5%, Val 16.7% (gap: +3.9%), LR: 1.00e-02 \n",
      "         Epoch  15: Train 19.9%, Val 18.5% (gap: +1.4%), LR: 1.00e-02 \n",
      "         Epoch  20: Train 20.8%, Val 18.5% (gap: +2.3%), LR: 5.00e-03 \n",
      "         Epoch  22: Train 21.0%, Val 25.9% (gap: -4.9%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch  25: Train 20.0%, Val 18.5% (gap: +1.5%), LR: 5.00e-03 \n",
      "         Epoch  30: Train 19.8%, Val 20.4% (gap: -0.6%), LR: 5.00e-03 \n",
      "         Epoch  35: Train 20.2%, Val 18.5% (gap: +1.7%), LR: 5.00e-03 \n",
      "         Epoch  40: Train 21.0%, Val 18.5% (gap: +2.5%), LR: 2.50e-03 \n",
      "         Epoch  45: Train 20.8%, Val 24.1% (gap: -3.3%), LR: 2.50e-03 \n",
      "         Epoch  50: Train 21.1%, Val 18.5% (gap: +2.6%), LR: 2.50e-03 \n",
      "         Epoch  51: Train 20.2%, Val 29.6% (gap: -9.4%), LR: 2.50e-03 ğŸ”¥\n",
      "         Epoch  55: Train 20.4%, Val 20.4% (gap: +0.0%), LR: 2.50e-03 \n",
      "         Epoch  60: Train 21.1%, Val 27.8% (gap: -6.7%), LR: 1.25e-03 \n",
      "         Epoch  65: Train 20.9%, Val 27.8% (gap: -6.9%), LR: 1.25e-03 \n",
      "         Epoch  70: Train 20.6%, Val 22.2% (gap: -1.6%), LR: 1.25e-03 \n",
      "         Epoch  75: Train 20.5%, Val 16.7% (gap: +3.8%), LR: 1.25e-03 \n",
      "         Epoch  80: Train 20.7%, Val 18.5% (gap: +2.2%), LR: 6.25e-04 \n",
      "         Epoch  85: Train 21.6%, Val 18.5% (gap: +3.0%), LR: 6.25e-04 \n",
      "         Epoch  90: Train 21.4%, Val 18.5% (gap: +2.8%), LR: 6.25e-04 \n",
      "         Epoch  91: Train 22.3%, Val 22.2% (gap: +0.0%), LR: 6.25e-04 \n",
      "         Epoch  92: Train 21.7%, Val 18.5% (gap: +3.2%), LR: 6.25e-04 \n",
      "         Epoch  93: Train 21.9%, Val 27.8% (gap: -5.9%), LR: 6.25e-04 \n",
      "         Epoch  94: Train 21.5%, Val 13.0% (gap: +8.5%), LR: 6.25e-04 \n",
      "         Epoch  95: Train 20.7%, Val 18.5% (gap: +2.2%), LR: 6.25e-04 \n",
      "         Epoch  96: Train 21.3%, Val 20.4% (gap: +1.0%), LR: 6.25e-04 \n",
      "         Epoch  97: Train 20.9%, Val 24.1% (gap: -3.2%), LR: 6.25e-04 \n",
      "         Epoch  98: Train 21.9%, Val 27.8% (gap: -5.9%), LR: 6.25e-04 \n",
      "         Epoch  99: Train 20.6%, Val 16.7% (gap: +3.9%), LR: 3.13e-04 \n",
      "         ğŸ¯ EfficientNet-B7 (frozen) FINAL: Val 29.6%, Test 15.3%\n",
      "      ğŸ† Best EfficientNet-B7: EfficientNet-B7-Frozen (15.3%)\n",
      "      â±ï¸ Total time for EfficientNet-B7: 7609.5s\n",
      "\n",
      "ğŸ“Š CURRENT TOP 5 (after 3 architectures):\n",
      "   1. EfficientNet-B5-Frozen: 37.5%\n",
      "   2. EfficientNet-B5-Partial: 29.2%\n",
      "   3. EfficientNet-B6-Frozen: 18.1%\n",
      "   4. EfficientNet-B7-Frozen: 15.3%\n",
      "\n",
      "\n",
      "[4/43] ğŸ”¥ ULTRA AGGRESSIVE EfficientNetV2-S\n",
      "----------------------------------------------------------------------\n",
      "      ğŸ§Š FROZEN BACKBONE STRATEGY:\n",
      "      ğŸ”§ Creating EfficientNetV2-S...\n",
      "         ğŸ¯ Trying pretrained: efficientnetv2_s\n",
      "         âŒ Pretrained failed: No pretrained weights exist for efficientnetv2_s. ...\n",
      "         ğŸ¯ Trying alternative pretrained: tf_efficientnetv2_s_in21ft1k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41916630a10b4d739016f29ae2c6abc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/86.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         âœ… SUCCESS with alternative pretrained weights!\n",
      "         ğŸ§Š Freezing backbone layers...\n",
      "         ğŸ“Š Loaded: 20,183,893 total, 334,085 trainable (pretrained_alt)\n",
      "      ğŸ”¥ ULTRA AGGRESSIVE TRAINING: EfficientNetV2-S (frozen)...\n",
      "         ğŸ“Š ULTRA SETUP: 100 epochs, LR=0.01, patience=50\n",
      "         Epoch   0: Train 21.9%, Val 24.1% (gap: -2.1%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   1: Train 21.4%, Val 16.7% (gap: +4.7%), LR: 1.00e-02 \n",
      "         Epoch   2: Train 22.6%, Val 18.5% (gap: +4.1%), LR: 1.00e-02 \n",
      "         Epoch   3: Train 23.8%, Val 13.0% (gap: +10.9%), LR: 1.00e-02 \n",
      "         Epoch   4: Train 23.5%, Val 18.5% (gap: +4.9%), LR: 1.00e-02 \n",
      "         Epoch   5: Train 25.0%, Val 20.4% (gap: +4.6%), LR: 1.00e-02 \n",
      "         Epoch   6: Train 24.5%, Val 18.5% (gap: +6.0%), LR: 1.00e-02 \n",
      "         Epoch   7: Train 25.1%, Val 20.4% (gap: +4.8%), LR: 1.00e-02 \n",
      "         Epoch   8: Train 25.7%, Val 24.1% (gap: +1.6%), LR: 1.00e-02 \n",
      "         Epoch   9: Train 25.2%, Val 16.7% (gap: +8.5%), LR: 1.00e-02 \n",
      "         Epoch  10: Train 26.2%, Val 16.7% (gap: +9.6%), LR: 1.00e-02 \n",
      "         Epoch  11: Train 26.3%, Val 31.5% (gap: -5.2%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch  15: Train 25.9%, Val 16.7% (gap: +9.3%), LR: 1.00e-02 \n",
      "         Epoch  20: Train 28.9%, Val 27.8% (gap: +1.1%), LR: 5.00e-03 \n",
      "         Epoch  25: Train 29.8%, Val 27.8% (gap: +2.0%), LR: 5.00e-03 \n",
      "         Epoch  26: Train 29.6%, Val 33.3% (gap: -3.7%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch  30: Train 30.5%, Val 18.5% (gap: +12.0%), LR: 5.00e-03 \n",
      "         Epoch  35: Train 30.0%, Val 27.8% (gap: +2.3%), LR: 5.00e-03 \n",
      "         Epoch  40: Train 31.2%, Val 20.4% (gap: +10.9%), LR: 2.50e-03 \n",
      "         Epoch  45: Train 32.1%, Val 27.8% (gap: +4.4%), LR: 2.50e-03 \n",
      "         Epoch  50: Train 32.2%, Val 31.5% (gap: +0.8%), LR: 2.50e-03 \n",
      "         Epoch  53: Train 33.1%, Val 38.9% (gap: -5.8%), LR: 2.50e-03 ğŸ”¥\n",
      "         Epoch  55: Train 32.7%, Val 24.1% (gap: +8.6%), LR: 2.50e-03 \n",
      "         Epoch  60: Train 33.9%, Val 29.6% (gap: +4.3%), LR: 1.25e-03 \n",
      "         Epoch  65: Train 35.3%, Val 27.8% (gap: +7.5%), LR: 1.25e-03 \n",
      "         Epoch  70: Train 35.9%, Val 29.6% (gap: +6.2%), LR: 1.25e-03 \n",
      "         Epoch  75: Train 35.3%, Val 42.6% (gap: -7.2%), LR: 1.25e-03 ğŸ”¥\n",
      "         Epoch  80: Train 37.2%, Val 42.6% (gap: -5.4%), LR: 6.25e-04 \n",
      "         Epoch  85: Train 37.0%, Val 18.5% (gap: +18.5%), LR: 6.25e-04 \n",
      "         Epoch  90: Train 36.8%, Val 20.4% (gap: +16.5%), LR: 6.25e-04 \n",
      "         Epoch  91: Train 37.3%, Val 29.6% (gap: +7.7%), LR: 6.25e-04 \n",
      "         Epoch  92: Train 36.4%, Val 29.6% (gap: +6.8%), LR: 6.25e-04 \n",
      "         Epoch  93: Train 37.6%, Val 38.9% (gap: -1.3%), LR: 6.25e-04 \n",
      "         Epoch  94: Train 36.2%, Val 22.2% (gap: +14.0%), LR: 6.25e-04 \n",
      "         Epoch  95: Train 37.3%, Val 27.8% (gap: +9.5%), LR: 6.25e-04 \n",
      "         Epoch  96: Train 36.8%, Val 25.9% (gap: +10.8%), LR: 6.25e-04 \n",
      "         Epoch  97: Train 37.0%, Val 38.9% (gap: -1.9%), LR: 6.25e-04 \n",
      "         Epoch  98: Train 37.0%, Val 27.8% (gap: +9.2%), LR: 6.25e-04 \n",
      "         Epoch  99: Train 37.2%, Val 38.9% (gap: -1.6%), LR: 3.13e-04 \n",
      "         ğŸ¯ EfficientNetV2-S (frozen) FINAL: Val 42.6%, Test 31.9%\n",
      "      â„ï¸ PARTIAL FREEZE STRATEGY:\n",
      "      ğŸ”§ Creating EfficientNetV2-S...\n",
      "         ğŸ¯ Trying pretrained: efficientnetv2_s\n",
      "         âŒ Pretrained failed: No pretrained weights exist for efficientnetv2_s. ...\n",
      "         ğŸ¯ Trying alternative pretrained: tf_efficientnetv2_s_in21ft1k\n",
      "         âœ… SUCCESS with alternative pretrained weights!\n",
      "         â„ï¸ Partial freeze (last 30% unfrozen)...\n",
      "         ğŸ“Š Loaded: 20,183,893 total, 10,388,101 trainable (pretrained_alt)\n",
      "      ğŸ”¥ ULTRA AGGRESSIVE TRAINING: EfficientNetV2-S (partial)...\n",
      "         ğŸ“Š ULTRA SETUP: 100 epochs, LR=0.005, patience=50\n",
      "         Epoch   0: Train 21.9%, Val 16.7% (gap: +5.2%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch   1: Train 22.6%, Val 24.1% (gap: -1.5%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch   2: Train 23.4%, Val 16.7% (gap: +6.7%), LR: 5.00e-03 \n",
      "         Epoch   3: Train 25.2%, Val 18.5% (gap: +6.7%), LR: 5.00e-03 \n",
      "         Epoch   4: Train 28.2%, Val 13.0% (gap: +15.2%), LR: 5.00e-03 \n",
      "         Epoch   5: Train 28.6%, Val 24.1% (gap: +4.5%), LR: 5.00e-03 \n",
      "         Epoch   6: Train 29.8%, Val 22.2% (gap: +7.6%), LR: 5.00e-03 \n",
      "         Epoch   7: Train 30.6%, Val 18.5% (gap: +12.1%), LR: 5.00e-03 \n",
      "         Epoch   8: Train 31.4%, Val 27.8% (gap: +3.6%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch   9: Train 31.8%, Val 18.5% (gap: +13.3%), LR: 5.00e-03 \n",
      "         Epoch  10: Train 32.3%, Val 16.7% (gap: +15.6%), LR: 5.00e-03 \n",
      "         Epoch  15: Train 35.1%, Val 29.6% (gap: +5.4%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch  20: Train 37.4%, Val 31.5% (gap: +5.9%), LR: 2.50e-03 ğŸ”¥\n",
      "         Epoch  25: Train 41.1%, Val 22.2% (gap: +18.9%), LR: 2.50e-03 \n",
      "         Epoch  26: Train 40.4%, Val 37.0% (gap: +3.3%), LR: 2.50e-03 ğŸ”¥\n",
      "         Epoch  29: Train 42.6%, Val 38.9% (gap: +3.7%), LR: 2.50e-03 ğŸ”¥\n",
      "         Epoch  30: Train 43.6%, Val 29.6% (gap: +14.0%), LR: 2.50e-03 \n",
      "         Epoch  35: Train 45.0%, Val 27.8% (gap: +17.3%), LR: 2.50e-03 \n",
      "         Epoch  40: Train 49.1%, Val 27.8% (gap: +21.3%), LR: 1.25e-03 \n",
      "         Epoch  45: Train 51.5%, Val 33.3% (gap: +18.2%), LR: 1.25e-03 \n",
      "         Epoch  50: Train 52.3%, Val 29.6% (gap: +22.7%), LR: 1.25e-03 \n",
      "         Epoch  55: Train 53.3%, Val 40.7% (gap: +12.6%), LR: 1.25e-03 ğŸ”¥\n",
      "         Epoch  60: Train 55.2%, Val 33.3% (gap: +21.9%), LR: 6.25e-04 \n",
      "         Epoch  65: Train 55.8%, Val 24.1% (gap: +31.7%), LR: 6.25e-04 \n",
      "         Epoch  70: Train 56.7%, Val 24.1% (gap: +32.6%), LR: 6.25e-04 \n",
      "         Epoch  75: Train 57.3%, Val 24.1% (gap: +33.2%), LR: 6.25e-04 \n",
      "         Epoch  80: Train 58.4%, Val 37.0% (gap: +21.4%), LR: 3.13e-04 \n",
      "         Epoch  81: Train 58.7%, Val 44.4% (gap: +14.3%), LR: 3.13e-04 ğŸ”¥\n",
      "         Epoch  85: Train 58.8%, Val 38.9% (gap: +19.9%), LR: 3.13e-04 \n",
      "         Epoch  90: Train 59.1%, Val 25.9% (gap: +33.2%), LR: 3.13e-04 \n",
      "         Epoch  91: Train 59.8%, Val 25.9% (gap: +33.9%), LR: 3.13e-04 \n",
      "         Epoch  92: Train 59.1%, Val 37.0% (gap: +22.1%), LR: 3.13e-04 \n",
      "         Epoch  93: Train 59.5%, Val 24.1% (gap: +35.4%), LR: 3.13e-04 \n",
      "         Epoch  94: Train 59.8%, Val 29.6% (gap: +30.1%), LR: 3.13e-04 \n",
      "         Epoch  95: Train 59.8%, Val 27.8% (gap: +32.0%), LR: 3.13e-04 \n",
      "         Epoch  96: Train 59.3%, Val 40.7% (gap: +18.6%), LR: 3.13e-04 \n",
      "         Epoch  97: Train 59.3%, Val 33.3% (gap: +25.9%), LR: 3.13e-04 \n",
      "         Epoch  98: Train 59.8%, Val 29.6% (gap: +30.1%), LR: 3.13e-04 \n",
      "         Epoch  99: Train 59.7%, Val 35.2% (gap: +24.6%), LR: 1.56e-04 \n",
      "         ğŸ¯ EfficientNetV2-S (partial) FINAL: Val 44.4%, Test 27.8%\n",
      "      ğŸ† Best EfficientNetV2-S: EfficientNetV2-S-Frozen (31.9%)\n",
      "      â±ï¸ Total time for EfficientNetV2-S: 7233.6s\n",
      "\n",
      "[5/43] ğŸ”¥ ULTRA AGGRESSIVE EfficientNetV2-M\n",
      "----------------------------------------------------------------------\n",
      "      ğŸ§Š FROZEN BACKBONE STRATEGY:\n",
      "      ğŸ”§ Creating EfficientNetV2-M...\n",
      "         ğŸ¯ Trying pretrained: efficientnetv2_m\n",
      "         âŒ Pretrained failed: No pretrained weights exist for efficientnetv2_m. ...\n",
      "         ğŸ¯ Trying alternative pretrained: tf_efficientnetv2_m_in21ft1k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282fe691b6ee412781f8c2845b7b4262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         âœ… SUCCESS with alternative pretrained weights!\n",
      "         ğŸ§Š Freezing backbone layers...\n",
      "         ğŸ“Š Loaded: 52,864,761 total, 661,765 trainable (pretrained_alt)\n",
      "      ğŸ”¥ ULTRA AGGRESSIVE TRAINING: EfficientNetV2-M (frozen)...\n",
      "         ğŸ“Š ULTRA SETUP: 100 epochs, LR=0.01, patience=50\n",
      "         Epoch   0: Train 21.6%, Val 25.9% (gap: -4.4%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   1: Train 21.6%, Val 22.2% (gap: -0.6%), LR: 1.00e-02 \n",
      "         Epoch   2: Train 22.8%, Val 20.4% (gap: +2.4%), LR: 1.00e-02 \n",
      "         Epoch   3: Train 24.2%, Val 31.5% (gap: -7.3%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch   4: Train 24.4%, Val 13.0% (gap: +11.4%), LR: 1.00e-02 \n",
      "         Epoch   5: Train 25.8%, Val 13.0% (gap: +12.8%), LR: 1.00e-02 \n",
      "         Epoch   6: Train 25.6%, Val 20.4% (gap: +5.2%), LR: 1.00e-02 \n",
      "         Epoch   7: Train 26.5%, Val 20.4% (gap: +6.2%), LR: 1.00e-02 \n",
      "         Epoch   8: Train 26.1%, Val 16.7% (gap: +9.4%), LR: 1.00e-02 \n",
      "         Epoch   9: Train 26.7%, Val 18.5% (gap: +8.1%), LR: 1.00e-02 \n",
      "         Epoch  10: Train 26.9%, Val 16.7% (gap: +10.2%), LR: 1.00e-02 \n",
      "         Epoch  13: Train 27.8%, Val 37.0% (gap: -9.3%), LR: 1.00e-02 ğŸ”¥\n",
      "         Epoch  15: Train 27.3%, Val 27.8% (gap: -0.5%), LR: 1.00e-02 \n",
      "         Epoch  20: Train 30.0%, Val 25.9% (gap: +4.1%), LR: 5.00e-03 \n",
      "         Epoch  25: Train 31.6%, Val 29.6% (gap: +1.9%), LR: 5.00e-03 \n",
      "         Epoch  30: Train 32.4%, Val 35.2% (gap: -2.8%), LR: 5.00e-03 \n",
      "         Epoch  35: Train 31.5%, Val 22.2% (gap: +9.3%), LR: 5.00e-03 \n",
      "         Epoch  40: Train 34.2%, Val 31.5% (gap: +2.7%), LR: 2.50e-03 \n",
      "         Epoch  44: Train 35.0%, Val 42.6% (gap: -7.6%), LR: 2.50e-03 ğŸ”¥\n",
      "         Epoch  45: Train 35.5%, Val 24.1% (gap: +11.4%), LR: 2.50e-03 \n",
      "         Epoch  50: Train 34.8%, Val 29.6% (gap: +5.1%), LR: 2.50e-03 \n",
      "         Epoch  55: Train 35.6%, Val 29.6% (gap: +6.0%), LR: 2.50e-03 \n",
      "         Epoch  60: Train 37.2%, Val 31.5% (gap: +5.7%), LR: 1.25e-03 \n",
      "         Epoch  65: Train 37.8%, Val 38.9% (gap: -1.1%), LR: 1.25e-03 \n",
      "         Epoch  70: Train 38.5%, Val 31.5% (gap: +7.0%), LR: 1.25e-03 \n",
      "         Epoch  75: Train 37.9%, Val 38.9% (gap: -1.0%), LR: 1.25e-03 \n",
      "         Epoch  80: Train 38.6%, Val 31.5% (gap: +7.1%), LR: 6.25e-04 \n",
      "         Epoch  85: Train 39.5%, Val 27.8% (gap: +11.7%), LR: 6.25e-04 \n",
      "         Epoch  90: Train 40.6%, Val 27.8% (gap: +12.9%), LR: 6.25e-04 \n",
      "         Epoch  91: Train 39.9%, Val 25.9% (gap: +14.0%), LR: 6.25e-04 \n",
      "         Epoch  92: Train 39.6%, Val 31.5% (gap: +8.2%), LR: 6.25e-04 \n",
      "         Epoch  93: Train 40.7%, Val 35.2% (gap: +5.5%), LR: 6.25e-04 \n",
      "         Epoch  94: Train 39.9%, Val 35.2% (gap: +4.7%), LR: 6.25e-04 \n",
      "         Epoch  95: Train 40.7%, Val 33.3% (gap: +7.4%), LR: 6.25e-04 \n",
      "         Epoch  96: Train 40.6%, Val 22.2% (gap: +18.4%), LR: 6.25e-04 \n",
      "         Epoch  97: Train 40.2%, Val 20.4% (gap: +19.8%), LR: 6.25e-04 \n",
      "         Epoch  98: Train 40.8%, Val 29.6% (gap: +11.2%), LR: 6.25e-04 \n",
      "         Epoch  99: Train 40.5%, Val 27.8% (gap: +12.8%), LR: 3.13e-04 \n",
      "         ğŸ¯ EfficientNetV2-M (frozen) FINAL: Val 42.6%, Test 27.8%\n",
      "      â„ï¸ PARTIAL FREEZE STRATEGY:\n",
      "      ğŸ”§ Creating EfficientNetV2-M...\n",
      "         ğŸ¯ Trying pretrained: efficientnetv2_m\n",
      "         âŒ Pretrained failed: No pretrained weights exist for efficientnetv2_m. ...\n",
      "         ğŸ¯ Trying alternative pretrained: tf_efficientnetv2_m_in21ft1k\n",
      "         âœ… SUCCESS with alternative pretrained weights!\n",
      "         â„ï¸ Partial freeze (last 30% unfrozen)...\n",
      "         ğŸ“Š Loaded: 52,864,761 total, 31,910,089 trainable (pretrained_alt)\n",
      "      ğŸ”¥ ULTRA AGGRESSIVE TRAINING: EfficientNetV2-M (partial)...\n",
      "         ğŸ“Š ULTRA SETUP: 100 epochs, LR=0.005, patience=50\n",
      "         Epoch   0: Train 20.9%, Val 37.0% (gap: -16.1%), LR: 5.00e-03 ğŸ”¥\n",
      "         Epoch   1: Train 20.3%, Val 18.5% (gap: +1.8%), LR: 5.00e-03 \n",
      "         Epoch   2: Train 21.3%, Val 16.7% (gap: +4.6%), LR: 5.00e-03 \n",
      "         Epoch   3: Train 22.7%, Val 16.7% (gap: +6.0%), LR: 5.00e-03 \n",
      "         Epoch   4: Train 24.4%, Val 18.5% (gap: +5.9%), LR: 5.00e-03 \n",
      "         Epoch   5: Train 26.4%, Val 24.1% (gap: +2.3%), LR: 5.00e-03 \n",
      "         Epoch   6: Train 28.7%, Val 24.1% (gap: +4.6%), LR: 5.00e-03 \n",
      "         Epoch   7: Train 29.9%, Val 24.1% (gap: +5.9%), LR: 5.00e-03 \n",
      "         Epoch   8: Train 31.1%, Val 20.4% (gap: +10.7%), LR: 5.00e-03 \n",
      "         Epoch   9: Train 32.4%, Val 22.2% (gap: +10.2%), LR: 5.00e-03 \n",
      "         Epoch  10: Train 33.8%, Val 16.7% (gap: +17.1%), LR: 5.00e-03 \n",
      "         Epoch  15: Train 36.3%, Val 16.7% (gap: +19.6%), LR: 5.00e-03 \n",
      "         Epoch  20: Train 40.1%, Val 37.0% (gap: +3.1%), LR: 2.50e-03 \n",
      "         Epoch  21: Train 40.8%, Val 38.9% (gap: +1.9%), LR: 2.50e-03 ğŸ”¥\n",
      "         Epoch  23: Train 42.4%, Val 44.4% (gap: -2.1%), LR: 2.50e-03 ğŸ”¥\n",
      "         Epoch  25: Train 43.2%, Val 24.1% (gap: +19.1%), LR: 2.50e-03 \n",
      "         Epoch  30: Train 46.3%, Val 29.6% (gap: +16.7%), LR: 2.50e-03 \n",
      "         Epoch  35: Train 49.6%, Val 31.5% (gap: +18.2%), LR: 2.50e-03 \n",
      "         Epoch  40: Train 54.0%, Val 33.3% (gap: +20.6%), LR: 1.25e-03 \n",
      "         Epoch  45: Train 56.7%, Val 27.8% (gap: +28.9%), LR: 1.25e-03 \n",
      "         Epoch  50: Train 57.5%, Val 38.9% (gap: +18.6%), LR: 1.25e-03 \n",
      "         Epoch  55: Train 58.0%, Val 18.5% (gap: +39.4%), LR: 1.25e-03 \n",
      "         Epoch  60: Train 60.4%, Val 27.8% (gap: +32.6%), LR: 6.25e-04 \n",
      "         Epoch  65: Train 61.4%, Val 31.5% (gap: +29.9%), LR: 6.25e-04 \n",
      "         Epoch  70: Train 61.8%, Val 35.2% (gap: +26.7%), LR: 6.25e-04 \n",
      "         Epoch  75: Train 61.4%, Val 31.5% (gap: +29.9%), LR: 6.25e-04 \n",
      "         Epoch  80: Train 62.3%, Val 29.6% (gap: +32.7%), LR: 3.13e-04 \n",
      "         Epoch  85: Train 62.7%, Val 22.2% (gap: +40.5%), LR: 3.13e-04 \n",
      "         Epoch  90: Train 62.1%, Val 24.1% (gap: +38.1%), LR: 3.13e-04 \n",
      "         Epoch  91: Train 62.7%, Val 20.4% (gap: +42.4%), LR: 3.13e-04 \n",
      "         Epoch  92: Train 62.2%, Val 37.0% (gap: +25.2%), LR: 3.13e-04 \n",
      "         Epoch  93: Train 62.5%, Val 31.5% (gap: +31.1%), LR: 3.13e-04 \n",
      "         Epoch  94: Train 62.8%, Val 20.4% (gap: +42.4%), LR: 3.13e-04 \n",
      "         Epoch  95: Train 62.9%, Val 18.5% (gap: +44.4%), LR: 3.13e-04 \n",
      "         Epoch  96: Train 63.2%, Val 29.6% (gap: +33.6%), LR: 3.13e-04 \n",
      "         Epoch  97: Train 62.8%, Val 38.9% (gap: +23.9%), LR: 3.13e-04 \n",
      "         Epoch  98: Train 62.6%, Val 31.5% (gap: +31.1%), LR: 3.13e-04 \n",
      "         Epoch  99: Train 62.6%, Val 33.3% (gap: +29.3%), LR: 1.56e-04 \n",
      "         ğŸ¯ EfficientNetV2-M (partial) FINAL: Val 44.4%, Test 33.3%\n",
      "      ğŸ† Best EfficientNetV2-M: EfficientNetV2-M-Partial (33.3%)\n",
      "      â±ï¸ Total time for EfficientNetV2-M: 12721.4s\n",
      "\n",
      "[6/43] ğŸ”¥ ULTRA AGGRESSIVE EfficientNetV2-L\n",
      "----------------------------------------------------------------------\n",
      "      ğŸ§Š FROZEN BACKBONE STRATEGY:\n",
      "      ğŸ”§ Creating EfficientNetV2-L...\n",
      "         ğŸ¯ Trying pretrained: efficientnetv2_l\n",
      "         âŒ Pretrained failed: No pretrained weights exist for efficientnetv2_l. ...\n",
      "         ğŸ¯ Trying alternative pretrained: tf_efficientnetv2_l_in21ft1k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a08f884ecb4ffca1aa9b5599a5517b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         âœ… SUCCESS with alternative pretrained weights!\n",
      "         ğŸ§Š Freezing backbone layers...\n",
      "         ğŸ“Š Loaded: 117,240,677 total, 825,605 trainable (pretrained_alt)\n",
      "      ğŸ”¥ ULTRA AGGRESSIVE TRAINING: EfficientNetV2-L (frozen)...\n",
      "         ğŸ“Š ULTRA SETUP: 100 epochs, LR=0.01, patience=50\n",
      "         Epoch   0: Train 22.2%, Val 18.5% (gap: +3.6%), LR: 1.00e-02 ğŸ”¥\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_original_data():\n",
    "    \"\"\"Load the original 357 images\"\"\"\n",
    "    print(\"ğŸ“‚ LOADING ORIGINAL DATA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load using the user's method\n",
    "    from buck.analysis.basics import ingest_images\n",
    "    \n",
    "    fpath = \"C:\\\\Users\\\\aaron\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*.png\"\n",
    "    images, ages = ingest_images(fpath)\n",
    "    print(f\"   âœ… Loaded {len(images)} original images\")\n",
    "    \n",
    "    # Group ages: 5.5+ all become 5.5 (creating exactly 5 classes)\n",
    "    print(\"   ğŸ”„ Grouping ages: 5.5+ â†’ 5.5\")\n",
    "    ages_grouped = []\n",
    "    for age in ages:\n",
    "        if age >= 5.5:\n",
    "            ages_grouped.append(5.5)\n",
    "        else:\n",
    "            ages_grouped.append(age)\n",
    "    \n",
    "    # Print age distribution before and after grouping\n",
    "    print(f\"   ğŸ“Š Original age distribution: {dict(Counter(ages))}\")\n",
    "    print(f\"   ğŸ“Š Grouped age distribution: {dict(Counter(ages_grouped))}\")\n",
    "    \n",
    "    return images, ages_grouped\n",
    "\n",
    "def create_train_val_test_split(images, ages, test_size=0.2, val_size=0.15, random_state=42):\n",
    "    \"\"\"Create train/validation/test split\"\"\"\n",
    "    print(\"\\nğŸ”€ CREATING TRAIN/VAL/TEST SPLIT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    if not isinstance(images, np.ndarray):\n",
    "        images = np.array(images)\n",
    "    if not isinstance(ages, np.ndarray):\n",
    "        ages = np.array(ages)\n",
    "    \n",
    "    # Check if stratified split is possible\n",
    "    age_counts = Counter(ages)\n",
    "    min_count = min(age_counts.values())\n",
    "    can_stratify = min_count >= 2\n",
    "    \n",
    "    print(f\"   ğŸ“Š Age distribution: {dict(age_counts)}\")\n",
    "    print(f\"   ğŸ“Š Minimum class size: {min_count}\")\n",
    "    print(f\"   ğŸ¯ Can use stratified split: {can_stratify}\")\n",
    "    \n",
    "    if can_stratify:\n",
    "        # First split: separate test set (stratified)\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            images, ages, test_size=test_size, random_state=random_state, stratify=ages\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation from remaining data\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # Adjust for remaining data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, stratify=y_temp\n",
    "        )\n",
    "    else:\n",
    "        print(\"   âš ï¸ Using random split (some classes too small for stratification)\")\n",
    "        # First split: separate test set (random)\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            images, ages, test_size=test_size, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation from remaining data\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # Adjust for remaining data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, shuffle=True\n",
    "        )\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    reverse_mapping = {i: age for age, i in label_mapping.items()}\n",
    "    \n",
    "    print(f\"   ğŸ“Š Train: {len(X_train)} samples\")\n",
    "    print(f\"   ğŸ“Š Val: {len(X_val)} samples\") \n",
    "    print(f\"   ğŸ“Š Test: {len(X_test)} samples\")\n",
    "    print(f\"   ğŸ·ï¸ Label mapping: {label_mapping}\")\n",
    "    print(f\"   ğŸ¯ Number of classes: {len(unique_ages)}\")\n",
    "    \n",
    "    # Convert ages to class indices\n",
    "    y_train_indices = np.array([label_mapping[age] for age in y_train])\n",
    "    y_val_indices = np.array([label_mapping[age] for age in y_val])\n",
    "    y_test_indices = np.array([label_mapping[age] for age in y_test])\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ Train distribution: {Counter(y_train_indices)}\")\n",
    "    print(f\"   ğŸ“ˆ Val distribution: {Counter(y_val_indices)}\")\n",
    "    print(f\"   ğŸ“ˆ Test distribution: {Counter(y_test_indices)}\")\n",
    "    \n",
    "    return (X_train, y_train_indices, X_val, y_val_indices, X_test, y_test_indices, \n",
    "            label_mapping, reverse_mapping)\n",
    "\n",
    "def augment_image(image):\n",
    "    \"\"\"Apply random augmentation to an image\"\"\"\n",
    "    # Ensure image is uint8\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Random rotation\n",
    "    if random.random() < 0.5:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Random horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Random brightness/contrast\n",
    "    if random.random() < 0.5:\n",
    "        alpha = random.uniform(0.8, 1.2)  # Contrast\n",
    "        beta = random.randint(-20, 20)    # Brightness\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Random noise (fixed data type issue)\n",
    "    if random.random() < 0.3:\n",
    "        # Create noise with same dtype as image\n",
    "        noise = np.random.normal(0, 5, image.shape).astype(np.int16)  # Use int16 to handle negative values\n",
    "        # Convert image to int16 for safe addition\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        # Add noise and clip to valid range\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        # Convert back to uint8\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def balance_and_augment_data(X_train, y_train, augment_multiplier=30, num_classes=5):\n",
    "    \"\"\"Balance classes and augment training data\"\"\"\n",
    "    print(f\"\\nğŸ”„ BALANCING AND AUGMENTING DATA\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"   ğŸ¯ Target: {augment_multiplier}x augmentation per class\")\n",
    "    \n",
    "    # Count samples per class\n",
    "    class_counts = Counter(y_train)\n",
    "    print(f\"   ğŸ“Š Original distribution: {dict(class_counts)}\")\n",
    "    \n",
    "    # Find target count (based on largest class * multiplier)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max_count * augment_multiplier\n",
    "    print(f\"   ğŸ¯ Target samples per class: {target_count}\")\n",
    "    \n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Get samples for this class\n",
    "        class_mask = y_train == class_idx\n",
    "        class_images = X_train[class_mask]\n",
    "        class_labels = y_train[class_mask]\n",
    "        \n",
    "        current_count = len(class_images)\n",
    "        needed_count = target_count\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ Class {class_idx}: {current_count} â†’ {needed_count} samples\")\n",
    "        \n",
    "        # Add original samples\n",
    "        X_augmented.extend(class_images)\n",
    "        y_augmented.extend(class_labels)\n",
    "        \n",
    "        # Generate augmented samples\n",
    "        augmented_needed = needed_count - current_count\n",
    "        \n",
    "        for i in range(augmented_needed):\n",
    "            # Pick random original image from this class\n",
    "            original_idx = random.randint(0, current_count - 1)\n",
    "            original_image = class_images[original_idx].copy()\n",
    "            \n",
    "            # Augment it\n",
    "            augmented_image = augment_image(original_image)\n",
    "            \n",
    "            X_augmented.append(augmented_image)\n",
    "            y_augmented.append(class_idx)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    X_augmented = np.array(X_augmented)\n",
    "    y_augmented = np.array(y_augmented)\n",
    "    \n",
    "    print(f\"   âœ… Augmentation complete: {len(X_augmented)} total samples\")\n",
    "    print(f\"   ğŸ“Š Final distribution: {Counter(y_augmented)}\")\n",
    "    \n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    \"\"\"Dataset for deer aging with preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, transform=True):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.FloatTensor(X)\n",
    "        else:\n",
    "            self.X = torch.FloatTensor(np.array(X))\n",
    "            \n",
    "        if isinstance(y, np.ndarray):\n",
    "            self.y = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.y = torch.LongTensor(np.array(y))\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Normalize to [0,1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Ensure CHW format (channels first)\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Resize to 224x224\n",
    "        if image.shape[-2:] != (224, 224):\n",
    "            image = image.unsqueeze(0)\n",
    "            image = F.interpolate(image, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            image = image.squeeze(0)\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        image = (image - mean) / std\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class CompleteDeerAgeTrainer:\n",
    "    \"\"\"Complete deer age trainer starting from EfficientNet-B5\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.all_results = []\n",
    "        print(f\"ğŸ”¥ COMPLETE DEER AGE TRAINER\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Classes: {num_classes}\")\n",
    "    \n",
    "    def get_all_architectures_with_fallback(self):\n",
    "        \"\"\"Get ALL architectures with graceful fallback for missing pretrained weights\"\"\"\n",
    "        \n",
    "        architectures = {\n",
    "            # EfficientNet COMPLETE SERIES (B5-B7 + variants)\n",
    "            'EfficientNet-B5': {'model_name': 'efficientnet_b5', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-B6': {'model_name': 'efficientnet_b6', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-B7': {'model_name': 'efficientnet_b7', 'family': 'EfficientNet'},\n",
    "            \n",
    "            # EfficientNetV2 (try multiple naming conventions)\n",
    "            'EfficientNetV2-S': {'model_name': 'efficientnetv2_s', 'family': 'EfficientNetV2', 'alternatives': ['tf_efficientnetv2_s_in21ft1k', 'efficientnetv2_rw_s']},\n",
    "            'EfficientNetV2-M': {'model_name': 'efficientnetv2_m', 'family': 'EfficientNetV2', 'alternatives': ['tf_efficientnetv2_m_in21ft1k', 'efficientnetv2_rw_m']},\n",
    "            'EfficientNetV2-L': {'model_name': 'efficientnetv2_l', 'family': 'EfficientNetV2', 'alternatives': ['tf_efficientnetv2_l_in21ft1k']},\n",
    "            \n",
    "            # DenseNet Family\n",
    "            'DenseNet-121': {'model_name': 'densenet121', 'family': 'DenseNet'},\n",
    "            'DenseNet-161': {'model_name': 'densenet161', 'family': 'DenseNet'},\n",
    "            'DenseNet-169': {'model_name': 'densenet169', 'family': 'DenseNet'},\n",
    "            'DenseNet-201': {'model_name': 'densenet201', 'family': 'DenseNet'},\n",
    "            \n",
    "            # ResNet Family\n",
    "            'ResNet-18': {'model_name': 'resnet18', 'family': 'ResNet'},\n",
    "            'ResNet-34': {'model_name': 'resnet34', 'family': 'ResNet'},\n",
    "            'ResNet-50': {'model_name': 'resnet50', 'family': 'ResNet'},\n",
    "            'ResNet-101': {'model_name': 'resnet101', 'family': 'ResNet'},\n",
    "            'ResNet-152': {'model_name': 'resnet152', 'family': 'ResNet'},\n",
    "            \n",
    "            # ResNeXt\n",
    "            'ResNeXt-50': {'model_name': 'resnext50_32x4d', 'family': 'ResNeXt'},\n",
    "            'ResNeXt-101': {'model_name': 'resnext101_32x8d', 'family': 'ResNeXt'},\n",
    "            \n",
    "            # Wide ResNet\n",
    "            'Wide-ResNet-50': {'model_name': 'wide_resnet50_2', 'family': 'Wide-ResNet'},\n",
    "            'Wide-ResNet-101': {'model_name': 'wide_resnet101_2', 'family': 'Wide-ResNet'},\n",
    "            \n",
    "            # MobileNet Family\n",
    "            'MobileNetV2': {'model_name': 'mobilenetv2_100', 'family': 'MobileNet'},\n",
    "            'MobileNetV3-Small': {'model_name': 'mobilenetv3_small_100', 'family': 'MobileNet'},\n",
    "            'MobileNetV3-Large': {'model_name': 'mobilenetv3_large_100', 'family': 'MobileNet'},\n",
    "            \n",
    "            # RegNet Family (try multiple naming conventions)\n",
    "            'RegNetX-400MF': {'model_name': 'regnetx_400mf', 'family': 'RegNet', 'alternatives': ['regnetx_002', 'regnetx_004']},\n",
    "            'RegNetX-800MF': {'model_name': 'regnetx_800mf', 'family': 'RegNet', 'alternatives': ['regnetx_004', 'regnetx_006']},\n",
    "            'RegNetY-400MF': {'model_name': 'regnety_400mf', 'family': 'RegNet', 'alternatives': ['regnety_002', 'regnety_004']},\n",
    "            'RegNetY-800MF': {'model_name': 'regnety_800mf', 'family': 'RegNet', 'alternatives': ['regnety_004', 'regnety_006']},\n",
    "            \n",
    "            # ConvNeXt Family\n",
    "            'ConvNeXt-Tiny': {'model_name': 'convnext_tiny', 'family': 'ConvNeXt', 'alternatives': ['convnext_tiny_in22ft1k']},\n",
    "            'ConvNeXt-Small': {'model_name': 'convnext_small', 'family': 'ConvNeXt', 'alternatives': ['convnext_small_in22ft1k']},\n",
    "            'ConvNeXt-Base': {'model_name': 'convnext_base', 'family': 'ConvNeXt', 'alternatives': ['convnext_base_in22ft1k']},\n",
    "            \n",
    "            # Vision Transformer variants\n",
    "            'Swin-Tiny': {'model_name': 'swin_tiny_patch4_window7_224', 'family': 'Swin', 'alternatives': ['swin_tiny_patch4_window7_224_in22k']},\n",
    "            'Swin-Small': {'model_name': 'swin_small_patch4_window7_224', 'family': 'Swin', 'alternatives': ['swin_small_patch4_window7_224_in22k']},\n",
    "            \n",
    "            # VGG (classic)\n",
    "            'VGG-16': {'model_name': 'vgg16', 'family': 'VGG'},\n",
    "            'VGG-19': {'model_name': 'vgg19', 'family': 'VGG'},\n",
    "            \n",
    "            # Vision Transformers\n",
    "            'DeiT-Tiny': {'model_name': 'deit_tiny_patch16_224', 'family': 'DeiT'},\n",
    "            'DeiT-Small': {'model_name': 'deit_small_patch16_224', 'family': 'DeiT'},\n",
    "            'DeiT-Base': {'model_name': 'deit_base_patch16_224', 'family': 'DeiT'},\n",
    "            \n",
    "            # Additional EfficientNet variants\n",
    "            'EfficientNet-ES': {'model_name': 'efficientnet_es', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-EM': {'model_name': 'efficientnet_em', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-EL': {'model_name': 'efficientnet_el', 'family': 'EfficientNet'},\n",
    "            \n",
    "            # Additional strong architectures\n",
    "            'ResNet-26': {'model_name': 'resnet26', 'family': 'ResNet'},\n",
    "            'ResNet-26d': {'model_name': 'resnet26d', 'family': 'ResNet'},\n",
    "            'SEResNet-50': {'model_name': 'seresnet50', 'family': 'SEResNet'},\n",
    "            'SEResNeXt-50': {'model_name': 'seresnext50_32x4d', 'family': 'SEResNeXt'},\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ—ï¸ COMPLETE ARCHITECTURE ARSENAL ({len(architectures)} models)\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"ğŸ¯ FALLBACK STRATEGY: Pretrained â†’ Alternative Names â†’ Random Init\")\n",
    "        print(\"ğŸ“Š ALL models will be tested regardless of pretrained weight availability\")\n",
    "        \n",
    "        # Group by family and show counts\n",
    "        families = {}\n",
    "        for arch_name, arch_info in architectures.items():\n",
    "            family = arch_info['family']\n",
    "            if family not in families:\n",
    "                families[family] = []\n",
    "            families[family].append(arch_name)\n",
    "        \n",
    "        for family, models in families.items():\n",
    "            print(f\"ğŸ“ {family} ({len(models)} models): {', '.join(models)}\")\n",
    "        \n",
    "        return architectures\n",
    "    \n",
    "    def create_model_with_fallback(self, arch_name, arch_info, freeze_strategy='none'):\n",
    "        \"\"\"Create model with graceful fallback for missing pretrained weights\"\"\"\n",
    "        model_name = arch_info['model_name']\n",
    "        alternatives = arch_info.get('alternatives', [])\n",
    "        \n",
    "        print(f\"      ğŸ”§ Creating {arch_name}...\")\n",
    "        \n",
    "        # Strategy 1: Try primary model name with pretrained=True\n",
    "        try:\n",
    "            print(f\"         ğŸ¯ Trying pretrained: {model_name}\")\n",
    "            model = timm.create_model(model_name, pretrained=True, num_classes=self.num_classes)\n",
    "            initialization_type = \"pretrained\"\n",
    "            final_model_name = model_name\n",
    "            print(f\"         âœ… SUCCESS with pretrained weights!\")\n",
    "        except Exception as e1:\n",
    "            print(f\"         âŒ Pretrained failed: {str(e1)[:50]}...\")\n",
    "            \n",
    "            # Strategy 2: Try alternative names with pretrained=True\n",
    "            model = None\n",
    "            for alt_name in alternatives:\n",
    "                try:\n",
    "                    print(f\"         ğŸ¯ Trying alternative pretrained: {alt_name}\")\n",
    "                    model = timm.create_model(alt_name, pretrained=True, num_classes=self.num_classes)\n",
    "                    initialization_type = \"pretrained_alt\"\n",
    "                    final_model_name = alt_name\n",
    "                    print(f\"         âœ… SUCCESS with alternative pretrained weights!\")\n",
    "                    break\n",
    "                except Exception as e2:\n",
    "                    print(f\"         âŒ Alternative {alt_name} failed: {str(e2)[:30]}...\")\n",
    "                    continue\n",
    "            \n",
    "            # Strategy 3: Fall back to random initialization\n",
    "            if model is None:\n",
    "                try:\n",
    "                    print(f\"         ğŸ² Falling back to random initialization: {model_name}\")\n",
    "                    model = timm.create_model(model_name, pretrained=False, num_classes=self.num_classes)\n",
    "                    initialization_type = \"random\"\n",
    "                    final_model_name = model_name\n",
    "                    print(f\"         âœ… SUCCESS with random initialization!\")\n",
    "                except Exception as e3:\n",
    "                    # Try alternatives with random initialization\n",
    "                    for alt_name in alternatives:\n",
    "                        try:\n",
    "                            print(f\"         ğŸ² Trying alternative random: {alt_name}\")\n",
    "                            model = timm.create_model(alt_name, pretrained=False, num_classes=self.num_classes)\n",
    "                            initialization_type = \"random_alt\"\n",
    "                            final_model_name = alt_name\n",
    "                            print(f\"         âœ… SUCCESS with alternative random initialization!\")\n",
    "                            break\n",
    "                        except Exception as e4:\n",
    "                            continue\n",
    "                    \n",
    "                    # If still failed, return None\n",
    "                    if model is None:\n",
    "                        print(f\"         âŒ COMPLETE FAILURE: All strategies failed\")\n",
    "                        return None, None, None\n",
    "        \n",
    "        # Apply freezing strategy\n",
    "        if freeze_strategy == 'backbone':\n",
    "            print(f\"         ğŸ§Š Freezing backbone layers...\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'classifier' not in name and 'head' not in name and 'fc' not in name:\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"         ğŸ“Š Loaded: {total_params:,} total, {trainable_params:,} trainable ({initialization_type})\")\n",
    "        \n",
    "        elif freeze_strategy == 'partial':\n",
    "            print(f\"         â„ï¸ Partial freeze (last 30% unfrozen)...\")\n",
    "            all_params = list(model.named_parameters())\n",
    "            total_layers = len(all_params)\n",
    "            freeze_until = int(total_layers * 0.7)\n",
    "            \n",
    "            for i, (name, param) in enumerate(all_params):\n",
    "                if i < freeze_until and 'classifier' not in name and 'head' not in name and 'fc' not in name:\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"         ğŸ“Š Loaded: {total_params:,} total, {trainable_params:,} trainable ({initialization_type})\")\n",
    "        \n",
    "        else:  # no freezing\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"         ğŸ“Š Loaded: {total_params:,} parameters (all trainable, {initialization_type})\")\n",
    "        \n",
    "        model = model.to(self.device)\n",
    "        return model, initialization_type, final_model_name\n",
    "    \n",
    "    def ultra_aggressive_training(self, model, arch_name, train_loader, val_loader, test_loader, strategy='unfrozen'):\n",
    "        \"\"\"Ultra aggressive training with minimal early stopping\"\"\"\n",
    "        print(f\"      ğŸ”¥ ULTRA AGGRESSIVE TRAINING: {arch_name} ({strategy})...\")\n",
    "        \n",
    "        # More aggressive setup\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Strategy-specific hyperparameters\n",
    "        if strategy == 'frozen':\n",
    "            lr = 0.01\n",
    "            max_epochs = 100\n",
    "            patience = 50\n",
    "        elif strategy == 'partial':\n",
    "            lr = 0.005\n",
    "            max_epochs = 100\n",
    "            patience = 50\n",
    "        else:  # unfrozen\n",
    "            lr = 0.001\n",
    "            max_epochs = 100\n",
    "            patience = 50\n",
    "        \n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=0.01,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        # Simple step scheduler\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"         ğŸ“Š ULTRA SETUP: {max_epochs} epochs, LR={lr}, patience={patience}\")\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            # Very lenient early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                improvement = \"ğŸ”¥\"\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                improvement = \"\"\n",
    "            \n",
    "            # More frequent progress updates\n",
    "            if epoch % 5 == 0 or epoch < 10 or improvement or epoch > max_epochs - 10:\n",
    "                gap = train_acc - val_acc\n",
    "                print(f\"         Epoch {epoch:3d}: Train {train_acc:.1f}%, Val {val_acc:.1f}% (gap: {gap:+.1f}%), LR: {current_lr:.2e} {improvement}\")\n",
    "        \n",
    "        # Restore best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Test evaluation\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        print(f\"         ğŸ¯ {arch_name} ({strategy}) FINAL: Val {best_val_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "        \n",
    "        return best_val_acc, test_acc\n",
    "    \n",
    "    def test_architecture_with_multiple_strategies(self, arch_name, arch_info, train_loader, val_loader, test_loader):\n",
    "        \"\"\"Test architecture with multiple training strategies and fallback support\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Strategy 1: Frozen backbone (fast warmup)\n",
    "        print(f\"      ğŸ§Š FROZEN BACKBONE STRATEGY:\")\n",
    "        model_frozen, init_type_frozen, final_name_frozen = self.create_model_with_fallback(arch_name, arch_info, freeze_strategy='backbone')\n",
    "        if model_frozen is not None:\n",
    "            try:\n",
    "                val_acc_frozen, test_acc_frozen = self.ultra_aggressive_training(\n",
    "                    model_frozen, arch_name, train_loader, val_loader, test_loader, strategy='frozen'\n",
    "                )\n",
    "                results.append({\n",
    "                    'name': f\"{arch_name}-Frozen\",\n",
    "                    'strategy': 'frozen',\n",
    "                    'val_accuracy': val_acc_frozen,\n",
    "                    'test_accuracy': test_acc_frozen,\n",
    "                    'family': arch_info['family'],\n",
    "                    'initialization': init_type_frozen,\n",
    "                    'final_model_name': final_name_frozen,\n",
    "                    'original_model_name': arch_info['model_name']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"         âŒ Frozen strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        # Strategy 2: Partial freeze (if frozen worked reasonably)\n",
    "        if results and results[-1]['val_accuracy'] > 35:\n",
    "            print(f\"      â„ï¸ PARTIAL FREEZE STRATEGY:\")\n",
    "            model_partial, init_type_partial, final_name_partial = self.create_model_with_fallback(arch_name, arch_info, freeze_strategy='partial')\n",
    "            if model_partial is not None:\n",
    "                try:\n",
    "                    val_acc_partial, test_acc_partial = self.ultra_aggressive_training(\n",
    "                        model_partial, arch_name, train_loader, val_loader, test_loader, strategy='partial'\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'name': f\"{arch_name}-Partial\",\n",
    "                        'strategy': 'partial',\n",
    "                        'val_accuracy': val_acc_partial,\n",
    "                        'test_accuracy': test_acc_partial,\n",
    "                        'family': arch_info['family'],\n",
    "                        'initialization': init_type_partial,\n",
    "                        'final_model_name': final_name_partial,\n",
    "                        'original_model_name': arch_info['model_name']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"         âŒ Partial strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        # Strategy 3: Full unfrozen (if partial worked well)\n",
    "        if results and max(r['val_accuracy'] for r in results) > 45:\n",
    "            print(f\"      ğŸ”¥ FULL UNFROZEN STRATEGY:\")\n",
    "            model_unfrozen, init_type_unfrozen, final_name_unfrozen = self.create_model_with_fallback(arch_name, arch_info, freeze_strategy='none')\n",
    "            if model_unfrozen is not None:\n",
    "                try:\n",
    "                    val_acc_unfrozen, test_acc_unfrozen = self.ultra_aggressive_training(\n",
    "                        model_unfrozen, arch_name, train_loader, val_loader, test_loader, strategy='unfrozen'\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'name': f\"{arch_name}-Unfrozen\",\n",
    "                        'strategy': 'unfrozen',\n",
    "                        'val_accuracy': val_acc_unfrozen,\n",
    "                        'test_accuracy': test_acc_unfrozen,\n",
    "                        'family': arch_info['family'],\n",
    "                        'initialization': init_type_unfrozen,\n",
    "                        'final_model_name': final_name_unfrozen,\n",
    "                        'original_model_name': arch_info['model_name']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"         âŒ Unfrozen strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_complete_pipeline(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        \"\"\"Run the complete pipeline with result storage\"\"\"\n",
    "        print(\"ğŸ”¥ COMPLETE DEER AGING PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"ğŸ¯ Starting from EfficientNet-B5 onwards\")\n",
    "        print(\"ğŸ¯ All results will be saved automatically\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = DeerDataset(X_train, y_train)\n",
    "        val_dataset = DeerDataset(X_val, y_val)\n",
    "        test_dataset = DeerDataset(X_test, y_test)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "        \n",
    "        print(f\"ğŸ“Š Data ready: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n",
    "        \n",
    "        # Get all architectures\n",
    "        architectures = self.get_all_architectures_with_fallback()\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\nğŸ”¥ ULTRA AGGRESSIVE TESTING: {len(architectures)} ARCHITECTURES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, (arch_name, arch_info) in enumerate(architectures.items(), 1):\n",
    "            print(f\"\\n[{i}/{len(architectures)}] ğŸ”¥ ULTRA AGGRESSIVE {arch_name}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Test with multiple strategies\n",
    "            arch_results = self.test_architecture_with_multiple_strategies(\n",
    "                arch_name, arch_info, train_loader, val_loader, test_loader\n",
    "            )\n",
    "            \n",
    "            # Add metadata and timing\n",
    "            for result in arch_results:\n",
    "                result['architecture_family'] = arch_info['family']\n",
    "                result['training_time'] = time.time() - start_time\n",
    "                result['timestamp'] = datetime.now().isoformat()\n",
    "                self.all_results.append(result)\n",
    "            \n",
    "            if arch_results:\n",
    "                best_arch_result = max(arch_results, key=lambda x: x['test_accuracy'])\n",
    "                print(f\"      ğŸ† Best {arch_name}: {best_arch_result['name']} ({best_arch_result['test_accuracy']:.1f}%)\")\n",
    "            \n",
    "            print(f\"      â±ï¸ Total time for {arch_name}: {time.time() - start_time:.1f}s\")\n",
    "            \n",
    "            # Save results after each architecture (backup)\n",
    "            self.save_results_backup(i)\n",
    "            \n",
    "            # Intermediate leaderboard every 3 architectures\n",
    "            if i % 3 == 0:\n",
    "                self.show_intermediate_leaderboard(i)\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        \n",
    "        # Save final results\n",
    "        self.save_final_results(total_time)\n",
    "        \n",
    "        # Display final leaderboard\n",
    "        self.show_final_leaderboard(total_time)\n",
    "        \n",
    "        return self.all_results\n",
    "    \n",
    "    def save_results_backup(self, completed_count):\n",
    "        \"\"\"Save backup results after each architecture\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open(f'deer_aging_results_backup_{completed_count}_{timestamp}.json', 'w') as f:\n",
    "            json.dump(self.all_results, f, indent=2)\n",
    "        \n",
    "        # Save as pickle for full Python objects\n",
    "        with open(f'deer_aging_results_backup_{completed_count}_{timestamp}.pkl', 'wb') as f:\n",
    "            pickle.dump(self.all_results, f)\n",
    "    \n",
    "    def save_final_results(self, total_time):\n",
    "        \"\"\"Save comprehensive final results\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Prepare comprehensive results\n",
    "        final_data = {\n",
    "            'experiment_info': {\n",
    "                'timestamp': timestamp,\n",
    "                'total_runtime_hours': total_time / 3600,\n",
    "                'total_models_tested': len(self.all_results),\n",
    "                'device': str(self.device),\n",
    "                'num_classes': self.num_classes\n",
    "            },\n",
    "            'results': self.all_results,\n",
    "            'leaderboard': sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        }\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open(f'deer_aging_final_results_{timestamp}.json', 'w') as f:\n",
    "            json.dump(final_data, f, indent=2)\n",
    "        \n",
    "        # Save as pickle\n",
    "        with open(f'deer_aging_final_results_{timestamp}.pkl', 'wb') as f:\n",
    "            pickle.dump(final_data, f)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ RESULTS SAVED:\")\n",
    "        print(f\"   ğŸ“ deer_aging_final_results_{timestamp}.json\")\n",
    "        print(f\"   ğŸ“ deer_aging_final_results_{timestamp}.pkl\")\n",
    "    \n",
    "    def show_intermediate_leaderboard(self, completed_count):\n",
    "        \"\"\"Show intermediate leaderboard\"\"\"\n",
    "        current_best = sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)[:5]\n",
    "        print(f\"\\nğŸ“Š CURRENT TOP 5 (after {completed_count} architectures):\")\n",
    "        for j, result in enumerate(current_best, 1):\n",
    "            print(f\"   {j}. {result['name']}: {result['test_accuracy']:.1f}%\")\n",
    "        print()\n",
    "    \n",
    "    def show_final_leaderboard(self, total_time):\n",
    "        \"\"\"Show comprehensive final leaderboard\"\"\"\n",
    "        # Sort all results\n",
    "        sorted_results = sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        \n",
    "        print(f\"\\nğŸ† FINAL COMPREHENSIVE RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"â° Total testing time: {total_time/3600:.1f} hours\")\n",
    "        print(f\"ğŸ¯ Models tested: {len(self.all_results)}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Rank':<4} {'Model':<30} {'Strategy':<10} {'Init':<12} {'Val%':<8} {'Test%':<8} {'Status'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(sorted_results, 1):\n",
    "            val_acc = result['val_accuracy']\n",
    "            test_acc = result['test_accuracy']\n",
    "            strategy = result['strategy']\n",
    "            init_type = result.get('initialization', 'unknown')\n",
    "            \n",
    "            # Format initialization type for display\n",
    "            if init_type == 'pretrained':\n",
    "                init_display = \"ğŸ¯ Pretrained\"\n",
    "            elif init_type == 'pretrained_alt':\n",
    "                init_display = \"ğŸ¯ Alt-Pre\"\n",
    "            elif init_type == 'random':\n",
    "                init_display = \"ğŸ² Random\"\n",
    "            elif init_type == 'random_alt':\n",
    "                init_display = \"ğŸ² Alt-Rand\"\n",
    "            else:\n",
    "                init_display = \"â“ Unknown\"\n",
    "            \n",
    "            if test_acc >= 75.0:\n",
    "                status = \"ğŸ‰ BREAKTHROUGH!\"\n",
    "            elif test_acc >= 65.0:\n",
    "                status = \"ğŸ”¥ EXCELLENT!\"\n",
    "            elif test_acc > 54.2:\n",
    "                status = \"ğŸš€ NEW BEST!\"\n",
    "            elif test_acc > 45.0:\n",
    "                status = \"ğŸ“ˆ Good\"\n",
    "            else:\n",
    "                status = \"ğŸ“‰ Weak\"\n",
    "            \n",
    "            print(f\"{i:<4} {result['name']:<30} {strategy:<10} {init_display:<12} {val_acc:<7.1f} {test_acc:<7.1f} {status}\")\n",
    "        \n",
    "        # Additional analysis by initialization type\n",
    "        print(f\"\\nğŸ“Š ANALYSIS BY INITIALIZATION:\")\n",
    "        init_groups = {}\n",
    "        for result in sorted_results:\n",
    "            init_type = result.get('initialization', 'unknown')\n",
    "            if init_type not in init_groups:\n",
    "                init_groups[init_type] = []\n",
    "            init_groups[init_type].append(result)\n",
    "        \n",
    "        for init_type, group in init_groups.items():\n",
    "            avg_test = sum(r['test_accuracy'] for r in group) / len(group)\n",
    "            best_test = max(r['test_accuracy'] for r in group)\n",
    "            print(f\"   {init_type:15}: {len(group):2d} models, avg: {avg_test:.1f}%, best: {best_test:.1f}%\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        if sorted_results:\n",
    "            best = sorted_results[0]\n",
    "            breakthrough_count = sum(1 for r in sorted_results if r['test_accuracy'] > 54.2)\n",
    "            excellent_count = sum(1 for r in sorted_results if r['test_accuracy'] >= 65.0)\n",
    "            \n",
    "            print(f\"\\nğŸŠ FINAL SUMMARY:\")\n",
    "            print(f\"   ğŸ† ULTIMATE CHAMPION: {best['name']} ({best['test_accuracy']:.1f}%)\")\n",
    "            print(f\"   ğŸš€ Beat 54.2% baseline: {breakthrough_count}/{len(sorted_results)} models\")\n",
    "            print(f\"   ğŸ‰ Achieved 65%+: {excellent_count} models\")\n",
    "            \n",
    "            if best['test_accuracy'] >= 75.0:\n",
    "                print(f\"   ğŸ‰ MISSION ACCOMPLISHED! Achieved 75%+ accuracy!\")\n",
    "            elif best['test_accuracy'] >= 65.0:\n",
    "                print(f\"   ğŸŠ EXCELLENT! Found 65%+ architecture!\")\n",
    "            elif best['test_accuracy'] > 54.2:\n",
    "                improvement = best['test_accuracy'] - 54.2\n",
    "                print(f\"   ğŸš€ SUCCESS! Improved by +{improvement:.1f}% over baseline!\")\n",
    "            \n",
    "            # Analysis of initialization types\n",
    "            best_pretrained = max([r for r in sorted_results if r.get('initialization', '').startswith('pretrained')], \n",
    "                                key=lambda x: x['test_accuracy'], default=None)\n",
    "            best_random = max([r for r in sorted_results if r.get('initialization', '').startswith('random')], \n",
    "                            key=lambda x: x['test_accuracy'], default=None)\n",
    "            \n",
    "            if best_pretrained and best_random:\n",
    "                print(f\"   ğŸ¯ Best Pretrained: {best_pretrained['name']} ({best_pretrained['test_accuracy']:.1f}%)\")\n",
    "                print(f\"   ğŸ² Best Random Init: {best_random['name']} ({best_random['test_accuracy']:.1f}%)\")\n",
    "                if best_random['test_accuracy'] > best_pretrained['test_accuracy']:\n",
    "                    print(f\"   ğŸ”¥ SURPRISE! Random initialization outperformed pretrained!\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "\n",
    "def run_complete_deer_aging_pipeline():\n",
    "    \"\"\"Run the complete deer aging pipeline from start to finish\"\"\"\n",
    "    print(\"ğŸš€ LAUNCHING COMPLETE DEER AGING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ“‹ PIPELINE STEPS:\")\n",
    "    print(\"   1. Load original 357 images\")\n",
    "    print(\"   2. Create train/val/test splits\")\n",
    "    print(\"   3. Balance and augment training data\")\n",
    "    print(\"   4. Test all architectures (starting from EfficientNet-B5)\")\n",
    "    print(\"   5. Save results and create leaderboard\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    images, ages = load_original_data()\n",
    "    \n",
    "    # Step 2: Create splits\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, label_mapping, reverse_mapping = create_train_val_test_split(images, ages)\n",
    "    \n",
    "    # Step 3: Augment data\n",
    "    X_train_aug, y_train_aug = balance_and_augment_data(X_train, y_train, augment_multiplier=30, num_classes=len(label_mapping))\n",
    "    \n",
    "    # Step 4: Run complete testing\n",
    "    trainer = CompleteDeerAgeTrainer(num_classes=len(label_mapping))\n",
    "    results = trainer.run_complete_pipeline(X_train_aug, y_train_aug, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    print(\"\\nğŸ‰ PIPELINE COMPLETE!\")\n",
    "    print(\"ğŸ“ All results saved with timestamps\")\n",
    "    print(\"ğŸ† Check the final leaderboard above\")\n",
    "    \n",
    "    return results, label_mapping, reverse_mapping\n",
    "\n",
    "# ğŸ”¥ RUN COMPLETE PIPELINE\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ”¥ LAUNCHING COMPLETE DEER AGING PIPELINE...\")\n",
    "    print(\"âš ï¸  Starting from EfficientNet-B5 (as requested)\")\n",
    "    print(\"ğŸ¯ TESTING ALL MODELS: Pretrained â†’ Alternatives â†’ Random Init\")\n",
    "    print(\"ğŸ’¾ All results will be automatically saved\")\n",
    "    print(\"ğŸ“Š Will show which models used pretrained vs random initialization\")\n",
    "    \n",
    "    final_results, final_label_mapping, final_reverse_mapping = run_complete_deer_aging_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a316f97-c111-4262-8bef-f33af9d68e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
