{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2dad4-961a-479e-9619-1a78b5561d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_22108\\52124910.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "G:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_original_data():\n",
    "    \"\"\"Load the original 357 images\"\"\"\n",
    "    print(\"üìÇ LOADING ORIGINAL DATA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load using the user's method\n",
    "    from buck.analysis.basics import ingest_images\n",
    "    \n",
    "    fpath = \"C:\\\\Users\\\\aaron\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*.png\"\n",
    "    images, ages = ingest_images(fpath)\n",
    "    print(f\"   ‚úÖ Loaded {len(images)} original images\")\n",
    "    \n",
    "    # Group ages: 5.5+ all become 5.5 (creating exactly 5 classes)\n",
    "    print(\"   üîÑ Grouping ages: 5.5+ ‚Üí 5.5\")\n",
    "    ages_grouped = []\n",
    "    for age in ages:\n",
    "        if age >= 5.5:\n",
    "            ages_grouped.append(5.5)\n",
    "        else:\n",
    "            ages_grouped.append(age)\n",
    "    \n",
    "    # Print age distribution before and after grouping\n",
    "    print(f\"   üìä Original age distribution: {dict(Counter(ages))}\")\n",
    "    print(f\"   üìä Grouped age distribution: {dict(Counter(ages_grouped))}\")\n",
    "    \n",
    "    return images, ages_grouped\n",
    "\n",
    "def create_train_val_test_split(images, ages, test_size=0.2, val_size=0.15, random_state=42):\n",
    "    \"\"\"Create train/validation/test split\"\"\"\n",
    "    print(\"\\nüîÄ CREATING TRAIN/VAL/TEST SPLIT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    if not isinstance(images, np.ndarray):\n",
    "        images = np.array(images)\n",
    "    if not isinstance(ages, np.ndarray):\n",
    "        ages = np.array(ages)\n",
    "    \n",
    "    # Check if stratified split is possible\n",
    "    age_counts = Counter(ages)\n",
    "    min_count = min(age_counts.values())\n",
    "    can_stratify = min_count >= 2\n",
    "    \n",
    "    print(f\"   üìä Age distribution: {dict(age_counts)}\")\n",
    "    print(f\"   üìä Minimum class size: {min_count}\")\n",
    "    print(f\"   üéØ Can use stratified split: {can_stratify}\")\n",
    "    \n",
    "    if can_stratify:\n",
    "        # First split: separate test set (stratified)\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            images, ages, test_size=test_size, random_state=random_state, stratify=ages\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation from remaining data\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # Adjust for remaining data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, stratify=y_temp\n",
    "        )\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Using random split (some classes too small for stratification)\")\n",
    "        # First split: separate test set (random)\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            images, ages, test_size=test_size, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation from remaining data\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # Adjust for remaining data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, shuffle=True\n",
    "        )\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    reverse_mapping = {i: age for age, i in label_mapping.items()}\n",
    "    \n",
    "    print(f\"   üìä Train: {len(X_train)} samples\")\n",
    "    print(f\"   üìä Val: {len(X_val)} samples\") \n",
    "    print(f\"   üìä Test: {len(X_test)} samples\")\n",
    "    print(f\"   üè∑Ô∏è Label mapping: {label_mapping}\")\n",
    "    print(f\"   üéØ Number of classes: {len(unique_ages)}\")\n",
    "    \n",
    "    # Convert ages to class indices\n",
    "    y_train_indices = np.array([label_mapping[age] for age in y_train])\n",
    "    y_val_indices = np.array([label_mapping[age] for age in y_val])\n",
    "    y_test_indices = np.array([label_mapping[age] for age in y_test])\n",
    "    \n",
    "    print(f\"   üìà Train distribution: {Counter(y_train_indices)}\")\n",
    "    print(f\"   üìà Val distribution: {Counter(y_val_indices)}\")\n",
    "    print(f\"   üìà Test distribution: {Counter(y_test_indices)}\")\n",
    "    \n",
    "    return (X_train, y_train_indices, X_val, y_val_indices, X_test, y_test_indices, \n",
    "            label_mapping, reverse_mapping)\n",
    "\n",
    "def augment_image(image):\n",
    "    \"\"\"Apply random augmentation to an image\"\"\"\n",
    "    # Ensure image is uint8\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Random rotation\n",
    "    if random.random() < 0.5:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Random horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Random brightness/contrast\n",
    "    if random.random() < 0.5:\n",
    "        alpha = random.uniform(0.8, 1.2)  # Contrast\n",
    "        beta = random.randint(-20, 20)    # Brightness\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Random noise (fixed data type issue)\n",
    "    if random.random() < 0.3:\n",
    "        # Create noise with same dtype as image\n",
    "        noise = np.random.normal(0, 5, image.shape).astype(np.int16)  # Use int16 to handle negative values\n",
    "        # Convert image to int16 for safe addition\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        # Add noise and clip to valid range\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        # Convert back to uint8\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def balance_and_augment_data(X_train, y_train, augment_multiplier=30, num_classes=5):\n",
    "    \"\"\"Balance classes and augment training data\"\"\"\n",
    "    print(f\"\\nüîÑ BALANCING AND AUGMENTING DATA\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"   üéØ Target: {augment_multiplier}x augmentation per class\")\n",
    "    \n",
    "    # Count samples per class\n",
    "    class_counts = Counter(y_train)\n",
    "    print(f\"   üìä Original distribution: {dict(class_counts)}\")\n",
    "    \n",
    "    # Find target count (based on largest class * multiplier)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max_count * augment_multiplier\n",
    "    print(f\"   üéØ Target samples per class: {target_count}\")\n",
    "    \n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Get samples for this class\n",
    "        class_mask = y_train == class_idx\n",
    "        class_images = X_train[class_mask]\n",
    "        class_labels = y_train[class_mask]\n",
    "        \n",
    "        current_count = len(class_images)\n",
    "        needed_count = target_count\n",
    "        \n",
    "        print(f\"   üìà Class {class_idx}: {current_count} ‚Üí {needed_count} samples\")\n",
    "        \n",
    "        # Add original samples\n",
    "        X_augmented.extend(class_images)\n",
    "        y_augmented.extend(class_labels)\n",
    "        \n",
    "        # Generate augmented samples\n",
    "        augmented_needed = needed_count - current_count\n",
    "        \n",
    "        for i in range(augmented_needed):\n",
    "            # Pick random original image from this class\n",
    "            original_idx = random.randint(0, current_count - 1)\n",
    "            original_image = class_images[original_idx].copy()\n",
    "            \n",
    "            # Augment it\n",
    "            augmented_image = augment_image(original_image)\n",
    "            \n",
    "            X_augmented.append(augmented_image)\n",
    "            y_augmented.append(class_idx)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    X_augmented = np.array(X_augmented)\n",
    "    y_augmented = np.array(y_augmented)\n",
    "    \n",
    "    print(f\"   ‚úÖ Augmentation complete: {len(X_augmented)} total samples\")\n",
    "    print(f\"   üìä Final distribution: {Counter(y_augmented)}\")\n",
    "    \n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    \"\"\"Dataset for deer aging with preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, transform=True):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.FloatTensor(X)\n",
    "        else:\n",
    "            self.X = torch.FloatTensor(np.array(X))\n",
    "            \n",
    "        if isinstance(y, np.ndarray):\n",
    "            self.y = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.y = torch.LongTensor(np.array(y))\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Normalize to [0,1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Ensure CHW format (channels first)\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Resize to 224x224\n",
    "        if image.shape[-2:] != (224, 224):\n",
    "            image = image.unsqueeze(0)\n",
    "            image = F.interpolate(image, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            image = image.squeeze(0)\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        image = (image - mean) / std\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class UltraAggressiveTrainer:\n",
    "    \"\"\"Ultra aggressive training with maximum epochs and minimal early stopping\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"üî• ULTRA AGGRESSIVE TRAINER\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Classes: {num_classes}\")\n",
    "    \n",
    "    def get_all_architectures(self):\n",
    "        \"\"\"Get comprehensive list of ALL architectures\"\"\"\n",
    "        \n",
    "        architectures = {\n",
    "            # EfficientNet B0-B7 (COMPLETE SERIES)\n",
    "            'EfficientNet-B0': {'model_name': 'efficientnet_b0', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-B1': {'model_name': 'efficientnet_b1', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-B2': {'model_name': 'efficientnet_b2', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-B3': {'model_name': 'efficientnet_b3', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-B4': {'model_name': 'efficientnet_b4', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-B5': {'model_name': 'efficientnet_b5', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-B6': {'model_name': 'efficientnet_b6', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-B7': {'model_name': 'efficientnet_b7', 'family': 'EfficientNet'},\n",
    "            \n",
    "            # EfficientNetV2 \n",
    "            'EfficientNetV2-S': {'model_name': 'efficientnetv2_s', 'family': 'EfficientNetV2'},\n",
    "            'EfficientNetV2-M': {'model_name': 'efficientnetv2_m', 'family': 'EfficientNetV2'},\n",
    "            'EfficientNetV2-L': {'model_name': 'efficientnetv2_l', 'family': 'EfficientNetV2'},\n",
    "            \n",
    "            # DenseNet Family\n",
    "            'DenseNet-121': {'model_name': 'densenet121', 'family': 'DenseNet'},\n",
    "            'DenseNet-161': {'model_name': 'densenet161', 'family': 'DenseNet'},\n",
    "            'DenseNet-169': {'model_name': 'densenet169', 'family': 'DenseNet'},\n",
    "            'DenseNet-201': {'model_name': 'densenet201', 'family': 'DenseNet'},\n",
    "            \n",
    "            # ResNet Family\n",
    "            'ResNet-34': {'model_name': 'resnet34', 'family': 'ResNet'},\n",
    "            'ResNet-50': {'model_name': 'resnet50', 'family': 'ResNet'},\n",
    "            'ResNet-101': {'model_name': 'resnet101', 'family': 'ResNet'},\n",
    "            'ResNet-152': {'model_name': 'resnet152', 'family': 'ResNet'},\n",
    "            \n",
    "            # ResNeXt\n",
    "            'ResNeXt-50': {'model_name': 'resnext50_32x4d', 'family': 'ResNeXt'},\n",
    "            'ResNeXt-101': {'model_name': 'resnext101_32x8d', 'family': 'ResNeXt'},\n",
    "            \n",
    "            # Wide ResNet\n",
    "            'Wide-ResNet-50': {'model_name': 'wide_resnet50_2', 'family': 'Wide-ResNet'},\n",
    "            'Wide-ResNet-101': {'model_name': 'wide_resnet101_2', 'family': 'Wide-ResNet'},\n",
    "            \n",
    "            # MobileNet Family\n",
    "            'MobileNetV2': {'model_name': 'mobilenetv2_100', 'family': 'MobileNet'},\n",
    "            'MobileNetV3-Small': {'model_name': 'mobilenetv3_small_100', 'family': 'MobileNet'},\n",
    "            'MobileNetV3-Large': {'model_name': 'mobilenetv3_large_100', 'family': 'MobileNet'},\n",
    "            \n",
    "            # RegNet Family\n",
    "            'RegNetX-400MF': {'model_name': 'regnetx_400mf', 'family': 'RegNet'},\n",
    "            'RegNetX-800MF': {'model_name': 'regnetx_800mf', 'family': 'RegNet'},\n",
    "            'RegNetY-400MF': {'model_name': 'regnety_400mf', 'family': 'RegNet'},\n",
    "            'RegNetY-800MF': {'model_name': 'regnety_800mf', 'family': 'RegNet'},\n",
    "            \n",
    "            # ConvNeXt\n",
    "            'ConvNeXt-Tiny': {'model_name': 'convnext_tiny', 'family': 'ConvNeXt'},\n",
    "            'ConvNeXt-Small': {'model_name': 'convnext_small', 'family': 'ConvNeXt'},\n",
    "            'ConvNeXt-Base': {'model_name': 'convnext_base', 'family': 'ConvNeXt'},\n",
    "            \n",
    "            # Vision Transformer variants\n",
    "            'Swin-Tiny': {'model_name': 'swin_tiny_patch4_window7_224', 'family': 'Swin'},\n",
    "            'Swin-Small': {'model_name': 'swin_small_patch4_window7_224', 'family': 'Swin'},\n",
    "            \n",
    "            # VGG (classic)\n",
    "            'VGG-16': {'model_name': 'vgg16', 'family': 'VGG'},\n",
    "            'VGG-19': {'model_name': 'vgg19', 'family': 'VGG'},\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüèóÔ∏è ULTRA AGGRESSIVE ARSENAL ({len(architectures)} models)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Group by family and show counts\n",
    "        families = {}\n",
    "        for arch_name, arch_info in architectures.items():\n",
    "            family = arch_info['family']\n",
    "            if family not in families:\n",
    "                families[family] = []\n",
    "            families[family].append(arch_name)\n",
    "        \n",
    "        for family, models in families.items():\n",
    "            print(f\"üìÅ {family} ({len(models)} models): {', '.join(models)}\")\n",
    "        \n",
    "        return architectures\n",
    "    \n",
    "    def create_model(self, arch_name, model_name, freeze_strategy='none'):\n",
    "        \"\"\"Create model with different freezing strategies\"\"\"\n",
    "        try:\n",
    "            print(f\"      üîß Creating {arch_name}...\")\n",
    "            model = timm.create_model(model_name, pretrained=True, num_classes=self.num_classes)\n",
    "            \n",
    "            if freeze_strategy == 'backbone':\n",
    "                print(f\"         üßä Freezing backbone layers...\")\n",
    "                for name, param in model.named_parameters():\n",
    "                    if 'classifier' not in name and 'head' not in name and 'fc' not in name:\n",
    "                        param.requires_grad = False\n",
    "                \n",
    "                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"         ‚úÖ Loaded: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "            \n",
    "            elif freeze_strategy == 'partial':\n",
    "                print(f\"         ‚ùÑÔ∏è Partial freeze (last 2 blocks unfrozen)...\")\n",
    "                # More sophisticated partial freezing\n",
    "                all_params = list(model.named_parameters())\n",
    "                total_layers = len(all_params)\n",
    "                freeze_until = int(total_layers * 0.7)  # Freeze first 70% of layers\n",
    "                \n",
    "                for i, (name, param) in enumerate(all_params):\n",
    "                    if i < freeze_until and 'classifier' not in name and 'head' not in name and 'fc' not in name:\n",
    "                        param.requires_grad = False\n",
    "                \n",
    "                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"         ‚úÖ Loaded: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "            \n",
    "            else:  # no freezing\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"         ‚úÖ Loaded: {total_params:,} parameters (all trainable)\")\n",
    "            \n",
    "            model = model.to(self.device)\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"         ‚ùå Failed: {str(e)[:60]}...\")\n",
    "            return None\n",
    "    \n",
    "    def ultra_aggressive_training(self, model, arch_name, train_loader, val_loader, test_loader, strategy='unfrozen'):\n",
    "        \"\"\"Ultra aggressive training with minimal early stopping\"\"\"\n",
    "        print(f\"      üî• ULTRA AGGRESSIVE TRAINING: {arch_name} ({strategy})...\")\n",
    "        \n",
    "        # More aggressive setup\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Strategy-specific hyperparameters\n",
    "        if strategy == 'frozen':\n",
    "            lr = 0.01\n",
    "            max_epochs = 250  # More epochs for frozen\n",
    "            patience = 100    # Much higher patience\n",
    "        elif strategy == 'partial':\n",
    "            lr = 0.005\n",
    "            max_epochs = 250  # Even more epochs for partial\n",
    "            patience = 100\n",
    "        else:  # unfrozen\n",
    "            lr = 0.001\n",
    "            max_epochs = 250  # Maximum epochs for full training\n",
    "            patience = 100     # Very high patience\n",
    "        \n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=0.01,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        # Simple step scheduler (more stable than cosine annealing)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"         üìä ULTRA SETUP: {max_epochs} epochs, LR={lr}, patience={patience}\")\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            # Very lenient early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                improvement = \"üî•\"\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                improvement = \"\"\n",
    "            \n",
    "            # More frequent progress updates\n",
    "            if epoch % 5 == 0 or epoch < 10 or improvement or epoch > max_epochs - 10:\n",
    "                gap = train_acc - val_acc\n",
    "                print(f\"         Epoch {epoch:3d}: Train {train_acc:.1f}%, Val {val_acc:.1f}% (gap: {gap:+.1f}%), LR: {current_lr:.2e} {improvement}\")\n",
    "            \n",
    "            ## Much more lenient early stopping\n",
    "            #if patience_counter >= patience:\n",
    "            #    print(f\"         Early stopping at epoch {epoch} (patience={patience})\")\n",
    "            #    break\n",
    "        \n",
    "        # Restore best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Test evaluation\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        print(f\"         üéØ {arch_name} ({strategy}) FINAL: Val {best_val_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "        \n",
    "        return best_val_acc, test_acc\n",
    "    \n",
    "    def test_architecture_with_multiple_strategies(self, arch_name, model_name, train_loader, val_loader, test_loader):\n",
    "        \"\"\"Test architecture with multiple training strategies\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Strategy 1: Frozen backbone (fast warmup)\n",
    "        print(f\"      üßä FROZEN BACKBONE STRATEGY:\")\n",
    "        model_frozen = self.create_model(arch_name, model_name, freeze_strategy='backbone')\n",
    "        if model_frozen is not None:\n",
    "            try:\n",
    "                val_acc_frozen, test_acc_frozen = self.ultra_aggressive_training(\n",
    "                    model_frozen, arch_name, train_loader, val_loader, test_loader, strategy='frozen'\n",
    "                )\n",
    "                results.append({\n",
    "                    'name': f\"{arch_name}-Frozen\",\n",
    "                    'strategy': 'frozen',\n",
    "                    'val_accuracy': val_acc_frozen,\n",
    "                    'test_accuracy': test_acc_frozen\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"         ‚ùå Frozen strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        # Strategy 2: Partial freeze (if frozen worked reasonably)\n",
    "        if results and results[-1]['val_accuracy'] > 35:\n",
    "            print(f\"      ‚ùÑÔ∏è PARTIAL FREEZE STRATEGY:\")\n",
    "            model_partial = self.create_model(arch_name, model_name, freeze_strategy='partial')\n",
    "            if model_partial is not None:\n",
    "                try:\n",
    "                    val_acc_partial, test_acc_partial = self.ultra_aggressive_training(\n",
    "                        model_partial, arch_name, train_loader, val_loader, test_loader, strategy='partial'\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'name': f\"{arch_name}-Partial\",\n",
    "                        'strategy': 'partial',\n",
    "                        'val_accuracy': val_acc_partial,\n",
    "                        'test_accuracy': test_acc_partial\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"         ‚ùå Partial strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        # Strategy 3: Full unfrozen (if partial worked well)\n",
    "        if results and max(r['val_accuracy'] for r in results) > 45:\n",
    "            print(f\"      üî• FULL UNFROZEN STRATEGY:\")\n",
    "            model_unfrozen = self.create_model(arch_name, model_name, freeze_strategy='none')\n",
    "            if model_unfrozen is not None:\n",
    "                try:\n",
    "                    val_acc_unfrozen, test_acc_unfrozen = self.ultra_aggressive_training(\n",
    "                        model_unfrozen, arch_name, train_loader, val_loader, test_loader, strategy='unfrozen'\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'name': f\"{arch_name}-Unfrozen\",\n",
    "                        'strategy': 'unfrozen',\n",
    "                        'val_accuracy': val_acc_unfrozen,\n",
    "                        'test_accuracy': test_acc_unfrozen\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"         ‚ùå Unfrozen strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_ultra_aggressive_test(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        \"\"\"Run ultra aggressive test on ALL architectures\"\"\"\n",
    "        print(\"üî• ULTRA AGGRESSIVE ARCHITECTURE TEST\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"üéØ ULTRA AGGRESSIVE OPTIMIZATIONS:\")\n",
    "        print(\"   ‚Ä¢ 60-100 epochs per strategy (vs 20)\")\n",
    "        print(\"   ‚Ä¢ 25-40 patience (vs 10)\")\n",
    "        print(\"   ‚Ä¢ Multiple training strategies per architecture\")\n",
    "        print(\"   ‚Ä¢ Step LR scheduler (more stable)\")\n",
    "        print(\"   ‚Ä¢ Strategy progression (frozen ‚Üí partial ‚Üí unfrozen)\")\n",
    "        print(\"   ‚Ä¢ NO QUITTING until models reach potential!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = DeerDataset(X_train, y_train)\n",
    "        val_dataset = DeerDataset(X_val, y_val)\n",
    "        test_dataset = DeerDataset(X_test, y_test)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "        \n",
    "        print(f\"üìä Data ready: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n",
    "        \n",
    "        # Get all architectures\n",
    "        architectures = self.get_all_architectures()\n",
    "        \n",
    "        all_results = []\n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\nüî• ULTRA AGGRESSIVE TESTING: {len(architectures)} ARCHITECTURES\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"‚è∞ WARNING: This will take many hours but WILL find the best performance!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, (arch_name, arch_info) in enumerate(architectures.items(), 1):\n",
    "            print(f\"\\n[{i}/{len(architectures)}] üî• ULTRA AGGRESSIVE {arch_name}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Test with multiple strategies\n",
    "            arch_results = self.test_architecture_with_multiple_strategies(\n",
    "                arch_name, arch_info['model_name'], train_loader, val_loader, test_loader\n",
    "            )\n",
    "            \n",
    "            # Add metadata and timing\n",
    "            for result in arch_results:\n",
    "                result['family'] = arch_info['family']\n",
    "                result['training_time'] = time.time() - start_time\n",
    "                all_results.append(result)\n",
    "            \n",
    "            if arch_results:\n",
    "                best_arch_result = max(arch_results, key=lambda x: x['test_accuracy'])\n",
    "                print(f\"      üèÜ Best {arch_name}: {best_arch_result['name']} ({best_arch_result['test_accuracy']:.1f}%)\")\n",
    "            \n",
    "            print(f\"      ‚è±Ô∏è Total time for {arch_name}: {time.time() - start_time:.1f}s\")\n",
    "            \n",
    "            # Intermediate leaderboard every 3 architectures\n",
    "            if i % 3 == 0:\n",
    "                current_best = sorted(all_results, key=lambda x: x['test_accuracy'], reverse=True)[:3]\n",
    "                print(f\"\\nüìä CURRENT LEADERBOARD (after {i} architectures):\")\n",
    "                for j, result in enumerate(current_best, 1):\n",
    "                    print(f\"   {j}. {result['name']}: {result['test_accuracy']:.1f}%\")\n",
    "                print()\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        \n",
    "        # Sort all results\n",
    "        all_results.sort(key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        \n",
    "        # Display ultra comprehensive results\n",
    "        print(f\"\\nüèÜ ULTRA AGGRESSIVE FINAL RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"‚è∞ Total testing time: {total_time/3600:.1f} hours\")\n",
    "        print(f\"üéØ Models tested: {len(all_results)}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Rank':<4} {'Model':<30} {'Strategy':<10} {'Val%':<8} {'Test%':<8} {'vs 54.2%'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(all_results, 1):\n",
    "            val_acc = result['val_accuracy']\n",
    "            test_acc = result['test_accuracy']\n",
    "            strategy = result['strategy']\n",
    "            \n",
    "            if test_acc >= 75.0:\n",
    "                status = \"üéâ BREAKTHROUGH!\"\n",
    "            elif test_acc >= 65.0:\n",
    "                status = \"üî• EXCELLENT!\"\n",
    "            elif test_acc > 54.2:\n",
    "                status = \"üöÄ NEW BEST!\"\n",
    "            elif test_acc > 45.0:\n",
    "                status = \"üìà Good\"\n",
    "            else:\n",
    "                status = \"üìâ Weak\"\n",
    "            \n",
    "            print(f\"{i:<4} {result['name']:<30} {strategy:<10} {val_acc:<7.1f} {test_acc:<7.1f} {status}\")\n",
    "        \n",
    "        # Analysis\n",
    "        if all_results:\n",
    "            best = all_results[0]\n",
    "            breakthrough_count = sum(1 for r in all_results if r['test_accuracy'] > 54.2)\n",
    "            excellent_count = sum(1 for r in all_results if r['test_accuracy'] >= 65.0)\n",
    "            \n",
    "            print(f\"\\nüéä ULTRA AGGRESSIVE SUMMARY:\")\n",
    "            print(f\"   üèÜ ULTIMATE CHAMPION: {best['name']} ({best['test_accuracy']:.1f}%)\")\n",
    "            print(f\"   üöÄ Beat 54.2% baseline: {breakthrough_count}/{len(all_results)} models\")\n",
    "            print(f\"   üéâ Achieved 65%+: {excellent_count} models\")\n",
    "            \n",
    "            if best['test_accuracy'] >= 75.0:\n",
    "                print(f\"   üéâ MISSION ACCOMPLISHED! Achieved 75%+ accuracy!\")\n",
    "            elif best['test_accuracy'] >= 65.0:\n",
    "                print(f\"   üéä EXCELLENT! Found 65%+ architecture!\")\n",
    "            elif best['test_accuracy'] > 54.2:\n",
    "                improvement = best['test_accuracy'] - 54.2\n",
    "                print(f\"   üöÄ SUCCESS! Improved by +{improvement:.1f}% over baseline!\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        return all_results\n",
    "\n",
    "def run_ultra_aggressive_pipeline():\n",
    "    \"\"\"Run the ultra aggressive pipeline\"\"\"\n",
    "    print(\"üî• ULTRA AGGRESSIVE DEER AGING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"üéØ GOAL: Train each architecture to its MAXIMUM potential\")\n",
    "    print(\"üéØ METHOD: 60-100 epochs, multiple strategies, high patience\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    images, ages = load_original_data()\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, label_mapping, reverse_mapping = create_train_val_test_split(images, ages)\n",
    "    X_train_aug, y_train_aug = balance_and_augment_data(X_train, y_train, augment_multiplier=30, num_classes=len(label_mapping))\n",
    "    \n",
    "    # Ultra aggressive testing\n",
    "    trainer = UltraAggressiveTrainer(num_classes=len(label_mapping))\n",
    "    results = trainer.run_ultra_aggressive_test(X_train_aug, y_train_aug, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    return results, label_mapping\n",
    "\n",
    "# üî• RUN ULTRA AGGRESSIVE PIPELINE\n",
    "print(\"üî• LAUNCHING ULTRA AGGRESSIVE PIPELINE...\")\n",
    "print(\"‚ö†Ô∏è  This will train each architecture to its MAXIMUM potential - expect 8-12 hours!\")\n",
    "print(\"üí™ But you wanted proper training time - here it is!\")\n",
    "ultra_results, final_label_mapping = run_ultra_aggressive_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a316f97-c111-4262-8bef-f33af9d68e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
