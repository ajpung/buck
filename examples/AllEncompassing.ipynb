{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2dad4-961a-479e-9619-1a78b5561d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAUNCHING COMPLETE DEER AGING PIPELINE...\n",
      "Starting from EfficientNet-B5 (as requested)\n",
      "TESTING ALL MODELS: Pretrained -> Alternatives -> Random Init\n",
      "All results will be automatically saved\n",
      "Will show which models used pretrained vs random initialization\n",
      "================================================================================\n",
      "CRASH RECOVERY TIPS:\n",
      "   Progress files: 'deer_aging_progress_YYYYMMDD_HHMMSS.txt'\n",
      "   Backup files: 'deer_aging_backup_*.pkl'\n",
      "   To recover: results = load_latest_results()\n",
      "   To test data loading: test_data_loading()\n",
      "================================================================================\n",
      "Testing data loading before starting pipeline...\n",
      "TESTING DATA LOADING\n",
      "==================================================\n",
      "Current working directory: G:\\Dropbox\\AI Projects\\buck\\examples\n",
      "\n",
      "Testing path: G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\\*_NDA.png\n",
      "   Files found: 197\n",
      "   First few files: ['G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\111009_111009_KY_2p5_NDA.png', 'G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\171220_171220_MO_2p5_NDA.png', 'G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\171220_171220_MO_3p5_NDA.png']\n",
      "   This path works!\n",
      "\n",
      "Successfully imported buck.analysis.basics\n",
      "\n",
      "Testing ingest_images with: G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\\*_NDA.png\n",
      "ingest_images returned: 196 images, 196 ages\n",
      "Sample ages: [2.5, 2.5, 3.5, 2.5, 3.5]\n",
      "Age range: 1.5 to 12.5\n",
      "Unique ages: [1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 8.5, 12.5]\n",
      "Data loading test passed!\n",
      "\n",
      "Starting main pipeline...\n",
      "LAUNCHING COMPLETE DEER AGING PIPELINE\n",
      "================================================================================\n",
      "PIPELINE STEPS:\n",
      "   1. Load original 357 images\n",
      "   2. Create train/val/test splits\n",
      "   3. Balance and augment training data\n",
      "   4. Test all architectures (starting from EfficientNet-B5)\n",
      "   5. Save results and create leaderboard\n",
      "================================================================================\n",
      "CRASH RECOVERY: Progress saved after each model!\n",
      "Look for 'deer_aging_progress_*.txt' files for latest results\n",
      "JSON/pickle backups: 'deer_aging_backup_*.json/.pkl'\n",
      "================================================================================\n",
      "LOADING ORIGINAL DATA\n",
      "==================================================\n",
      "   Trying primary path: G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\\*_NDA.png\n",
      "   SUCCESS: Loaded 196 original images from: G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\\*_NDA.png\n",
      "   Grouping ages: 5.5+ -> 5.5\n",
      "   Original age distribution: {2.5: 36, 3.5: 35, 4.5: 52, 5.5: 32, 12.5: 1, 1.5: 30, 6.5: 6, 8.5: 4}\n",
      "   Grouped age distribution: {2.5: 36, 3.5: 35, 4.5: 52, 5.5: 43, 1.5: 30}\n",
      "\n",
      "CREATING TRAIN/VAL/TEST SPLIT\n",
      "==================================================\n",
      "   Age distribution: {np.float64(2.5): 36, np.float64(3.5): 35, np.float64(4.5): 52, np.float64(5.5): 43, np.float64(1.5): 30}\n",
      "   Minimum class size: 30\n",
      "   Can use stratified split: True\n",
      "   Train: 126 samples\n",
      "   Val: 30 samples\n",
      "   Test: 40 samples\n",
      "   Label mapping: {np.float64(1.5): 0, np.float64(2.5): 1, np.float64(3.5): 2, np.float64(4.5): 3, np.float64(5.5): 4}\n",
      "   Number of classes: 5\n",
      "   Train distribution: Counter({np.int64(3): 33, np.int64(4): 28, np.int64(2): 23, np.int64(1): 23, np.int64(0): 19})\n",
      "   Val distribution: Counter({np.int64(3): 8, np.int64(1): 6, np.int64(4): 6, np.int64(2): 5, np.int64(0): 5})\n",
      "   Test distribution: Counter({np.int64(3): 11, np.int64(4): 9, np.int64(1): 7, np.int64(2): 7, np.int64(0): 6})\n",
      "\n",
      "BALANCING AND AUGMENTING DATA\n",
      "==================================================\n",
      "   Target: 30x augmentation per class\n",
      "   Original distribution: {np.int64(4): 28, np.int64(2): 23, np.int64(3): 33, np.int64(0): 19, np.int64(1): 23}\n",
      "   Target samples per class: 990\n",
      "   Class 0: 19 -> 990 samples\n",
      "   Class 1: 23 -> 990 samples\n",
      "   Class 2: 23 -> 990 samples\n",
      "   Class 3: 33 -> 990 samples\n",
      "   Class 4: 28 -> 990 samples\n",
      "   Augmentation complete: 4950 total samples\n",
      "   Final distribution: Counter({np.int64(0): 990, np.int64(1): 990, np.int64(2): 990, np.int64(3): 990, np.int64(4): 990})\n",
      "COMPLETE DEER AGE TRAINER\n",
      "   Device: cpu\n",
      "   Classes: 5\n",
      "COMPLETE DEER AGING PIPELINE\n",
      "================================================================================\n",
      "Starting from EfficientNet-B5 onwards\n",
      "All results will be saved automatically\n",
      "================================================================================\n",
      "Data ready: 4950 train, 30 val, 40 test\n",
      "\n",
      "COMPLETE ARCHITECTURE ARSENAL (33 models)\n",
      "================================================================================\n",
      "FALLBACK STRATEGY: Pretrained -> Alternative Names -> Random Init\n",
      "ALL models will be tested regardless of pretrained weight availability\n",
      "ResNet (7 models): ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152, ResNet-26, ResNet-26d\n",
      "ResNeXt (2 models): ResNeXt-50, ResNeXt-101\n",
      "Wide-ResNet (2 models): Wide-ResNet-50, Wide-ResNet-101\n",
      "MobileNet (3 models): MobileNetV2, MobileNetV3-Small, MobileNetV3-Large\n",
      "RegNet (4 models): RegNetX-400MF, RegNetX-800MF, RegNetY-400MF, RegNetY-800MF\n",
      "ConvNeXt (3 models): ConvNeXt-Tiny, ConvNeXt-Small, ConvNeXt-Base\n",
      "Swin (2 models): Swin-Tiny, Swin-Small\n",
      "VGG (2 models): VGG-16, VGG-19\n",
      "DeiT (3 models): DeiT-Tiny, DeiT-Small, DeiT-Base\n",
      "EfficientNet (3 models): EfficientNet-ES, EfficientNet-EM, EfficientNet-EL\n",
      "SEResNet (1 models): SEResNet-50\n",
      "SEResNeXt (1 models): SEResNeXt-50\n",
      "\n",
      "ULTRA AGGRESSIVE TESTING: 33 ARCHITECTURES\n",
      "================================================================================\n",
      "\n",
      "[1/33] ULTRA AGGRESSIVE ResNet-18\n",
      "----------------------------------------------------------------------\n",
      "      FROZEN BACKBONE STRATEGY:\n",
      "      Creating ResNet-18...\n",
      "         Trying pretrained: resnet18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aad54a964ff4fb1b23ec1fb0e67aa96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         SUCCESS with pretrained weights!\n",
      "         Freezing backbone layers...\n",
      "         Loaded: 11,179,077 total, 2,565 trainable (pretrained)\n",
      "      ULTRA AGGRESSIVE TRAINING: ResNet-18 (frozen)...\n",
      "         ULTRA SETUP: 100 epochs, LR=0.01, patience=50\n",
      "         Epoch   0: Train 21.6%, Val 20.0% (gap: +1.6%), LR: 1.00e-02 BEST\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_original_data():\n",
    "    \"\"\"Load the original 357 images\"\"\"\n",
    "    print(\"LOADING ORIGINAL DATA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Load using the user's method\n",
    "        from buck.analysis.basics import ingest_images\n",
    "        \n",
    "        # Try primary path first\n",
    "        fpath = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "        print(f\"   Trying primary path: {fpath}\")\n",
    "        \n",
    "        images, ages = ingest_images(fpath)\n",
    "        \n",
    "        print(f\"   SUCCESS: Loaded {len(images)} original images from: {fpath}\")\n",
    "        \n",
    "        # Group ages: 5.5+ all become 5.5 (creating exactly 5 classes)\n",
    "        print(\"   Grouping ages: 5.5+ -> 5.5\")\n",
    "        ages_grouped = []\n",
    "        for age in ages:\n",
    "            if age >= 5.5:\n",
    "                ages_grouped.append(5.5)\n",
    "            else:\n",
    "                ages_grouped.append(age)\n",
    "        \n",
    "        # Verify we still have data after grouping\n",
    "        if len(ages_grouped) == 0:\n",
    "            print(\"   ERROR: No ages remaining after grouping\")\n",
    "            raise ValueError(\"No ages remaining after grouping\")\n",
    "        \n",
    "        # Print age distribution before and after grouping\n",
    "        print(f\"   Original age distribution: {dict(Counter(ages))}\")\n",
    "        print(f\"   Grouped age distribution: {dict(Counter(ages_grouped))}\")\n",
    "        \n",
    "        return images, ages_grouped\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"   ERROR: Cannot import buck.analysis.basics: {e}\")\n",
    "        print(\"   Make sure the buck package is in your Python path\")\n",
    "        print(\"   You might need to run: sys.path.append('/path/to/buck')\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR loading data: {e}\")\n",
    "        print(\"   Run test_data_loading() to debug the issue\")\n",
    "        raise\n",
    "\n",
    "def create_train_val_test_split(images, ages, test_size=0.2, val_size=0.15, random_state=42):\n",
    "    \"\"\"Create train/validation/test split\"\"\"\n",
    "    print(\"\\nCREATING TRAIN/VAL/TEST SPLIT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Validate input data\n",
    "    if len(images) == 0 or len(ages) == 0:\n",
    "        print(f\"   ERROR: Empty input data - images: {len(images)}, ages: {len(ages)}\")\n",
    "        raise ValueError(\"Input data is empty\")\n",
    "    \n",
    "    if len(images) != len(ages):\n",
    "        print(f\"   ERROR: Mismatched data lengths - images: {len(images)}, ages: {len(ages)}\")\n",
    "        raise ValueError(\"Images and ages must have the same length\")\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    if not isinstance(images, np.ndarray):\n",
    "        images = np.array(images)\n",
    "    if not isinstance(ages, np.ndarray):\n",
    "        ages = np.array(ages)\n",
    "    \n",
    "    # Check if stratified split is possible\n",
    "    age_counts = Counter(ages)\n",
    "    \n",
    "    # Additional check for empty counter\n",
    "    if len(age_counts) == 0:\n",
    "        print(f\"   ERROR: No age data found\")\n",
    "        raise ValueError(\"Age counter is empty\")\n",
    "    \n",
    "    min_count = min(age_counts.values())\n",
    "    can_stratify = min_count >= 2\n",
    "    \n",
    "    print(f\"   Age distribution: {dict(age_counts)}\")\n",
    "    print(f\"   Minimum class size: {min_count}\")\n",
    "    print(f\"   Can use stratified split: {can_stratify}\")\n",
    "    \n",
    "    if can_stratify:\n",
    "        # First split: separate test set (stratified)\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            images, ages, test_size=test_size, random_state=random_state, stratify=ages\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation from remaining data\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # Adjust for remaining data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, stratify=y_temp\n",
    "        )\n",
    "    else:\n",
    "        print(\"   WARNING: Using random split (some classes too small for stratification)\")\n",
    "        # First split: separate test set (random)\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            images, ages, test_size=test_size, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation from remaining data\n",
    "        val_size_adjusted = val_size / (1 - test_size)  # Adjust for remaining data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, shuffle=True\n",
    "        )\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    reverse_mapping = {i: age for age, i in label_mapping.items()}\n",
    "    \n",
    "    print(f\"   Train: {len(X_train)} samples\")\n",
    "    print(f\"   Val: {len(X_val)} samples\") \n",
    "    print(f\"   Test: {len(X_test)} samples\")\n",
    "    print(f\"   Label mapping: {label_mapping}\")\n",
    "    print(f\"   Number of classes: {len(unique_ages)}\")\n",
    "    \n",
    "    # Convert ages to class indices\n",
    "    y_train_indices = np.array([label_mapping[age] for age in y_train])\n",
    "    y_val_indices = np.array([label_mapping[age] for age in y_val])\n",
    "    y_test_indices = np.array([label_mapping[age] for age in y_test])\n",
    "    \n",
    "    print(f\"   Train distribution: {Counter(y_train_indices)}\")\n",
    "    print(f\"   Val distribution: {Counter(y_val_indices)}\")\n",
    "    print(f\"   Test distribution: {Counter(y_test_indices)}\")\n",
    "    \n",
    "    return (X_train, y_train_indices, X_val, y_val_indices, X_test, y_test_indices, \n",
    "            label_mapping, reverse_mapping)\n",
    "\n",
    "def augment_image(image):\n",
    "    \"\"\"Apply random augmentation to an image\"\"\"\n",
    "    # Ensure image is uint8\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Random rotation\n",
    "    if random.random() < 0.5:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Random horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Random brightness/contrast\n",
    "    if random.random() < 0.5:\n",
    "        alpha = random.uniform(0.8, 1.2)  # Contrast\n",
    "        beta = random.randint(-20, 20)    # Brightness\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Random noise (fixed data type issue)\n",
    "    if random.random() < 0.3:\n",
    "        # Create noise with same dtype as image\n",
    "        noise = np.random.normal(0, 5, image.shape).astype(np.int16)  # Use int16 to handle negative values\n",
    "        # Convert image to int16 for safe addition\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        # Add noise and clip to valid range\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        # Convert back to uint8\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def balance_and_augment_data(X_train, y_train, augment_multiplier=30, num_classes=5):\n",
    "    \"\"\"Balance classes and augment training data\"\"\"\n",
    "    print(f\"\\nBALANCING AND AUGMENTING DATA\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"   Target: {augment_multiplier}x augmentation per class\")\n",
    "    \n",
    "    # Count samples per class\n",
    "    class_counts = Counter(y_train)\n",
    "    print(f\"   Original distribution: {dict(class_counts)}\")\n",
    "    \n",
    "    # Find target count (based on largest class * multiplier)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max_count * augment_multiplier\n",
    "    print(f\"   Target samples per class: {target_count}\")\n",
    "    \n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Get samples for this class\n",
    "        class_mask = y_train == class_idx\n",
    "        class_images = X_train[class_mask]\n",
    "        class_labels = y_train[class_mask]\n",
    "        \n",
    "        current_count = len(class_images)\n",
    "        needed_count = target_count\n",
    "        \n",
    "        print(f\"   Class {class_idx}: {current_count} -> {needed_count} samples\")\n",
    "        \n",
    "        # Add original samples\n",
    "        X_augmented.extend(class_images)\n",
    "        y_augmented.extend(class_labels)\n",
    "        \n",
    "        # Generate augmented samples\n",
    "        augmented_needed = needed_count - current_count\n",
    "        \n",
    "        for i in range(augmented_needed):\n",
    "            # Pick random original image from this class\n",
    "            original_idx = random.randint(0, current_count - 1)\n",
    "            original_image = class_images[original_idx].copy()\n",
    "            \n",
    "            # Augment it\n",
    "            augmented_image = augment_image(original_image)\n",
    "            \n",
    "            X_augmented.append(augmented_image)\n",
    "            y_augmented.append(class_idx)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    X_augmented = np.array(X_augmented)\n",
    "    y_augmented = np.array(y_augmented)\n",
    "    \n",
    "    print(f\"   Augmentation complete: {len(X_augmented)} total samples\")\n",
    "    print(f\"   Final distribution: {Counter(y_augmented)}\")\n",
    "    \n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    \"\"\"Dataset for deer aging with preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, transform=True):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X = torch.FloatTensor(X)\n",
    "        else:\n",
    "            self.X = torch.FloatTensor(np.array(X))\n",
    "            \n",
    "        if isinstance(y, np.ndarray):\n",
    "            self.y = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.y = torch.LongTensor(np.array(y))\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Normalize to [0,1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Ensure CHW format (channels first)\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Resize to 224x224\n",
    "        if image.shape[-2:] != (224, 224):\n",
    "            image = image.unsqueeze(0)\n",
    "            image = F.interpolate(image, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            image = image.squeeze(0)\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        image = (image - mean) / std\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class CompleteDeerAgeTrainer:\n",
    "    \"\"\"Complete deer age trainer starting from EfficientNet-B5\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.all_results = []\n",
    "        print(f\"COMPLETE DEER AGE TRAINER\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Classes: {num_classes}\")\n",
    "    \n",
    "    def get_all_architectures_with_fallback(self):\n",
    "        \"\"\"Get ALL architectures with graceful fallback for missing pretrained weights\"\"\"\n",
    "        \n",
    "        architectures = {\n",
    "            # DenseNet Family\n",
    "            #'DenseNet-201': {'model_name': 'densenet201', 'family': 'DenseNet'},\n",
    "            \n",
    "            # ResNet Family\n",
    "            'ResNet-18': {'model_name': 'resnet18', 'family': 'ResNet'},\n",
    "            'ResNet-34': {'model_name': 'resnet34', 'family': 'ResNet'},\n",
    "            'ResNet-50': {'model_name': 'resnet50', 'family': 'ResNet'},\n",
    "            'ResNet-101': {'model_name': 'resnet101', 'family': 'ResNet'},\n",
    "            'ResNet-152': {'model_name': 'resnet152', 'family': 'ResNet'},\n",
    "            \n",
    "            # ResNeXt\n",
    "            'ResNeXt-50': {'model_name': 'resnext50_32x4d', 'family': 'ResNeXt'},\n",
    "            'ResNeXt-101': {'model_name': 'resnext101_32x8d', 'family': 'ResNeXt'},\n",
    "            \n",
    "            # Wide ResNet\n",
    "            'Wide-ResNet-50': {'model_name': 'wide_resnet50_2', 'family': 'Wide-ResNet'},\n",
    "            'Wide-ResNet-101': {'model_name': 'wide_resnet101_2', 'family': 'Wide-ResNet'},\n",
    "            \n",
    "            # MobileNet Family\n",
    "            'MobileNetV2': {'model_name': 'mobilenetv2_100', 'family': 'MobileNet'},\n",
    "            'MobileNetV3-Small': {'model_name': 'mobilenetv3_small_100', 'family': 'MobileNet'},\n",
    "            'MobileNetV3-Large': {'model_name': 'mobilenetv3_large_100', 'family': 'MobileNet'},\n",
    "            \n",
    "            # RegNet Family (try multiple naming conventions)\n",
    "            'RegNetX-400MF': {'model_name': 'regnetx_400mf', 'family': 'RegNet', 'alternatives': ['regnetx_002', 'regnetx_004']},\n",
    "            'RegNetX-800MF': {'model_name': 'regnetx_800mf', 'family': 'RegNet', 'alternatives': ['regnetx_004', 'regnetx_006']},\n",
    "            'RegNetY-400MF': {'model_name': 'regnety_400mf', 'family': 'RegNet', 'alternatives': ['regnety_002', 'regnety_004']},\n",
    "            'RegNetY-800MF': {'model_name': 'regnety_800mf', 'family': 'RegNet', 'alternatives': ['regnety_004', 'regnety_006']},\n",
    "            \n",
    "            # ConvNeXt Family\n",
    "            'ConvNeXt-Tiny': {'model_name': 'convnext_tiny', 'family': 'ConvNeXt', 'alternatives': ['convnext_tiny_in22ft1k']},\n",
    "            'ConvNeXt-Small': {'model_name': 'convnext_small', 'family': 'ConvNeXt', 'alternatives': ['convnext_small_in22ft1k']},\n",
    "            'ConvNeXt-Base': {'model_name': 'convnext_base', 'family': 'ConvNeXt', 'alternatives': ['convnext_base_in22ft1k']},\n",
    "            \n",
    "            # Vision Transformer variants\n",
    "            'Swin-Tiny': {'model_name': 'swin_tiny_patch4_window7_224', 'family': 'Swin', 'alternatives': ['swin_tiny_patch4_window7_224_in22k']},\n",
    "            'Swin-Small': {'model_name': 'swin_small_patch4_window7_224', 'family': 'Swin', 'alternatives': ['swin_small_patch4_window7_224_in22k']},\n",
    "            \n",
    "            # VGG (classic)\n",
    "            'VGG-16': {'model_name': 'vgg16', 'family': 'VGG'},\n",
    "            'VGG-19': {'model_name': 'vgg19', 'family': 'VGG'},\n",
    "            \n",
    "            # Vision Transformers\n",
    "            'DeiT-Tiny': {'model_name': 'deit_tiny_patch16_224', 'family': 'DeiT'},\n",
    "            'DeiT-Small': {'model_name': 'deit_small_patch16_224', 'family': 'DeiT'},\n",
    "            'DeiT-Base': {'model_name': 'deit_base_patch16_224', 'family': 'DeiT'},\n",
    "            \n",
    "            # Additional EfficientNet variants\n",
    "            'EfficientNet-ES': {'model_name': 'efficientnet_es', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-EM': {'model_name': 'efficientnet_em', 'family': 'EfficientNet'},\n",
    "            'EfficientNet-EL': {'model_name': 'efficientnet_el', 'family': 'EfficientNet'},\n",
    "            \n",
    "            # Additional strong architectures\n",
    "            'ResNet-26': {'model_name': 'resnet26', 'family': 'ResNet'},\n",
    "            'ResNet-26d': {'model_name': 'resnet26d', 'family': 'ResNet'},\n",
    "            'SEResNet-50': {'model_name': 'seresnet50', 'family': 'SEResNet'},\n",
    "            'SEResNeXt-50': {'model_name': 'seresnext50_32x4d', 'family': 'SEResNeXt'},\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nCOMPLETE ARCHITECTURE ARSENAL ({len(architectures)} models)\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"FALLBACK STRATEGY: Pretrained -> Alternative Names -> Random Init\")\n",
    "        print(\"ALL models will be tested regardless of pretrained weight availability\")\n",
    "        \n",
    "        # Group by family and show counts\n",
    "        families = {}\n",
    "        for arch_name, arch_info in architectures.items():\n",
    "            family = arch_info['family']\n",
    "            if family not in families:\n",
    "                families[family] = []\n",
    "            families[family].append(arch_name)\n",
    "        \n",
    "        for family, models in families.items():\n",
    "            print(f\"{family} ({len(models)} models): {', '.join(models)}\")\n",
    "        \n",
    "        return architectures\n",
    "    \n",
    "    def create_model_with_fallback(self, arch_name, arch_info, freeze_strategy='none'):\n",
    "        \"\"\"Create model with graceful fallback for missing pretrained weights\"\"\"\n",
    "        model_name = arch_info['model_name']\n",
    "        alternatives = arch_info.get('alternatives', [])\n",
    "        \n",
    "        print(f\"      Creating {arch_name}...\")\n",
    "        \n",
    "        # Strategy 1: Try primary model name with pretrained=True\n",
    "        try:\n",
    "            print(f\"         Trying pretrained: {model_name}\")\n",
    "            model = timm.create_model(model_name, pretrained=True, num_classes=self.num_classes)\n",
    "            initialization_type = \"pretrained\"\n",
    "            final_model_name = model_name\n",
    "            print(f\"         SUCCESS with pretrained weights!\")\n",
    "        except Exception as e1:\n",
    "            print(f\"         Pretrained failed: {str(e1)[:50]}...\")\n",
    "            \n",
    "            # Strategy 2: Try alternative names with pretrained=True\n",
    "            model = None\n",
    "            for alt_name in alternatives:\n",
    "                try:\n",
    "                    print(f\"         Trying alternative pretrained: {alt_name}\")\n",
    "                    model = timm.create_model(alt_name, pretrained=True, num_classes=self.num_classes)\n",
    "                    initialization_type = \"pretrained_alt\"\n",
    "                    final_model_name = alt_name\n",
    "                    print(f\"         SUCCESS with alternative pretrained weights!\")\n",
    "                    break\n",
    "                except Exception as e2:\n",
    "                    print(f\"         Alternative {alt_name} failed: {str(e2)[:30]}...\")\n",
    "                    continue\n",
    "            \n",
    "            # Strategy 3: Fall back to random initialization\n",
    "            if model is None:\n",
    "                try:\n",
    "                    print(f\"         Falling back to random initialization: {model_name}\")\n",
    "                    model = timm.create_model(model_name, pretrained=False, num_classes=self.num_classes)\n",
    "                    initialization_type = \"random\"\n",
    "                    final_model_name = model_name\n",
    "                    print(f\"         SUCCESS with random initialization!\")\n",
    "                except Exception as e3:\n",
    "                    # Try alternatives with random initialization\n",
    "                    for alt_name in alternatives:\n",
    "                        try:\n",
    "                            print(f\"         Trying alternative random: {alt_name}\")\n",
    "                            model = timm.create_model(alt_name, pretrained=False, num_classes=self.num_classes)\n",
    "                            initialization_type = \"random_alt\"\n",
    "                            final_model_name = alt_name\n",
    "                            print(f\"         SUCCESS with alternative random initialization!\")\n",
    "                            break\n",
    "                        except Exception as e4:\n",
    "                            continue\n",
    "                    \n",
    "                    # If still failed, return None\n",
    "                    if model is None:\n",
    "                        print(f\"         COMPLETE FAILURE: All strategies failed\")\n",
    "                        return None, None, None\n",
    "        \n",
    "        # Apply freezing strategy\n",
    "        if freeze_strategy == 'backbone':\n",
    "            print(f\"         Freezing backbone layers...\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'classifier' not in name and 'head' not in name and 'fc' not in name:\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"         Loaded: {total_params:,} total, {trainable_params:,} trainable ({initialization_type})\")\n",
    "        \n",
    "        elif freeze_strategy == 'partial':\n",
    "            print(f\"         Partial freeze (last 30% unfrozen)...\")\n",
    "            all_params = list(model.named_parameters())\n",
    "            total_layers = len(all_params)\n",
    "            freeze_until = int(total_layers * 0.7)\n",
    "            \n",
    "            for i, (name, param) in enumerate(all_params):\n",
    "                if i < freeze_until and 'classifier' not in name and 'head' not in name and 'fc' not in name:\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"         Loaded: {total_params:,} total, {trainable_params:,} trainable ({initialization_type})\")\n",
    "        \n",
    "        else:  # no freezing\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"         Loaded: {total_params:,} parameters (all trainable, {initialization_type})\")\n",
    "        \n",
    "        model = model.to(self.device)\n",
    "        return model, initialization_type, final_model_name\n",
    "    \n",
    "    def ultra_aggressive_training(self, model, arch_name, train_loader, val_loader, test_loader, strategy='unfrozen'):\n",
    "        \"\"\"Ultra aggressive training with minimal early stopping\"\"\"\n",
    "        print(f\"      ULTRA AGGRESSIVE TRAINING: {arch_name} ({strategy})...\")\n",
    "        \n",
    "        # More aggressive setup\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Strategy-specific hyperparameters\n",
    "        if strategy == 'frozen':\n",
    "            lr = 0.01\n",
    "            max_epochs = 100\n",
    "            patience = 50\n",
    "        elif strategy == 'partial':\n",
    "            lr = 0.005\n",
    "            max_epochs = 100\n",
    "            patience = 50\n",
    "        else:  # unfrozen\n",
    "            lr = 0.001\n",
    "            max_epochs = 100\n",
    "            patience = 50\n",
    "        \n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=0.01,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        # Simple step scheduler\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"         ULTRA SETUP: {max_epochs} epochs, LR={lr}, patience={patience}\")\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            # Very lenient early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                improvement = \"BEST\"\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                improvement = \"\"\n",
    "            \n",
    "            # More frequent progress updates\n",
    "            if epoch % 5 == 0 or epoch < 10 or improvement or epoch > max_epochs - 10:\n",
    "                gap = train_acc - val_acc\n",
    "                print(f\"         Epoch {epoch:3d}: Train {train_acc:.1f}%, Val {val_acc:.1f}% (gap: {gap:+.1f}%), LR: {current_lr:.2e} {improvement}\")\n",
    "        \n",
    "        # Restore best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Test evaluation\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        print(f\"         {arch_name} ({strategy}) FINAL: Val {best_val_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "        \n",
    "        return best_val_acc, test_acc\n",
    "    \n",
    "    def test_architecture_with_multiple_strategies(self, arch_name, arch_info, train_loader, val_loader, test_loader):\n",
    "        \"\"\"Test architecture with multiple training strategies and fallback support\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Strategy 1: Frozen backbone (fast warmup)\n",
    "        print(f\"      FROZEN BACKBONE STRATEGY:\")\n",
    "        model_frozen, init_type_frozen, final_name_frozen = self.create_model_with_fallback(arch_name, arch_info, freeze_strategy='backbone')\n",
    "        if model_frozen is not None:\n",
    "            try:\n",
    "                val_acc_frozen, test_acc_frozen = self.ultra_aggressive_training(\n",
    "                    model_frozen, arch_name, train_loader, val_loader, test_loader, strategy='frozen'\n",
    "                )\n",
    "                results.append({\n",
    "                    'name': f\"{arch_name}-Frozen\",\n",
    "                    'strategy': 'frozen',\n",
    "                    'val_accuracy': val_acc_frozen,\n",
    "                    'test_accuracy': test_acc_frozen,\n",
    "                    'family': arch_info['family'],\n",
    "                    'initialization': init_type_frozen,\n",
    "                    'final_model_name': final_name_frozen,\n",
    "                    'original_model_name': arch_info['model_name']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"         Frozen strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        # Strategy 2: Partial freeze (if frozen worked reasonably)\n",
    "        if results and results[-1]['val_accuracy'] > 35:\n",
    "            print(f\"      PARTIAL FREEZE STRATEGY:\")\n",
    "            model_partial, init_type_partial, final_name_partial = self.create_model_with_fallback(arch_name, arch_info, freeze_strategy='partial')\n",
    "            if model_partial is not None:\n",
    "                try:\n",
    "                    val_acc_partial, test_acc_partial = self.ultra_aggressive_training(\n",
    "                        model_partial, arch_name, train_loader, val_loader, test_loader, strategy='partial'\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'name': f\"{arch_name}-Partial\",\n",
    "                        'strategy': 'partial',\n",
    "                        'val_accuracy': val_acc_partial,\n",
    "                        'test_accuracy': test_acc_partial,\n",
    "                        'family': arch_info['family'],\n",
    "                        'initialization': init_type_partial,\n",
    "                        'final_model_name': final_name_partial,\n",
    "                        'original_model_name': arch_info['model_name']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"         Partial strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        # Strategy 3: Full unfrozen (if partial worked well)\n",
    "        if results and max(r['val_accuracy'] for r in results) > 45:\n",
    "            print(f\"      FULL UNFROZEN STRATEGY:\")\n",
    "            model_unfrozen, init_type_unfrozen, final_name_unfrozen = self.create_model_with_fallback(arch_name, arch_info, freeze_strategy='none')\n",
    "            if model_unfrozen is not None:\n",
    "                try:\n",
    "                    val_acc_unfrozen, test_acc_unfrozen = self.ultra_aggressive_training(\n",
    "                        model_unfrozen, arch_name, train_loader, val_loader, test_loader, strategy='unfrozen'\n",
    "                    )\n",
    "                    results.append({\n",
    "                        'name': f\"{arch_name}-Unfrozen\",\n",
    "                        'strategy': 'unfrozen',\n",
    "                        'val_accuracy': val_acc_unfrozen,\n",
    "                        'test_accuracy': test_acc_unfrozen,\n",
    "                        'family': arch_info['family'],\n",
    "                        'initialization': init_type_unfrozen,\n",
    "                        'final_model_name': final_name_unfrozen,\n",
    "                        'original_model_name': arch_info['model_name']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"         Unfrozen strategy failed: {str(e)[:50]}...\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_complete_pipeline(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        \"\"\"Run the complete pipeline with result storage\"\"\"\n",
    "        print(\"COMPLETE DEER AGING PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Starting from EfficientNet-B5 onwards\")\n",
    "        print(\"All results will be saved automatically\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = DeerDataset(X_train, y_train)\n",
    "        val_dataset = DeerDataset(X_val, y_val)\n",
    "        test_dataset = DeerDataset(X_test, y_test)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "        \n",
    "        print(f\"Data ready: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test\")\n",
    "        \n",
    "        # Get all architectures\n",
    "        architectures = self.get_all_architectures_with_fallback()\n",
    "        \n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\nULTRA AGGRESSIVE TESTING: {len(architectures)} ARCHITECTURES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, (arch_name, arch_info) in enumerate(architectures.items(), 1):\n",
    "            print(f\"\\n[{i}/{len(architectures)}] ULTRA AGGRESSIVE {arch_name}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Test with multiple strategies\n",
    "            arch_results = self.test_architecture_with_multiple_strategies(\n",
    "                arch_name, arch_info, train_loader, val_loader, test_loader\n",
    "            )\n",
    "            \n",
    "            # Add metadata and timing\n",
    "            for result in arch_results:\n",
    "                result['architecture_family'] = arch_info['family']\n",
    "                result['training_time'] = time.time() - start_time\n",
    "                result['timestamp'] = datetime.now().isoformat()\n",
    "                self.all_results.append(result)\n",
    "            \n",
    "            if arch_results:\n",
    "                best_arch_result = max(arch_results, key=lambda x: x['test_accuracy'])\n",
    "                print(f\"      Best {arch_name}: {best_arch_result['name']} ({best_arch_result['test_accuracy']:.1f}%)\")\n",
    "            \n",
    "            print(f\"      Total time for {arch_name}: {time.time() - start_time:.1f}s\")\n",
    "            \n",
    "            # Save progress text file and backup files after each architecture\n",
    "            elapsed_time = time.time() - total_start_time\n",
    "            progress_file = self.save_progress_text_file(i, len(architectures), arch_name, arch_results, elapsed_time)\n",
    "            json_file, pkl_file = self.save_backup_files(i)\n",
    "            \n",
    "            print(f\"      Backups: {json_file}, {pkl_file}\")\n",
    "            \n",
    "            # Intermediate leaderboard every 3 architectures\n",
    "            if i % 3 == 0:\n",
    "                self.show_intermediate_leaderboard(i)\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        \n",
    "        # Save final results\n",
    "        self.save_final_results(total_time)\n",
    "        \n",
    "        # Display final leaderboard\n",
    "        self.show_final_leaderboard(total_time)\n",
    "        \n",
    "        return self.all_results\n",
    "    \n",
    "    def save_progress_text_file(self, completed_count, total_count, arch_name, arch_results, total_time_so_far):\n",
    "        \"\"\"Save human-readable progress text file after each architecture\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'deer_aging_progress_{timestamp}.txt'\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"DEER AGING PIPELINE - PROGRESS REPORT\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Runtime so far: {total_time_so_far/3600:.2f} hours\\n\")\n",
    "            f.write(f\"Progress: {completed_count}/{total_count} architectures completed\\n\")\n",
    "            f.write(f\"Device: {self.device}\\n\")\n",
    "            f.write(f\"Classes: {self.num_classes}\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            # Just completed architecture results\n",
    "            f.write(f\"JUST COMPLETED: {arch_name}\\n\")\n",
    "            f.write(\"-\"*50 + \"\\n\")\n",
    "            if arch_results:\n",
    "                for result in arch_results:\n",
    "                    init_type = result.get('initialization', 'unknown')\n",
    "                    f.write(f\"   {result['name']:30} | {result['strategy']:8} | {init_type:12} | Val: {result['val_accuracy']:5.1f}% | Test: {result['test_accuracy']:5.1f}%\\n\")\n",
    "                \n",
    "                best_arch = max(arch_results, key=lambda x: x['test_accuracy'])\n",
    "                f.write(f\"   Best: {best_arch['name']} ({best_arch['test_accuracy']:.1f}%)\\n\")\n",
    "            else:\n",
    "                f.write(\"   No successful results for this architecture\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Current overall leaderboard (top 10)\n",
    "            sorted_results = sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "            f.write(f\"CURRENT TOP 10 LEADERBOARD\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(f\"{'Rank':<4} {'Model':<30} {'Strategy':<10} {'Init':<12} {'Val%':<8} {'Test%':<8}\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            \n",
    "            for i, result in enumerate(sorted_results[:10], 1):\n",
    "                init_type = result.get('initialization', 'unknown')\n",
    "                if init_type == 'pretrained':\n",
    "                    init_display = \"Pretrained\"\n",
    "                elif init_type == 'pretrained_alt':\n",
    "                    init_display = \"Alt-Pre\"\n",
    "                elif init_type == 'random':\n",
    "                    init_display = \"Random\"\n",
    "                elif init_type == 'random_alt':\n",
    "                    init_display = \"Alt-Rand\"\n",
    "                else:\n",
    "                    init_display = \"Unknown\"\n",
    "                \n",
    "                f.write(f\"{i:<4} {result['name']:<30} {result['strategy']:<10} {init_display:<12} {result['val_accuracy']:<7.1f} {result['test_accuracy']:<7.1f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            if sorted_results:\n",
    "                best_overall = sorted_results[0]\n",
    "                breakthrough_count = sum(1 for r in sorted_results if r['test_accuracy'] > 54.2)\n",
    "                excellent_count = sum(1 for r in sorted_results if r['test_accuracy'] >= 65.0)\n",
    "                \n",
    "                f.write(f\"SUMMARY STATISTICS\\n\")\n",
    "                f.write(\"-\"*50 + \"\\n\")\n",
    "                f.write(f\"Current Best: {best_overall['name']} ({best_overall['test_accuracy']:.1f}%)\\n\")\n",
    "                f.write(f\"Models beating 54.2% baseline: {breakthrough_count}/{len(sorted_results)}\\n\")\n",
    "                f.write(f\"Models achieving 65%+: {excellent_count}\\n\")\n",
    "                f.write(f\"Total models tested: {len(sorted_results)}\\n\")\n",
    "                \n",
    "                # Analysis by initialization type\n",
    "                f.write(f\"\\nBY INITIALIZATION TYPE:\\n\")\n",
    "                init_groups = {}\n",
    "                for result in sorted_results:\n",
    "                    init_type = result.get('initialization', 'unknown')\n",
    "                    if init_type not in init_groups:\n",
    "                        init_groups[init_type] = []\n",
    "                    init_groups[init_type].append(result)\n",
    "                \n",
    "                for init_type, group in init_groups.items():\n",
    "                    avg_test = sum(r['test_accuracy'] for r in group) / len(group)\n",
    "                    best_test = max(r['test_accuracy'] for r in group)\n",
    "                    f.write(f\"   {init_type:15}: {len(group):2d} models, avg: {avg_test:.1f}%, best: {best_test:.1f}%\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"REMAINING: {total_count - completed_count} architectures to test\\n\")\n",
    "            estimated_time_remaining = (total_time_so_far / completed_count) * (total_count - completed_count) if completed_count > 0 else 0\n",
    "            f.write(f\"Estimated time remaining: {estimated_time_remaining/3600:.1f} hours\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"This file: {filename}\\n\")\n",
    "            f.write(f\"Auto-saved at: {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
    "        \n",
    "        print(f\"      Progress saved: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def save_backup_files(self, completed_count):\n",
    "        \"\"\"Save JSON/pickle backup files after each architecture\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save as JSON\n",
    "        json_file = f'deer_aging_backup_{completed_count}_{timestamp}.json'\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(self.all_results, f, indent=2)\n",
    "        \n",
    "        # Save as pickle for full Python objects\n",
    "        pkl_file = f'deer_aging_backup_{completed_count}_{timestamp}.pkl'\n",
    "        with open(pkl_file, 'wb') as f:\n",
    "            pickle.dump(self.all_results, f)\n",
    "        \n",
    "        return json_file, pkl_file\n",
    "    \n",
    "    def save_final_results(self, total_time):\n",
    "        \"\"\"Save comprehensive final results\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Prepare comprehensive results\n",
    "        final_data = {\n",
    "            'experiment_info': {\n",
    "                'timestamp': timestamp,\n",
    "                'total_runtime_hours': total_time / 3600,\n",
    "                'total_models_tested': len(self.all_results),\n",
    "                'device': str(self.device),\n",
    "                'num_classes': self.num_classes\n",
    "            },\n",
    "            'results': self.all_results,\n",
    "            'leaderboard': sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        }\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open(f'deer_aging_final_results_{timestamp}.json', 'w') as f:\n",
    "            json.dump(final_data, f, indent=2)\n",
    "        \n",
    "        # Save as pickle\n",
    "        with open(f'deer_aging_final_results_{timestamp}.pkl', 'wb') as f:\n",
    "            pickle.dump(final_data, f)\n",
    "        \n",
    "        print(f\"\\nRESULTS SAVED:\")\n",
    "        print(f\"   deer_aging_final_results_{timestamp}.json\")\n",
    "        print(f\"   deer_aging_final_results_{timestamp}.pkl\")\n",
    "    \n",
    "    def show_intermediate_leaderboard(self, completed_count):\n",
    "        \"\"\"Show intermediate leaderboard\"\"\"\n",
    "        current_best = sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)[:5]\n",
    "        print(f\"\\nCURRENT TOP 5 (after {completed_count} architectures):\")\n",
    "        for j, result in enumerate(current_best, 1):\n",
    "            print(f\"   {j}. {result['name']}: {result['test_accuracy']:.1f}%\")\n",
    "        print()\n",
    "    \n",
    "    def show_final_leaderboard(self, total_time):\n",
    "        \"\"\"Show comprehensive final leaderboard\"\"\"\n",
    "        # Sort all results\n",
    "        sorted_results = sorted(self.all_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        \n",
    "        print(f\"\\nFINAL COMPREHENSIVE RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total testing time: {total_time/3600:.1f} hours\")\n",
    "        print(f\"Models tested: {len(self.all_results)}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Rank':<4} {'Model':<30} {'Strategy':<10} {'Init':<12} {'Val%':<8} {'Test%':<8} {'Status'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(sorted_results, 1):\n",
    "            val_acc = result['val_accuracy']\n",
    "            test_acc = result['test_accuracy']\n",
    "            strategy = result['strategy']\n",
    "            init_type = result.get('initialization', 'unknown')\n",
    "            \n",
    "            # Format initialization type for display\n",
    "            if init_type == 'pretrained':\n",
    "                init_display = \"Pretrained\"\n",
    "            elif init_type == 'pretrained_alt':\n",
    "                init_display = \"Alt-Pre\"\n",
    "            elif init_type == 'random':\n",
    "                init_display = \"Random\"\n",
    "            elif init_type == 'random_alt':\n",
    "                init_display = \"Alt-Rand\"\n",
    "            else:\n",
    "                init_display = \"Unknown\"\n",
    "            \n",
    "            if test_acc >= 75.0:\n",
    "                status = \"BREAKTHROUGH!\"\n",
    "            elif test_acc >= 65.0:\n",
    "                status = \"EXCELLENT!\"\n",
    "            elif test_acc > 54.2:\n",
    "                status = \"NEW BEST!\"\n",
    "            elif test_acc > 45.0:\n",
    "                status = \"Good\"\n",
    "            else:\n",
    "                status = \"Weak\"\n",
    "            \n",
    "            print(f\"{i:<4} {result['name']:<30} {strategy:<10} {init_display:<12} {val_acc:<7.1f} {test_acc:<7.1f} {status}\")\n",
    "        \n",
    "        # Additional analysis by initialization type\n",
    "        print(f\"\\nANALYSIS BY INITIALIZATION:\")\n",
    "        init_groups = {}\n",
    "        for result in sorted_results:\n",
    "            init_type = result.get('initialization', 'unknown')\n",
    "            if init_type not in init_groups:\n",
    "                init_groups[init_type] = []\n",
    "            init_groups[init_type].append(result)\n",
    "        \n",
    "        for init_type, group in init_groups.items():\n",
    "            avg_test = sum(r['test_accuracy'] for r in group) / len(group)\n",
    "            best_test = max(r['test_accuracy'] for r in group)\n",
    "            print(f\"   {init_type:15}: {len(group):2d} models, avg: {avg_test:.1f}%, best: {best_test:.1f}%\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        if sorted_results:\n",
    "            best = sorted_results[0]\n",
    "            breakthrough_count = sum(1 for r in sorted_results if r['test_accuracy'] > 54.2)\n",
    "            excellent_count = sum(1 for r in sorted_results if r['test_accuracy'] >= 65.0)\n",
    "            \n",
    "            print(f\"\\nFINAL SUMMARY:\")\n",
    "            print(f\"   ULTIMATE CHAMPION: {best['name']} ({best['test_accuracy']:.1f}%)\")\n",
    "            print(f\"   Beat 54.2% baseline: {breakthrough_count}/{len(sorted_results)} models\")\n",
    "            print(f\"   Achieved 65%+: {excellent_count} models\")\n",
    "            \n",
    "            if best['test_accuracy'] >= 75.0:\n",
    "                print(f\"   MISSION ACCOMPLISHED! Achieved 75%+ accuracy!\")\n",
    "            elif best['test_accuracy'] >= 65.0:\n",
    "                print(f\"   EXCELLENT! Found 65%+ architecture!\")\n",
    "            elif best['test_accuracy'] > 54.2:\n",
    "                improvement = best['test_accuracy'] - 54.2\n",
    "                print(f\"   SUCCESS! Improved by +{improvement:.1f}% over baseline!\")\n",
    "            \n",
    "            # Analysis of initialization types\n",
    "            best_pretrained = max([r for r in sorted_results if r.get('initialization', '').startswith('pretrained')], \n",
    "                                key=lambda x: x['test_accuracy'], default=None)\n",
    "            best_random = max([r for r in sorted_results if r.get('initialization', '').startswith('random')], \n",
    "                            key=lambda x: x['test_accuracy'], default=None)\n",
    "            \n",
    "            if best_pretrained and best_random:\n",
    "                print(f\"   Best Pretrained: {best_pretrained['name']} ({best_pretrained['test_accuracy']:.1f}%)\")\n",
    "                print(f\"   Best Random Init: {best_random['name']} ({best_random['test_accuracy']:.1f}%)\")\n",
    "                if best_random['test_accuracy'] > best_pretrained['test_accuracy']:\n",
    "                    print(f\"   SURPRISE! Random initialization outperformed pretrained!\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "\n",
    "def run_complete_deer_aging_pipeline():\n",
    "    \"\"\"Run the complete deer aging pipeline from start to finish\"\"\"\n",
    "    print(\"LAUNCHING COMPLETE DEER AGING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"PIPELINE STEPS:\")\n",
    "    print(\"   1. Load original 357 images\")\n",
    "    print(\"   2. Create train/val/test splits\")\n",
    "    print(\"   3. Balance and augment training data\")\n",
    "    print(\"   4. Test all architectures (starting from EfficientNet-B5)\")\n",
    "    print(\"   5. Save results and create leaderboard\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"CRASH RECOVERY: Progress saved after each model!\")\n",
    "    print(\"Look for 'deer_aging_progress_*.txt' files for latest results\")\n",
    "    print(\"JSON/pickle backups: 'deer_aging_backup_*.json/.pkl'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        images, ages = load_original_data()\n",
    "        \n",
    "        # Step 2: Create splits\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, label_mapping, reverse_mapping = create_train_val_test_split(images, ages)\n",
    "        \n",
    "        # Step 3: Augment data\n",
    "        X_train_aug, y_train_aug = balance_and_augment_data(X_train, y_train, augment_multiplier=30, num_classes=len(label_mapping))\n",
    "        \n",
    "        # Step 4: Run complete testing\n",
    "        trainer = CompleteDeerAgeTrainer(num_classes=len(label_mapping))\n",
    "        results = trainer.run_complete_pipeline(X_train_aug, y_train_aug, X_val, y_val, X_test, y_test)\n",
    "        \n",
    "        print(\"\\nPIPELINE COMPLETE!\")\n",
    "        print(\"All results saved with timestamps\")\n",
    "        print(\"Check the final leaderboard above\")\n",
    "        \n",
    "        return results, label_mapping, reverse_mapping\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nINTERRUPTED BY USER\")\n",
    "        print(\"Check latest 'deer_aging_progress_*.txt' file for current results\")\n",
    "        print(\"Backup files saved as 'deer_aging_backup_*.json/.pkl'\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"\\nPIPELINE CRASHED: {str(e)}\")\n",
    "        print(\"Check latest 'deer_aging_progress_*.txt' file for results up to crash\")\n",
    "        print(\"Backup files saved as 'deer_aging_backup_*.json/.pkl'\")\n",
    "        print(\"You can manually load the pickle files to recover results\")\n",
    "        raise\n",
    "\n",
    "# DEBUG FUNCTION\n",
    "def test_data_loading():\n",
    "    \"\"\"Test function to debug data loading issues\"\"\"\n",
    "    print(\"TESTING DATA LOADING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    import glob\n",
    "    import os\n",
    "    \n",
    "    # Test multiple file paths\n",
    "    test_paths = [\n",
    "        \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    \n",
    "    for fpath in test_paths:\n",
    "        print(f\"\\nTesting path: {fpath}\")\n",
    "        files = glob.glob(fpath)\n",
    "        print(f\"   Files found: {len(files)}\")\n",
    "        \n",
    "        if len(files) > 0:\n",
    "            print(f\"   First few files: {files[:3]}\")\n",
    "            print(f\"   This path works!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"   No files found at this path\")\n",
    "      \n",
    "    # Test import\n",
    "    try:\n",
    "        from buck.analysis.basics import ingest_images\n",
    "        print(\"\\nSuccessfully imported buck.analysis.basics\")\n",
    "    except ImportError as e:\n",
    "        print(f\"\\nCannot import buck.analysis.basics: {e}\")\n",
    "        print(\"Possible solutions:\")\n",
    "        print(\"   1. Make sure you're in the right directory\")\n",
    "        print(\"   2. Add buck to Python path: sys.path.append('/path/to/buck')\")\n",
    "        print(\"   3. Install buck package if it's a separate package\")\n",
    "        return False\n",
    "    \n",
    "    # Test the function that worked\n",
    "    for fpath in test_paths:\n",
    "        files = glob.glob(fpath)\n",
    "        if len(files) > 0:\n",
    "            try:\n",
    "                print(f\"\\nTesting ingest_images with: {fpath}\")\n",
    "                images, ages = ingest_images(fpath)\n",
    "                print(f\"ingest_images returned: {len(images)} images, {len(ages)} ages\")\n",
    "                if len(ages) > 0:\n",
    "                    print(f\"Sample ages: {ages[:5]}\")\n",
    "                    print(f\"Age range: {min(ages)} to {max(ages)}\")\n",
    "                    print(f\"Unique ages: {sorted(set(ages))}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"ingest_images failed: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return False\n",
    "\n",
    "# CRASH RECOVERY FUNCTION\n",
    "def load_latest_results():\n",
    "    \"\"\"Load the most recent backup results (for crash recovery)\"\"\"\n",
    "    import glob\n",
    "    import os\n",
    "    \n",
    "    print(\"CRASH RECOVERY MODE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Find latest backup files\n",
    "    pickle_files = glob.glob('deer_aging_backup_*.pkl')\n",
    "    if not pickle_files:\n",
    "        print(\"No backup files found!\")\n",
    "        return None\n",
    "    \n",
    "    # Get the most recent file\n",
    "    latest_pickle = max(pickle_files, key=os.path.getctime)\n",
    "    print(f\"Loading latest backup: {latest_pickle}\")\n",
    "    \n",
    "    # Load the results\n",
    "    with open(latest_pickle, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(results)} results\")\n",
    "    \n",
    "    # Show quick summary\n",
    "    if results:\n",
    "        sorted_results = sorted(results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        print(f\"Best model: {sorted_results[0]['name']} ({sorted_results[0]['test_accuracy']:.1f}%)\")\n",
    "        print(f\"Models tested: {len(results)}\")\n",
    "        \n",
    "        # Show latest progress file\n",
    "        progress_files = glob.glob('deer_aging_progress_*.txt')\n",
    "        if progress_files:\n",
    "            latest_progress = max(progress_files, key=os.path.getctime)\n",
    "            print(f\"Latest progress report: {latest_progress}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# RUN COMPLETE PIPELINE\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"LAUNCHING COMPLETE DEER AGING PIPELINE...\")\n",
    "    print(\"Starting from EfficientNet-B5 (as requested)\")\n",
    "    print(\"TESTING ALL MODELS: Pretrained -> Alternatives -> Random Init\")\n",
    "    print(\"All results will be automatically saved\")\n",
    "    print(\"Will show which models used pretrained vs random initialization\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"CRASH RECOVERY TIPS:\")\n",
    "    print(\"   Progress files: 'deer_aging_progress_YYYYMMDD_HHMMSS.txt'\")\n",
    "    print(\"   Backup files: 'deer_aging_backup_*.pkl'\") \n",
    "    print(\"   To recover: results = load_latest_results()\")\n",
    "    print(\"   To test data loading: test_data_loading()\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test data loading first\n",
    "    print(\"Testing data loading before starting pipeline...\")\n",
    "    if not test_data_loading():\n",
    "        print(\"\\nDATA LOADING FAILED!\")\n",
    "        print(\"Please fix the data loading issue before running the pipeline\")\n",
    "        print(\"Run test_data_loading() to debug the issue\")\n",
    "    else:\n",
    "        print(\"Data loading test passed!\")\n",
    "        print(\"\\nStarting main pipeline...\")\n",
    "        final_results, final_label_mapping, final_reverse_mapping = run_complete_deer_aging_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e0bfa-336e-4d66-843d-108f9bb85e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
