{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a5b6e2-ade6-40ef-9121-92a94d2a855e",
   "metadata": {},
   "source": [
    "## What changed?\n",
    "\n",
    "This notebook takes the output result of `250813_nda_all` and attempts to optimize a single model instead of an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62247f7c-827d-4f33-99dd-bc10db9c0700",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 2060\n",
      "GPU memory: 6.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not detected by PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccd28f-ace7-4023-989c-f597892f9d2b",
   "metadata": {},
   "source": [
    "### Initial color/grayscale exploration\n",
    "\n",
    "(ghostnet_100 wins, Val 84.2%, Test 72.9%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4928b-d228-4d97-b29e-b0b0abf0737e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Optimized Deer Age Prediction Model\n",
      "==================================================\n",
      "Loading color images...\n",
      "Loaded 199 color images\n",
      "Loading grayscale images...\n",
      "Loaded 37 grayscale images\n",
      "Total images: 236\n",
      "Final dataset: 236 images\n",
      "Age distribution: {2.5: 40, 3.5: 50, 4.5: 56, 5.5: 58, 1.5: 32}\n",
      "Source distribution: {'color': 199, 'grayscale': 37}\n",
      "\n",
      "Classes: 5\n",
      "Label mapping: {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
      "\n",
      "Data split:\n",
      "Train: 150 images\n",
      "Val: 38 images\n",
      "Test: 48 images\n",
      "\n",
      "Creating balanced training set...\n",
      "\n",
      "Original class distribution:\n",
      "  Class 0: 20 images\n",
      "  Class 1: 25 images\n",
      "  Class 2: 32 images\n",
      "  Class 3: 36 images\n",
      "  Class 4: 37 images\n",
      "\n",
      "Target samples per class: 1000\n",
      "  Class 0: 20 original + 980 augmented = 1000 total\n",
      "  Class 1: 25 original + 975 augmented = 1000 total\n",
      "  Class 2: 32 original + 968 augmented = 1000 total\n",
      "  Class 3: 36 original + 964 augmented = 1000 total\n",
      "  Class 4: 37 original + 963 augmented = 1000 total\n",
      "\n",
      "Final balanced class distribution:\n",
      "  Class 0: 1000 images\n",
      "  Class 1: 1000 images\n",
      "  Class 2: 1000 images\n",
      "  Class 3: 1000 images\n",
      "  Class 4: 1000 images\n",
      "Total training images after balancing: 5000\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Testing 15 diverse architectural families...\n",
      "[ 1/15] Testing efficientnet_b1...\n",
      "Training efficientnet_b1...\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 31 (patience reached)\n",
      "  efficientnet_b1 best validation: 73.7%\n",
      "  ‚úì efficientnet_b1: Val 73.7%, Test 70.8%\n",
      "\n",
      "[ 2/15] Testing resnet34...\n",
      "Training resnet34...\n",
      "    Epoch 20: Train 100.0%, Val 76.3%\n",
      "    Early stopping at epoch 32 (patience reached)\n",
      "  resnet34 best validation: 78.9%\n",
      "  ‚úì resnet34: Val 78.9%, Test 64.6%\n",
      "\n",
      "[ 3/15] Testing densenet169...\n",
      "Training densenet169...\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Epoch 40: Train 100.0%, Val 73.7%\n",
      "    Epoch 60: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 61 (patience reached)\n",
      "  densenet169 best validation: 81.6%\n",
      "  ‚úì densenet169: Val 81.6%, Test 66.7%\n",
      "\n",
      "[ 4/15] Testing hrnet_w32...\n",
      "Training hrnet_w32...\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 26 (patience reached)\n",
      "  hrnet_w32 best validation: 81.6%\n",
      "  ‚úì hrnet_w32: Val 81.6%, Test 62.5%\n",
      "\n",
      "[ 5/15] Testing mobilenetv3_large_100...\n",
      "Training mobilenetv3_large_100...\n",
      "    Epoch 20: Train 100.0%, Val 78.9%\n",
      "    Early stopping at epoch 25 (patience reached)\n",
      "  mobilenetv3_large_100 best validation: 84.2%\n",
      "  ‚úì mobilenetv3_large_100: Val 84.2%, Test 68.8%\n",
      "\n",
      "[ 6/15] Testing vit_small_patch16_224...\n",
      "Training vit_small_patch16_224...\n",
      "    Epoch 20: Train 97.4%, Val 73.7%\n",
      "    Early stopping at epoch 27 (patience reached)\n",
      "  vit_small_patch16_224 best validation: 78.9%\n",
      "  ‚úì vit_small_patch16_224: Val 78.9%, Test 66.7%\n",
      "\n",
      "[ 7/15] Testing regnetx_004...\n",
      "Training regnetx_004...\n",
      "  ‚úó regnetx_004 failed: mat1 and mat2 shapes cannot be multiplied (32256x7...\n",
      "\n",
      "[ 8/15] Testing convnext_tiny...\n",
      "Training convnext_tiny...\n",
      "  ‚úó convnext_tiny failed: mat1 and mat2 shapes cannot be multiplied (64512x7...\n",
      "\n",
      "[ 9/15] Testing swin_tiny_patch4_window7_224...\n",
      "Training swin_tiny_patch4_window7_224...\n",
      "  ‚úó swin_tiny_patch4_window7_224 failed: only batches of spatial targets supported (3D tens...\n",
      "\n",
      "[10/15] Testing maxvit_tiny_tf_224...\n",
      "Training maxvit_tiny_tf_224...\n",
      "  ‚úó maxvit_tiny_tf_224 failed: mat1 and mat2 shapes cannot be multiplied (43008x7...\n",
      "\n",
      "[11/15] Testing repvgg_b1...\n",
      "Training repvgg_b1...\n",
      "  ‚úó repvgg_b1 failed: mat1 and mat2 shapes cannot be multiplied (172032x...\n",
      "\n",
      "[12/15] Testing ghostnet_100...\n",
      "Training ghostnet_100...\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 29 (patience reached)\n",
      "  ghostnet_100 best validation: 84.2%\n",
      "  ‚úì ghostnet_100: Val 84.2%, Test 72.9%\n",
      "\n",
      "[13/15] Testing mobilevit_s...\n",
      "Training mobilevit_s...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9423ec14b924accbbfce9bacfa0a0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/22.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úó mobilevit_s failed: mat1 and mat2 shapes cannot be multiplied (53760x7...\n",
      "\n",
      "[14/15] Testing resnext50_32x4d...\n",
      "Training resnext50_32x4d...\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 25 (patience reached)\n",
      "  resnext50_32x4d best validation: 73.7%\n",
      "  ‚úì resnext50_32x4d: Val 73.7%, Test 60.4%\n",
      "\n",
      "[15/15] Testing seresnet50...\n",
      "Training seresnet50...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 15 different architectural families for comprehensive testing\n",
    "TOP_ARCHITECTURES = [\n",
    "    'efficientnet_b1',           # 1. EfficientNet (Google)\n",
    "    'resnet34',                  # 2. ResNet (Microsoft Research)\n",
    "    'densenet169',               # 3. DenseNet (Cornell/Tsinghua)\n",
    "    'hrnet_w32',                 # 4. HRNet (Microsoft Research)\n",
    "    'mobilenetv3_large_100',     # 5. MobileNet (Google)\n",
    "    'vit_small_patch16_224',     # 6. Vision Transformer (Google)\n",
    "    'regnetx_004',               # 7. RegNet (Facebook)\n",
    "    'convnext_tiny',             # 8. ConvNeXt (Facebook)\n",
    "    'swin_tiny_patch4_window7_224', # 9. Swin Transformer (Microsoft)\n",
    "    'maxvit_tiny_tf_224',        # 10. MaxViT (Google)\n",
    "    'repvgg_b1',                 # 11. RepVGG (Tsinghua)\n",
    "    'ghostnet_100',              # 12. GhostNet (Huawei)\n",
    "    'mobilevit_s',               # 13. MobileViT (Apple)\n",
    "    'resnext50_32x4d',           # 14. ResNeXt (Facebook)\n",
    "    'seresnet50'                 # 15. SENet (WMW)\n",
    "]\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUGMENTATION_TARGET = 1000\n",
    "BATCH_SIZE = 12  # RTX 2060 friendly\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    \"\"\"Detect if image is grayscale and convert to 3-channel RGB\"\"\"\n",
    "    if len(image.shape) == 2:  # Grayscale\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:  # Single channel\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:  # Already RGB\n",
    "            return image\n",
    "        elif image.shape[2] == 4:  # RGBA\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    \"\"\"Load data from both color and grayscale folders\"\"\"\n",
    "    color_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "    gray_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*_NDA.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []  # Track if image came from color or grayscale\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    # Group ages\n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    # Filter classes with enough samples\n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    print(f\"Source distribution: {dict(Counter(filtered_sources))}\")\n",
    "    \n",
    "    return np.array(filtered_images), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image(image):\n",
    "    \"\"\"Enhanced augmentation for deer images with strategic color conversion\"\"\"\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Rotation\n",
    "    if random.random() < 0.7:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Strategic color conversion (RGB -> Grayscale -> RGB)\n",
    "    # Based on ensemble results showing grayscale superiority\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.4:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Brightness/contrast\n",
    "    if random.random() < 0.8:\n",
    "        alpha = random.uniform(0.7, 1.3)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Gamma correction\n",
    "    if random.random() < 0.4:\n",
    "        gamma = random.uniform(0.8, 1.2)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    # Noise\n",
    "    if random.random() < 0.3:\n",
    "        noise = np.random.normal(0, 7, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_balanced_dataset(X, y):\n",
    "    \"\"\"Create balanced dataset through augmentation\"\"\"\n",
    "    print(f\"\\nOriginal class distribution:\")\n",
    "    class_counts = Counter(y)\n",
    "    for class_idx, count in sorted(class_counts.items()):\n",
    "        print(f\"  Class {class_idx}: {count} images\")\n",
    "    \n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max(AUGMENTATION_TARGET, max_count)\n",
    "    \n",
    "    print(f\"\\nTarget samples per class: {target_count}\")\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for class_idx in range(len(set(y))):\n",
    "        class_mask = np.array(y) == class_idx\n",
    "        class_images = X[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        if current_count == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add originals\n",
    "        X_balanced.extend(class_images)\n",
    "        y_balanced.extend([class_idx] * current_count)\n",
    "        \n",
    "        # Add augmented to reach target\n",
    "        needed = target_count - current_count\n",
    "        augmented_for_class = 0\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image(class_images[orig_idx].copy())\n",
    "            X_balanced.append(aug_img)\n",
    "            y_balanced.append(class_idx)\n",
    "            augmented_for_class += 1\n",
    "        \n",
    "        print(f\"  Class {class_idx}: {current_count} original + {augmented_for_class} augmented = {current_count + augmented_for_class} total\")\n",
    "    \n",
    "    # Verify final balance\n",
    "    final_counts = Counter(y_balanced)\n",
    "    print(f\"\\nFinal balanced class distribution:\")\n",
    "    for class_idx, count in sorted(final_counts.items()):\n",
    "        print(f\"  Class {class_idx}: {count} images\")\n",
    "    \n",
    "    print(f\"Total training images after balancing: {len(X_balanced)}\")\n",
    "    \n",
    "    return np.array(X_balanced), np.array(y_balanced)\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    def __init__(self, X, y, training=True):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.training = training\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Ensure CHW format\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Test time augmentation for validation\n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class SingleModelTrainer:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"single_model_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    def create_model(self, architecture):\n",
    "        \"\"\"Create model with optimized head for diverse architectures\"\"\"\n",
    "        model = timm.create_model(architecture, pretrained=True, num_classes=self.num_classes)\n",
    "        \n",
    "        # Architecture-specific layer freezing\n",
    "        if any(arch in architecture for arch in ['resnet', 'resnext', 'seresnet']):\n",
    "            frozen_layers = ['conv1', 'bn1', 'layer1', 'layer2']\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in frozen_layers):\n",
    "                    param.requires_grad = False\n",
    "        elif 'efficientnet' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(block in name for block in ['blocks.0', 'blocks.1', 'blocks.2']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'densenet' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['features.conv0', 'features.norm0', 'features.denseblock1']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'hrnet' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['conv1', 'bn1', 'stage1']):\n",
    "                    param.requires_grad = False\n",
    "        elif any(arch in architecture for arch in ['mobilenet', 'ghostnet']):\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['features.0', 'features.1', 'features.2']):\n",
    "                    param.requires_grad = False\n",
    "        elif any(arch in architecture for arch in ['vit', 'swin', 'mobilevit']):\n",
    "            # Freeze patch embedding and early transformer blocks\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['patch_embed', 'blocks.0', 'blocks.1', 'layers.0']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'regnet' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['stem', 's1']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'convnext' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['stem', 'stages.0']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'maxvit' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['stem', 'stages.0']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'repvgg' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['stage0', 'stage1']):\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # Replace classifier head based on architecture\n",
    "        if hasattr(model, 'fc'):\n",
    "            in_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        elif hasattr(model, 'classifier'):\n",
    "            if hasattr(model.classifier, 'in_features'):\n",
    "                in_features = model.classifier.in_features\n",
    "            else:\n",
    "                in_features = model.classifier[-1].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        elif hasattr(model, 'head'):\n",
    "            if hasattr(model.head, 'in_features'):\n",
    "                in_features = model.head.in_features\n",
    "            else:\n",
    "                in_features = model.head[-1].in_features if hasattr(model.head, '__getitem__') else 512\n",
    "            model.head = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def train_single_architecture(self, train_loader, val_loader, architecture):\n",
    "        \"\"\"Train a single architecture\"\"\"\n",
    "        print(f\"Training {architecture}...\")\n",
    "        \n",
    "        model = self.create_model(architecture)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Separate learning rates\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if any(head in name for head in ['fc', 'classifier', 'head']):\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': 0.0002},\n",
    "            {'params': classifier_params, 'lr': 0.0008}\n",
    "        ], weight_decay=0.02)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80, eta_min=1e-6)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 25  # Increased patience\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(80):  # Increased max epochs\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % 20 == 0 and epoch > 0:\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"    Early stopping at epoch {epoch} (patience reached)\")\n",
    "                break\n",
    "            \n",
    "            # Memory management for RTX 2060\n",
    "            if epoch % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load best weights\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        print(f\"  {architecture} best validation: {best_val_acc:.1f}%\")\n",
    "        return model, best_val_acc\n",
    "    \n",
    "    def find_best_architecture(self, train_loader, val_loader, test_loader):\n",
    "        \"\"\"Test all architectures and return the best\"\"\"\n",
    "        results = {}\n",
    "        failed_archs = []\n",
    "        \n",
    "        print(f\"Testing {len(TOP_ARCHITECTURES)} diverse architectural families...\")\n",
    "        for i, arch in enumerate(TOP_ARCHITECTURES, 1):\n",
    "            try:\n",
    "                print(f\"[{i:2d}/{len(TOP_ARCHITECTURES)}] Testing {arch}...\")\n",
    "                model, val_acc = self.train_single_architecture(train_loader, val_loader, arch)\n",
    "                \n",
    "                # Evaluate on test set for comparison (not selection)\n",
    "                test_acc = evaluate_model(model, test_loader, self.device)\n",
    "                \n",
    "                results[arch] = (model, val_acc, test_acc)\n",
    "                print(f\"  ‚úì {arch}: Val {val_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "                print()  # Add blank line between architectures\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó {arch} failed: {str(e)[:50]}...\")\n",
    "                print()  # Add blank line for failed architectures too\n",
    "                failed_archs.append(arch)\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "        \n",
    "        if not results:\n",
    "            raise ValueError(\"All architectures failed to train!\")\n",
    "        \n",
    "        # Find best based on VALIDATION (not test) to avoid contamination\n",
    "        best_arch = max(results.keys(), key=lambda x: results[x][1])\n",
    "        best_model, best_val_acc, best_test_acc = results[best_arch]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ARCHITECTURE COMPARISON RESULTS\")\n",
    "        print('='*60)\n",
    "        print(f\"{'Rank':<4} {'Architecture':<30} {'Validation':<12} {'Test':<8}\")\n",
    "        print('-'*60)\n",
    "        \n",
    "        # Sort by validation performance for ranking\n",
    "        sorted_results = sorted(results.items(), key=lambda x: x[1][1], reverse=True)\n",
    "        for i, (arch, (_, val_acc, test_acc)) in enumerate(sorted_results, 1):\n",
    "            marker = \"üèÜ\" if arch == best_arch else \"  \"\n",
    "            print(f\"{i:2d}. {marker} {arch:<28} {val_acc:5.1f}%      {test_acc:5.1f}%\")\n",
    "        \n",
    "        if failed_archs:\n",
    "            print(f\"\\nFailed architectures ({len(failed_archs)}): {', '.join(failed_archs)}\")\n",
    "        \n",
    "        print(f\"\\nüèÜ WINNER: {best_arch} (Val: {best_val_acc:.1f}%, Test: {best_test_acc:.1f}%)\")\n",
    "        print(\"Note: Selection based on validation performance to avoid test contamination\")\n",
    "        \n",
    "        return best_model, best_arch, best_val_acc\n",
    "    \n",
    "    def final_optimization(self, model, train_loader, val_loader, architecture):\n",
    "        \"\"\"Final optimization of the best model\"\"\"\n",
    "        print(f\"\\nFinal optimization of {architecture}...\")\n",
    "        \n",
    "        # Unfreeze more layers for fine-tuning\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Lower learning rate for fine-tuning\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-7)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 15\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    # Test time augmentation\n",
    "                    outputs1 = model(images)\n",
    "                    flipped = torch.flip(images, [3])\n",
    "                    outputs2 = model(flipped)\n",
    "                    outputs = (outputs1 + outputs2) / 2\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"  Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        print(f\"  Final optimization complete: {best_val_acc:.1f}%\")\n",
    "        return model, best_val_acc\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Test time augmentation\n",
    "            outputs1 = model(images)\n",
    "            flipped = torch.flip(images, [3])\n",
    "            outputs2 = model(flipped)\n",
    "            outputs = (outputs1 + outputs2) / 2\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * test_correct / test_total\n",
    "    return test_acc\n",
    "\n",
    "def main():\n",
    "    print(\"Single Optimized Deer Age Prediction Model\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load combined data\n",
    "    images, ages, sources = load_combined_data()\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    y_indices = np.array([label_mapping[age] for age in ages])\n",
    "    \n",
    "    print(f\"\\nClasses: {len(unique_ages)}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images, y_indices, test_size=0.2, random_state=42, stratify=y_indices\n",
    "    )\n",
    "    \n",
    "    # Further split training into train/val\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"Train: {len(X_train_final)} images\")\n",
    "    print(f\"Val: {len(X_val)} images\") \n",
    "    print(f\"Test: {len(X_test)} images\")\n",
    "    \n",
    "    # Create balanced training set\n",
    "    print(\"\\nCreating balanced training set...\")\n",
    "    X_train_balanced, y_train_balanced = create_balanced_dataset(X_train_final, y_train_final)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DeerDataset(X_train_balanced, y_train_balanced, training=True)\n",
    "    val_dataset = DeerDataset(X_val, y_val, training=False)\n",
    "    test_dataset = DeerDataset(X_test, y_test, training=False)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = SingleModelTrainer(num_classes=len(unique_ages))\n",
    "    \n",
    "    # Find best architecture\n",
    "    best_model, best_arch, val_acc = trainer.find_best_architecture(train_loader, val_loader, test_loader)\n",
    "    \n",
    "    # Final optimization\n",
    "    optimized_model, final_val_acc = trainer.final_optimization(best_model, train_loader, val_loader, best_arch)\n",
    "    \n",
    "    # Test evaluation\n",
    "    test_acc = evaluate_model(optimized_model, test_loader, trainer.device)\n",
    "    \n",
    "    # Save final model\n",
    "    save_path = os.path.join(trainer.save_dir, f\"deer_age_model_{best_arch}_{test_acc:.1f}pct.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': optimized_model.state_dict(),\n",
    "        'architecture': best_arch,\n",
    "        'num_classes': len(unique_ages),\n",
    "        'label_mapping': label_mapping,\n",
    "        'test_accuracy': test_acc,\n",
    "        'val_accuracy': final_val_acc,\n",
    "        'input_size': IMAGE_SIZE\n",
    "    }, save_path)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Best architecture: {best_arch}\")\n",
    "    print(f\"Validation accuracy: {final_val_acc:.1f}%\")\n",
    "    print(f\"Test accuracy: {test_acc:.1f}%\")\n",
    "    print(f\"Training time: {elapsed:.1f} minutes\")\n",
    "    print(f\"Model saved: {save_path}\")\n",
    "    \n",
    "    return optimized_model, best_arch, test_acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, architecture, accuracy = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1083fe-4efc-4cf3-874a-b0056cb1eafa",
   "metadata": {},
   "source": [
    "### Fine-tuning hyperparameters of ghostnet_100\n",
    "\n",
    "- original: Val 84.2%, Test 72.9%\n",
    "- tuned: Val 84.2%, Test 81.2%\n",
    "  ```\n",
    "  [23/30] Testing combination 23\n",
    "  Optimizer: adamw, LR: 0.0001/0.0005\n",
    "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
    "    Epoch 20: Train 100.0%, Val 78.9%\n",
    "    Early stopping at epoch 25\n",
    "  ‚úì Val: 84.2%, Test: 81.2%\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb90355-602e-495d-a537-d567709b2de2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GhostNet Hyperparameter Search Space\n",
    "HYPERPARAMETER_GRID = {\n",
    "    'backbone_lr': [0.0001, 0.0003, 0.0005],\n",
    "    'classifier_lr': [0.0005, 0.001, 0.002],\n",
    "    'weight_decay': [0.01, 0.03, 0.05],\n",
    "    'batch_size': [8, 12, 16],\n",
    "    'optimizer': ['adamw', 'sgd'],\n",
    "    'scheduler': ['cosine', 'plateau'],\n",
    "    'dropout': [0.2, 0.3, 0.4, 0.5],\n",
    "    'label_smoothing': [0.05, 0.1, 0.15],\n",
    "    'augmentation_strength': ['light', 'medium', 'heavy'],\n",
    "    'freeze_layers': [2, 3, 4]  # How many early block groups to freeze\n",
    "}\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUGMENTATION_TARGET = 1000\n",
    "MAX_COMBINATIONS = 30  # Test 30 combinations\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    \"\"\"Detect if image is grayscale and convert to 3-channel RGB\"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    \"\"\"Load data from both color and grayscale folders\"\"\"\n",
    "    color_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "    gray_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*_NDA.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    # Group ages\n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    # Filter classes with enough samples\n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    \n",
    "    return np.array(filtered_images), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image(image, strength='medium'):\n",
    "    \"\"\"Enhanced augmentation with variable strength\"\"\"\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Set probabilities based on strength\n",
    "    if strength == 'light':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.5, 0.3, 0.6, 0.2, 0.1\n",
    "        rot_range, bright_range = 10, (0.8, 1.2)\n",
    "    elif strength == 'medium':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.7, 0.5, 0.8, 0.4, 0.3\n",
    "        rot_range, bright_range = 15, (0.7, 1.3)\n",
    "    else:  # heavy\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.8, 0.6, 0.9, 0.5, 0.4\n",
    "        rot_range, bright_range = 20, (0.6, 1.4)\n",
    "    \n",
    "    # Rotation\n",
    "    if random.random() < rot_prob:\n",
    "        angle = random.uniform(-rot_range, rot_range)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Horizontal flip\n",
    "    if random.random() < flip_prob:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Strategic color conversion (RGB -> Grayscale -> RGB)\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.4:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Brightness/contrast\n",
    "    if random.random() < bright_prob:\n",
    "        alpha = random.uniform(*bright_range)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Gamma correction\n",
    "    if random.random() < gamma_prob:\n",
    "        gamma = random.uniform(0.8, 1.2)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    # Noise\n",
    "    if random.random() < noise_prob:\n",
    "        noise = np.random.normal(0, 7, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_balanced_dataset(X, y, aug_strength='medium'):\n",
    "    \"\"\"Create balanced dataset through augmentation\"\"\"\n",
    "    class_counts = Counter(y)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max(AUGMENTATION_TARGET, max_count)\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for class_idx in range(len(set(y))):\n",
    "        class_mask = np.array(y) == class_idx\n",
    "        class_images = X[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        if current_count == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add originals\n",
    "        X_balanced.extend(class_images)\n",
    "        y_balanced.extend([class_idx] * current_count)\n",
    "        \n",
    "        # Add augmented to reach target\n",
    "        needed = target_count - current_count\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image(class_images[orig_idx].copy(), aug_strength)\n",
    "            X_balanced.append(aug_img)\n",
    "            y_balanced.append(class_idx)\n",
    "    \n",
    "    return np.array(X_balanced), np.array(y_balanced)\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    def __init__(self, X, y, training=True):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.training = training\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class GhostNetHyperparameterTuner:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"ghostnet_tuning_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    def create_ghostnet_model(self, dropout=0.3, freeze_layers=3):\n",
    "        \"\"\"Create GhostNet model with specified dropout and freezing\"\"\"\n",
    "        model = timm.create_model('ghostnet_100', pretrained=True, num_classes=self.num_classes)\n",
    "        \n",
    "        # Freeze early layers based on freeze_layers parameter\n",
    "        freeze_patterns = [\n",
    "            ['conv_stem'],\n",
    "            ['conv_stem', 'blocks.0'],\n",
    "            ['conv_stem', 'blocks.0', 'blocks.1'],\n",
    "            ['conv_stem', 'blocks.0', 'blocks.1', 'blocks.2']\n",
    "        ]\n",
    "        \n",
    "        if freeze_layers <= len(freeze_patterns):\n",
    "            for name, param in model.named_parameters():\n",
    "                for pattern in freeze_patterns[freeze_layers - 1]:\n",
    "                    if pattern in name:\n",
    "                        param.requires_grad = False\n",
    "                        break\n",
    "        \n",
    "        # Replace classifier with custom dropout\n",
    "        if hasattr(model, 'classifier'):\n",
    "            in_features = model.classifier.in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def get_optimizer(self, model, opt_type, backbone_lr, classifier_lr, weight_decay):\n",
    "        \"\"\"Create optimizer based on hyperparameters\"\"\"\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'classifier' in name:\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        param_groups = [\n",
    "            {'params': backbone_params, 'lr': backbone_lr},\n",
    "            {'params': classifier_params, 'lr': classifier_lr}\n",
    "        ]\n",
    "        \n",
    "        if opt_type == 'adamw':\n",
    "            return optim.AdamW(param_groups, weight_decay=weight_decay)\n",
    "        elif opt_type == 'sgd':\n",
    "            return optim.SGD(param_groups, weight_decay=weight_decay, momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {opt_type}\")\n",
    "    \n",
    "    def get_scheduler(self, optimizer, scheduler_type, max_epochs):\n",
    "        \"\"\"Create learning rate scheduler\"\"\"\n",
    "        if scheduler_type == 'cosine':\n",
    "            return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-6)\n",
    "        elif scheduler_type == 'plateau':\n",
    "            return optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, verbose=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scheduler: {scheduler_type}\")\n",
    "    \n",
    "    def train_with_hyperparams(self, train_loader, val_loader, test_loader, hyperparams, combo_num):\n",
    "        \"\"\"Train model with specific hyperparameters\"\"\"\n",
    "        model = self.create_ghostnet_model(\n",
    "            dropout=hyperparams['dropout'], \n",
    "            freeze_layers=hyperparams['freeze_layers']\n",
    "        )\n",
    "        \n",
    "        optimizer = self.get_optimizer(\n",
    "            model, hyperparams['optimizer'], \n",
    "            hyperparams['backbone_lr'], hyperparams['classifier_lr'], \n",
    "            hyperparams['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = self.get_scheduler(optimizer, hyperparams['scheduler'], 80)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=hyperparams['label_smoothing'])\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 20\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(80):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Update scheduler\n",
    "            if hyperparams['scheduler'] == 'plateau':\n",
    "                scheduler.step(val_acc)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Print progress every 20 epochs\n",
    "            if epoch % 20 == 0 and epoch > 0:\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load best weights and evaluate on test\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        # Test evaluation with TTA\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Test time augmentation\n",
    "                outputs1 = model(images)\n",
    "                flipped = torch.flip(images, [3])\n",
    "                outputs2 = model(flipped)\n",
    "                outputs = (outputs1 + outputs2) / 2\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        return model, best_val_acc, test_acc\n",
    "    \n",
    "    def generate_hyperparameter_combinations(self):\n",
    "        \"\"\"Generate smart hyperparameter combinations\"\"\"\n",
    "        # Create all possible combinations\n",
    "        keys = list(HYPERPARAMETER_GRID.keys())\n",
    "        values = list(HYPERPARAMETER_GRID.values())\n",
    "        all_combinations = list(itertools.product(*values))\n",
    "        \n",
    "        # Shuffle and limit\n",
    "        random.shuffle(all_combinations)\n",
    "        selected_combinations = all_combinations[:MAX_COMBINATIONS]\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        combinations = []\n",
    "        for combo in selected_combinations:\n",
    "            hyperparams = dict(zip(keys, combo))\n",
    "            combinations.append(hyperparams)\n",
    "        \n",
    "        return combinations\n",
    "    \n",
    "    def tune_hyperparameters(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        \"\"\"Main hyperparameter tuning loop\"\"\"\n",
    "        print(f\"Starting GhostNet hyperparameter tuning...\")\n",
    "        print(f\"Testing {MAX_COMBINATIONS} hyperparameter combinations\")\n",
    "        \n",
    "        combinations = self.generate_hyperparameter_combinations()\n",
    "        results = []\n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        for i, hyperparams in enumerate(combinations, 1):\n",
    "            print(f\"\\n[{i:2d}/{MAX_COMBINATIONS}] Testing combination {i}\")\n",
    "            print(f\"  Optimizer: {hyperparams['optimizer']}, LR: {hyperparams['backbone_lr']}/{hyperparams['classifier_lr']}\")\n",
    "            print(f\"  Batch: {hyperparams['batch_size']}, Dropout: {hyperparams['dropout']}, Freeze: {hyperparams['freeze_layers']}\")\n",
    "            \n",
    "            try:\n",
    "                # Create datasets with current augmentation strength\n",
    "                X_train_aug, y_train_aug = create_balanced_dataset(\n",
    "                    X_train, y_train, hyperparams['augmentation_strength']\n",
    "                )\n",
    "                \n",
    "                train_dataset = DeerDataset(X_train_aug, y_train_aug, training=True)\n",
    "                val_dataset = DeerDataset(X_val, y_val, training=False)\n",
    "                test_dataset = DeerDataset(X_test, y_test, training=False)\n",
    "                \n",
    "                train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True, num_workers=0)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'], shuffle=False, num_workers=0)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'], shuffle=False, num_workers=0)\n",
    "                \n",
    "                model, val_acc, test_acc = self.train_with_hyperparams(\n",
    "                    train_loader, val_loader, test_loader, hyperparams, i\n",
    "                )\n",
    "                \n",
    "                result = {\n",
    "                    'combination': i,\n",
    "                    'hyperparams': hyperparams,\n",
    "                    'val_accuracy': val_acc,\n",
    "                    'test_accuracy': test_acc\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"  ‚úì Val: {val_acc:.1f}%, Test: {test_acc:.1f}%\")\n",
    "                \n",
    "                # Save best model so far\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_path = os.path.join(self.save_dir, f\"ghostnet_best_val_{val_acc:.1f}.pth\")\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'hyperparams': hyperparams,\n",
    "                        'val_accuracy': val_acc,\n",
    "                        'test_accuracy': test_acc,\n",
    "                        'combination': i\n",
    "                    }, best_path)\n",
    "                    print(f\"  üíæ New best model saved: {val_acc:.1f}%\")\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Combination {i} failed: {str(e)[:60]}...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "        \n",
    "        # Save all results\n",
    "        results_path = os.path.join(self.save_dir, \"hyperparameter_results.json\")\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set with TTA\"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Test time augmentation\n",
    "            outputs1 = model(images)\n",
    "            flipped = torch.flip(images, [3])\n",
    "            outputs2 = model(flipped)\n",
    "            outputs = (outputs1 + outputs2) / 2\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * test_correct / test_total\n",
    "    return test_acc\n",
    "\n",
    "def main():\n",
    "    print(\"GhostNet Hyperparameter Tuning for Deer Age Prediction\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load combined data\n",
    "    images, ages, sources = load_combined_data()\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    y_indices = np.array([label_mapping[age] for age in ages])\n",
    "    \n",
    "    print(f\"\\nClasses: {len(unique_ages)}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images, y_indices, test_size=0.2, random_state=42, stratify=y_indices\n",
    "    )\n",
    "    \n",
    "    # Further split training into train/val\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"Train: {len(X_train_final)} images\")\n",
    "    print(f\"Val: {len(X_val)} images\") \n",
    "    print(f\"Test: {len(X_test)} images\")\n",
    "    \n",
    "    # Initialize tuner\n",
    "    tuner = GhostNetHyperparameterTuner(num_classes=len(unique_ages))\n",
    "    \n",
    "    # Run hyperparameter tuning\n",
    "    results = tuner.tune_hyperparameters(X_train_final, y_train_final, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    # Analyze results\n",
    "    if results:\n",
    "        # Sort by validation accuracy\n",
    "        sorted_results = sorted(results, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "        \n",
    "        elapsed = (time.time() - start_time) / 60\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"GHOSTNET HYPERPARAMETER TUNING RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Rank':<4} {'Combination':<6} {'Validation':<12} {'Test':<8} {'Key Hyperparams'}\")\n",
    "        print('-' * 75)\n",
    "        \n",
    "        for i, result in enumerate(sorted_results[:10], 1):  # Top 10\n",
    "            hp = result['hyperparams']\n",
    "            key_params = f\"opt={hp['optimizer']}, lr={hp['backbone_lr']}, bs={hp['batch_size']}, drop={hp['dropout']}\"\n",
    "            print(f\"{i:2d}. {result['combination']:4d}       {result['val_accuracy']:5.1f}%      {result['test_accuracy']:5.1f}%    {key_params}\")\n",
    "        \n",
    "        best_result = sorted_results[0]\n",
    "        print(f\"\\nüèÜ BEST HYPERPARAMETERS:\")\n",
    "        for key, value in best_result['hyperparams'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\nüìä PERFORMANCE:\")\n",
    "        print(f\"  Best Validation: {best_result['val_accuracy']:.1f}%\")\n",
    "        print(f\"  Best Test: {best_result['test_accuracy']:.1f}%\")\n",
    "        print(f\"  Tuning Time: {elapsed:.1f} minutes\")\n",
    "        print(f\"  Results saved to: {tuner.save_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No successful combinations found!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9eeae-fac6-4e5c-8470-2c2ad0050976",
   "metadata": {},
   "source": [
    "### Removing randomness from the dataset applied to each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff8203-8b4e-4c4a-9929-eb4ccd19cf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focused GhostNet Hyperparameter Tuning for Deer Age Prediction\n",
      "======================================================================\n",
      "Loading color images...\n",
      "Loaded 200 color images\n",
      "Loading grayscale images...\n",
      "Loaded 38 grayscale images\n",
      "Total images: 238\n",
      "Final dataset: 238 images\n",
      "Age distribution: {2.5: 40, 3.5: 50, 4.5: 56, 5.5: 60, 1.5: 32}\n",
      "\n",
      "Classes: 5\n",
      "Label mapping: {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
      "\n",
      "Data split:\n",
      "Train: 152 images\n",
      "Val: 38 images\n",
      "Test: 48 images\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Starting Focused GhostNet hyperparameter tuning...\n",
      "Testing 324 hyperparameter combinations (best-first strategy)\n",
      "\n",
      "[  1/324] Testing combination 1\n",
      "  Optimizer: adamw, LR: 0.0001/0.0005\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "  ‚≠ê PRIORITY COMBINATION (known good)\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 28\n",
      "  ‚úì Val: 71.1%, Test: 60.4%\n",
      "  üíæ Model saved: combo_001_val_71.1_test_60.4.pth\n",
      "  üèÜ New best validation: 71.1%\n",
      "  üéØ New best test: 60.4%\n",
      "\n",
      "[  2/324] Testing combination 2\n",
      "  Optimizer: adamw, LR: 0.0001/0.00055\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "  ‚≠ê PRIORITY COMBINATION (known good)\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 29\n",
      "  ‚úì Val: 71.1%, Test: 54.2%\n",
      "  üíæ Model saved: combo_002_val_71.1_test_54.2.pth\n",
      "\n",
      "[  3/324] Testing combination 3\n",
      "  Optimizer: adamw, LR: 0.0001/0.0005\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "  ‚≠ê PRIORITY COMBINATION (known good)\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 22\n",
      "  ‚úì Val: 71.1%, Test: 70.8%\n",
      "  üíæ Model saved: combo_003_val_71.1_test_70.8.pth\n",
      "  üéØ New best test: 70.8%\n",
      "\n",
      "[  4/324] Testing combination 4\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Epoch 40: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 41\n",
      "  ‚úì Val: 73.7%, Test: 68.8%\n",
      "  üíæ Model saved: combo_004_val_73.7_test_68.8.pth\n",
      "  üèÜ New best validation: 73.7%\n",
      "\n",
      "[  5/324] Testing combination 5\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 48\n",
      "  ‚úì Val: 76.3%, Test: 62.5%\n",
      "  üíæ Model saved: combo_005_val_76.3_test_62.5.pth\n",
      "  üèÜ New best validation: 76.3%\n",
      "\n",
      "[  6/324] Testing combination 6\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 22\n",
      "  ‚úì Val: 71.1%, Test: 66.7%\n",
      "  üíæ Model saved: combo_006_val_71.1_test_66.7.pth\n",
      "\n",
      "[  7/324] Testing combination 7\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 57.9%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Epoch 60: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 68\n",
      "  ‚úì Val: 78.9%, Test: 66.7%\n",
      "  üíæ Model saved: combo_007_val_78.9_test_66.7.pth\n",
      "  üèÜ New best validation: 78.9%\n",
      "\n",
      "[  8/324] Testing combination 8\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 23\n",
      "  ‚úì Val: 73.7%, Test: 66.7%\n",
      "  üíæ Model saved: combo_008_val_73.7_test_66.7.pth\n",
      "\n",
      "[  9/324] Testing combination 9\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 51\n",
      "  ‚úì Val: 78.9%, Test: 62.5%\n",
      "  üíæ Model saved: combo_009_val_78.9_test_62.5.pth\n",
      "\n",
      "[ 10/324] Testing combination 10\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 41\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_010_val_73.7_test_64.6.pth\n",
      "\n",
      "[ 11/324] Testing combination 11\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 23\n",
      "  ‚úì Val: 76.3%, Test: 62.5%\n",
      "  üíæ Model saved: combo_011_val_76.3_test_62.5.pth\n",
      "\n",
      "[ 12/324] Testing combination 12\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 25\n",
      "  ‚úì Val: 73.7%, Test: 68.8%\n",
      "  üíæ Model saved: combo_012_val_73.7_test_68.8.pth\n",
      "\n",
      "[ 13/324] Testing combination 13\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_013_val_73.7_test_62.5.pth\n",
      "\n",
      "[ 14/324] Testing combination 14\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 42\n",
      "  ‚úì Val: 76.3%, Test: 64.6%\n",
      "  üíæ Model saved: combo_014_val_76.3_test_64.6.pth\n",
      "\n",
      "[ 15/324] Testing combination 15\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 45\n",
      "  ‚úì Val: 81.6%, Test: 62.5%\n",
      "  üíæ Model saved: combo_015_val_81.6_test_62.5.pth\n",
      "  üèÜ New best validation: 81.6%\n",
      "\n",
      "[ 16/324] Testing combination 16\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 99.9%, Val 68.4%\n",
      "    Early stopping at epoch 30\n",
      "  ‚úì Val: 71.1%, Test: 64.6%\n",
      "  üíæ Model saved: combo_016_val_71.1_test_64.6.pth\n",
      "\n",
      "[ 17/324] Testing combination 17\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 39\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_017_val_73.7_test_62.5.pth\n",
      "\n",
      "[ 18/324] Testing combination 18\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 25\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_018_val_73.7_test_64.6.pth\n",
      "\n",
      "[ 19/324] Testing combination 19\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 25\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_019_val_71.1_test_62.5.pth\n",
      "\n",
      "[ 20/324] Testing combination 20\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 76.3%, Test: 60.4%\n",
      "  üíæ Model saved: combo_020_val_76.3_test_60.4.pth\n",
      "\n",
      "[ 21/324] Testing combination 21\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 22\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_021_val_71.1_test_62.5.pth\n",
      "\n",
      "[ 22/324] Testing combination 22\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Epoch 40: Train 100.0%, Val 60.5%\n",
      "    Early stopping at epoch 45\n",
      "  ‚úì Val: 71.1%, Test: 64.6%\n",
      "  üíæ Model saved: combo_022_val_71.1_test_64.6.pth\n",
      "\n",
      "[ 23/324] Testing combination 23\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 73.7%, Test: 66.7%\n",
      "  üíæ Model saved: combo_023_val_73.7_test_66.7.pth\n",
      "\n",
      "[ 24/324] Testing combination 24\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 53\n",
      "  ‚úì Val: 78.9%, Test: 64.6%\n",
      "  üíæ Model saved: combo_024_val_78.9_test_64.6.pth\n",
      "\n",
      "[ 25/324] Testing combination 25\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 38\n",
      "  ‚úì Val: 76.3%, Test: 64.6%\n",
      "  üíæ Model saved: combo_025_val_76.3_test_64.6.pth\n",
      "\n",
      "[ 26/324] Testing combination 26\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 76.3%, Test: 62.5%\n",
      "  üíæ Model saved: combo_026_val_76.3_test_62.5.pth\n",
      "\n",
      "[ 27/324] Testing combination 27\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 33\n",
      "  ‚úì Val: 76.3%, Test: 68.8%\n",
      "  üíæ Model saved: combo_027_val_76.3_test_68.8.pth\n",
      "\n",
      "[ 28/324] Testing combination 28\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 34\n",
      "  ‚úì Val: 76.3%, Test: 66.7%\n",
      "  üíæ Model saved: combo_028_val_76.3_test_66.7.pth\n",
      "\n",
      "[ 29/324] Testing combination 29\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 21\n",
      "  ‚úì Val: 73.7%, Test: 68.8%\n",
      "  üíæ Model saved: combo_029_val_73.7_test_68.8.pth\n",
      "\n",
      "[ 30/324] Testing combination 30\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 43\n",
      "  ‚úì Val: 76.3%, Test: 64.6%\n",
      "  üíæ Model saved: combo_030_val_76.3_test_64.6.pth\n",
      "\n",
      "[ 31/324] Testing combination 31\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 39\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_031_val_73.7_test_64.6.pth\n",
      "\n",
      "[ 32/324] Testing combination 32\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 30\n",
      "  ‚úì Val: 76.3%, Test: 64.6%\n",
      "  üíæ Model saved: combo_032_val_76.3_test_64.6.pth\n",
      "\n",
      "[ 33/324] Testing combination 33\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 23\n",
      "  ‚úì Val: 68.4%, Test: 60.4%\n",
      "  üíæ Model saved: combo_033_val_68.4_test_60.4.pth\n",
      "\n",
      "[ 34/324] Testing combination 34\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 23\n",
      "  ‚úì Val: 71.1%, Test: 64.6%\n",
      "  üíæ Model saved: combo_034_val_71.1_test_64.6.pth\n",
      "\n",
      "[ 35/324] Testing combination 35\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 26\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_035_val_73.7_test_62.5.pth\n",
      "\n",
      "[ 36/324] Testing combination 36\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 35\n",
      "  ‚úì Val: 78.9%, Test: 62.5%\n",
      "  üíæ Model saved: combo_036_val_78.9_test_62.5.pth\n",
      "\n",
      "[ 37/324] Testing combination 37\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 32\n",
      "  ‚úì Val: 71.1%, Test: 68.8%\n",
      "  üíæ Model saved: combo_037_val_71.1_test_68.8.pth\n",
      "\n",
      "[ 38/324] Testing combination 38\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 43\n",
      "  ‚úì Val: 76.3%, Test: 66.7%\n",
      "  üíæ Model saved: combo_038_val_76.3_test_66.7.pth\n",
      "\n",
      "[ 39/324] Testing combination 39\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_039_val_71.1_test_62.5.pth\n",
      "\n",
      "[ 40/324] Testing combination 40\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 23\n",
      "  ‚úì Val: 68.4%, Test: 68.8%\n",
      "  üíæ Model saved: combo_040_val_68.4_test_68.8.pth\n",
      "\n",
      "[ 41/324] Testing combination 41\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 37\n",
      "  ‚úì Val: 76.3%, Test: 62.5%\n",
      "  üíæ Model saved: combo_041_val_76.3_test_62.5.pth\n",
      "\n",
      "[ 42/324] Testing combination 42\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 39\n",
      "  ‚úì Val: 73.7%, Test: 70.8%\n",
      "  üíæ Model saved: combo_042_val_73.7_test_70.8.pth\n",
      "\n",
      "[ 43/324] Testing combination 43\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 60.5%\n",
      "    Early stopping at epoch 20\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_043_val_73.7_test_64.6.pth\n",
      "\n",
      "[ 44/324] Testing combination 44\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_044_val_71.1_test_62.5.pth\n",
      "\n",
      "[ 45/324] Testing combination 45\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 50\n",
      "  ‚úì Val: 73.7%, Test: 68.8%\n",
      "  üíæ Model saved: combo_045_val_73.7_test_68.8.pth\n",
      "\n",
      "[ 46/324] Testing combination 46\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 32\n",
      "  ‚úì Val: 73.7%, Test: 68.8%\n",
      "  üíæ Model saved: combo_046_val_73.7_test_68.8.pth\n",
      "\n",
      "[ 47/324] Testing combination 47\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_047_val_73.7_test_64.6.pth\n",
      "\n",
      "[ 48/324] Testing combination 48\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 22\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_048_val_73.7_test_64.6.pth\n",
      "\n",
      "[ 49/324] Testing combination 49\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 68.4%\n",
      "    Epoch 60: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 62\n",
      "  ‚úì Val: 76.3%, Test: 62.5%\n",
      "  üíæ Model saved: combo_049_val_76.3_test_62.5.pth\n",
      "\n",
      "[ 50/324] Testing combination 50\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 37\n",
      "  ‚úì Val: 78.9%, Test: 62.5%\n",
      "  üíæ Model saved: combo_050_val_78.9_test_62.5.pth\n",
      "\n",
      "[ 51/324] Testing combination 51\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 31\n",
      "  ‚úì Val: 76.3%, Test: 64.6%\n",
      "  üíæ Model saved: combo_051_val_76.3_test_64.6.pth\n",
      "\n",
      "[ 52/324] Testing combination 52\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Epoch 40: Train 100.0%, Val 78.9%\n",
      "    Early stopping at epoch 50\n",
      "  ‚úì Val: 78.9%, Test: 62.5%\n",
      "  üíæ Model saved: combo_052_val_78.9_test_62.5.pth\n",
      "\n",
      "[ 53/324] Testing combination 53\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 30\n",
      "  ‚úì Val: 76.3%, Test: 64.6%\n",
      "  üíæ Model saved: combo_053_val_76.3_test_64.6.pth\n",
      "\n",
      "[ 54/324] Testing combination 54\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 38\n",
      "  ‚úì Val: 76.3%, Test: 62.5%\n",
      "  üíæ Model saved: combo_054_val_76.3_test_62.5.pth\n",
      "\n",
      "[ 55/324] Testing combination 55\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 60.5%\n",
      "    Early stopping at epoch 29\n",
      "  ‚úì Val: 68.4%, Test: 60.4%\n",
      "  üíæ Model saved: combo_055_val_68.4_test_60.4.pth\n",
      "\n",
      "[ 56/324] Testing combination 56\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 36\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_056_val_73.7_test_62.5.pth\n",
      "\n",
      "[ 57/324] Testing combination 57\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 48\n",
      "  ‚úì Val: 78.9%, Test: 68.8%\n",
      "  üíæ Model saved: combo_057_val_78.9_test_68.8.pth\n",
      "\n",
      "[ 58/324] Testing combination 58\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 44\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_058_val_71.1_test_62.5.pth\n",
      "\n",
      "[ 59/324] Testing combination 59\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 60.5%\n",
      "    Early stopping at epoch 35\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_059_val_73.7_test_62.5.pth\n",
      "\n",
      "[ 60/324] Testing combination 60\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 30\n",
      "  ‚úì Val: 71.1%, Test: 66.7%\n",
      "  üíæ Model saved: combo_060_val_71.1_test_66.7.pth\n",
      "\n",
      "[ 61/324] Testing combination 61\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 37\n",
      "  ‚úì Val: 73.7%, Test: 66.7%\n",
      "  üíæ Model saved: combo_061_val_73.7_test_66.7.pth\n",
      "\n",
      "[ 62/324] Testing combination 62\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 35\n",
      "  ‚úì Val: 71.1%, Test: 64.6%\n",
      "  üíæ Model saved: combo_062_val_71.1_test_64.6.pth\n",
      "\n",
      "[ 63/324] Testing combination 63\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 78.9%, Test: 64.6%\n",
      "  üíæ Model saved: combo_063_val_78.9_test_64.6.pth\n",
      "\n",
      "[ 64/324] Testing combination 64\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 60.5%\n",
      "    Early stopping at epoch 23\n",
      "  ‚úì Val: 68.4%, Test: 66.7%\n",
      "  üíæ Model saved: combo_064_val_68.4_test_66.7.pth\n",
      "\n",
      "[ 65/324] Testing combination 65\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 76.3%, Test: 62.5%\n",
      "  üíæ Model saved: combo_065_val_76.3_test_62.5.pth\n",
      "\n",
      "[ 66/324] Testing combination 66\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 31\n",
      "  ‚úì Val: 73.7%, Test: 60.4%\n",
      "  üíæ Model saved: combo_066_val_73.7_test_60.4.pth\n",
      "\n",
      "[ 67/324] Testing combination 67\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_067_val_73.7_test_62.5.pth\n",
      "\n",
      "[ 68/324] Testing combination 68\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 99.9%, Val 65.8%\n",
      "    Early stopping at epoch 28\n",
      "  ‚úì Val: 71.1%, Test: 64.6%\n",
      "  üíæ Model saved: combo_068_val_71.1_test_64.6.pth\n",
      "\n",
      "[ 69/324] Testing combination 69\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 29\n",
      "  ‚úì Val: 76.3%, Test: 60.4%\n",
      "  üíæ Model saved: combo_069_val_76.3_test_60.4.pth\n",
      "\n",
      "[ 70/324] Testing combination 70\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 38\n",
      "  ‚úì Val: 73.7%, Test: 66.7%\n",
      "  üíæ Model saved: combo_070_val_73.7_test_66.7.pth\n",
      "\n",
      "[ 71/324] Testing combination 71\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 33\n",
      "  ‚úì Val: 71.1%, Test: 68.8%\n",
      "  üíæ Model saved: combo_071_val_71.1_test_68.8.pth\n",
      "\n",
      "[ 72/324] Testing combination 72\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 32\n",
      "  ‚úì Val: 71.1%, Test: 64.6%\n",
      "  üíæ Model saved: combo_072_val_71.1_test_64.6.pth\n",
      "\n",
      "[ 73/324] Testing combination 73\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 28\n",
      "  ‚úì Val: 73.7%, Test: 60.4%\n",
      "  üíæ Model saved: combo_073_val_73.7_test_60.4.pth\n",
      "\n",
      "[ 74/324] Testing combination 74\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 23\n",
      "  ‚úì Val: 76.3%, Test: 62.5%\n",
      "  üíæ Model saved: combo_074_val_76.3_test_62.5.pth\n",
      "\n",
      "[ 75/324] Testing combination 75\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 28\n",
      "  ‚úì Val: 73.7%, Test: 66.7%\n",
      "  üíæ Model saved: combo_075_val_73.7_test_66.7.pth\n",
      "\n",
      "[ 76/324] Testing combination 76\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 37\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_076_val_73.7_test_62.5.pth\n",
      "\n",
      "[ 77/324] Testing combination 77\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 46\n",
      "  ‚úì Val: 76.3%, Test: 66.7%\n",
      "  üíæ Model saved: combo_077_val_76.3_test_66.7.pth\n",
      "\n",
      "[ 78/324] Testing combination 78\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 76.3%, Test: 60.4%\n",
      "  üíæ Model saved: combo_078_val_76.3_test_60.4.pth\n",
      "\n",
      "[ 79/324] Testing combination 79\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 23\n",
      "  ‚úì Val: 76.3%, Test: 64.6%\n",
      "  üíæ Model saved: combo_079_val_76.3_test_64.6.pth\n",
      "\n",
      "[ 80/324] Testing combination 80\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 25\n",
      "  ‚úì Val: 73.7%, Test: 58.3%\n",
      "  üíæ Model saved: combo_080_val_73.7_test_58.3.pth\n",
      "\n",
      "[ 81/324] Testing combination 81\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 31\n",
      "  ‚úì Val: 73.7%, Test: 54.2%\n",
      "  üíæ Model saved: combo_081_val_73.7_test_54.2.pth\n",
      "\n",
      "[ 82/324] Testing combination 82\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 33\n",
      "  ‚úì Val: 76.3%, Test: 70.8%\n",
      "  üíæ Model saved: combo_082_val_76.3_test_70.8.pth\n",
      "\n",
      "[ 83/324] Testing combination 83\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 43\n",
      "  ‚úì Val: 73.7%, Test: 75.0%\n",
      "  üíæ Model saved: combo_083_val_73.7_test_75.0.pth\n",
      "  üéØ New best test: 75.0%\n",
      "\n",
      "[ 84/324] Testing combination 84\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 29\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_084_val_71.1_test_62.5.pth\n",
      "\n",
      "[ 85/324] Testing combination 85\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 73.7%, Test: 52.1%\n",
      "  üíæ Model saved: combo_085_val_73.7_test_52.1.pth\n",
      "\n",
      "[ 86/324] Testing combination 86\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 35\n",
      "  ‚úì Val: 73.7%, Test: 56.2%\n",
      "  üíæ Model saved: combo_086_val_73.7_test_56.2.pth\n",
      "\n",
      "[ 87/324] Testing combination 87\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 39\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_087_val_73.7_test_62.5.pth\n",
      "\n",
      "[ 88/324] Testing combination 88\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 26\n",
      "  ‚úì Val: 73.7%, Test: 66.7%\n",
      "  üíæ Model saved: combo_088_val_73.7_test_66.7.pth\n",
      "\n",
      "[ 89/324] Testing combination 89\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 33\n",
      "  ‚úì Val: 73.7%, Test: 60.4%\n",
      "  üíæ Model saved: combo_089_val_73.7_test_60.4.pth\n",
      "\n",
      "[ 90/324] Testing combination 90\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 99.9%, Val 63.2%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 71.1%, Test: 70.8%\n",
      "  üíæ Model saved: combo_090_val_71.1_test_70.8.pth\n",
      "\n",
      "[ 91/324] Testing combination 91\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Epoch 40: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 40\n",
      "  ‚úì Val: 71.1%, Test: 60.4%\n",
      "  üíæ Model saved: combo_091_val_71.1_test_60.4.pth\n",
      "\n",
      "[ 92/324] Testing combination 92\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 30\n",
      "  ‚úì Val: 81.6%, Test: 60.4%\n",
      "  üíæ Model saved: combo_092_val_81.6_test_60.4.pth\n",
      "\n",
      "[ 93/324] Testing combination 93\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 60.5%\n",
      "    Early stopping at epoch 35\n",
      "  ‚úì Val: 71.1%, Test: 64.6%\n",
      "  üíæ Model saved: combo_093_val_71.1_test_64.6.pth\n",
      "\n",
      "[ 94/324] Testing combination 94\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 76.3%\n",
      "    Early stopping at epoch 38\n",
      "  ‚úì Val: 76.3%, Test: 62.5%\n",
      "  üíæ Model saved: combo_094_val_76.3_test_62.5.pth\n",
      "\n",
      "[ 95/324] Testing combination 95\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 42\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_095_val_73.7_test_62.5.pth\n",
      "\n",
      "[ 96/324] Testing combination 96\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 26\n",
      "  ‚úì Val: 71.1%, Test: 64.6%\n",
      "  üíæ Model saved: combo_096_val_71.1_test_64.6.pth\n",
      "\n",
      "[ 97/324] Testing combination 97\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 25\n",
      "  ‚úì Val: 71.1%, Test: 70.8%\n",
      "  üíæ Model saved: combo_097_val_71.1_test_70.8.pth\n",
      "\n",
      "[ 98/324] Testing combination 98\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 44\n",
      "  ‚úì Val: 76.3%, Test: 68.8%\n",
      "  üíæ Model saved: combo_098_val_76.3_test_68.8.pth\n",
      "\n",
      "[ 99/324] Testing combination 99\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 22\n",
      "  ‚úì Val: 73.7%, Test: 60.4%\n",
      "  üíæ Model saved: combo_099_val_73.7_test_60.4.pth\n",
      "\n",
      "[100/324] Testing combination 100\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 71.1%, Test: 66.7%\n",
      "  üíæ Model saved: combo_100_val_71.1_test_66.7.pth\n",
      "\n",
      "[101/324] Testing combination 101\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_101_val_73.7_test_64.6.pth\n",
      "\n",
      "[102/324] Testing combination 102\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 33\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_102_val_73.7_test_64.6.pth\n",
      "\n",
      "[103/324] Testing combination 103\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 21\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_103_val_71.1_test_62.5.pth\n",
      "\n",
      "[104/324] Testing combination 104\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 26\n",
      "  ‚úì Val: 68.4%, Test: 58.3%\n",
      "  üíæ Model saved: combo_104_val_68.4_test_58.3.pth\n",
      "\n",
      "[105/324] Testing combination 105\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Epoch 40: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 47\n",
      "  ‚úì Val: 76.3%, Test: 62.5%\n",
      "  üíæ Model saved: combo_105_val_76.3_test_62.5.pth\n",
      "\n",
      "[106/324] Testing combination 106\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 71.1%, Test: 66.7%\n",
      "  üíæ Model saved: combo_106_val_71.1_test_66.7.pth\n",
      "\n",
      "[107/324] Testing combination 107\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 51\n",
      "  ‚úì Val: 73.7%, Test: 66.7%\n",
      "  üíæ Model saved: combo_107_val_73.7_test_66.7.pth\n",
      "\n",
      "[108/324] Testing combination 108\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 49\n",
      "  ‚úì Val: 73.7%, Test: 70.8%\n",
      "  üíæ Model saved: combo_108_val_73.7_test_70.8.pth\n",
      "\n",
      "[109/324] Testing combination 109\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 30\n",
      "  ‚úì Val: 71.1%, Test: 64.6%\n",
      "  üíæ Model saved: combo_109_val_71.1_test_64.6.pth\n",
      "\n",
      "[110/324] Testing combination 110\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 31\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_110_val_71.1_test_62.5.pth\n",
      "\n",
      "[111/324] Testing combination 111\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 71.1%, Test: 66.7%\n",
      "  üíæ Model saved: combo_111_val_71.1_test_66.7.pth\n",
      "\n",
      "[112/324] Testing combination 112\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 57.9%\n",
      "    Early stopping at epoch 30\n",
      "  ‚úì Val: 73.7%, Test: 70.8%\n",
      "  üíæ Model saved: combo_112_val_73.7_test_70.8.pth\n",
      "\n",
      "[113/324] Testing combination 113\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 60.5%\n",
      "    Epoch 40: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 43\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_113_val_71.1_test_62.5.pth\n",
      "\n",
      "[114/324] Testing combination 114\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 34\n",
      "  ‚úì Val: 73.7%, Test: 68.8%\n",
      "  üíæ Model saved: combo_114_val_73.7_test_68.8.pth\n",
      "\n",
      "[115/324] Testing combination 115\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 47\n",
      "  ‚úì Val: 78.9%, Test: 64.6%\n",
      "  üíæ Model saved: combo_115_val_78.9_test_64.6.pth\n",
      "\n",
      "[116/324] Testing combination 116\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 99.9%, Val 76.3%\n",
      "    Early stopping at epoch 49\n",
      "  ‚úì Val: 78.9%, Test: 58.3%\n",
      "  üíæ Model saved: combo_116_val_78.9_test_58.3.pth\n",
      "\n",
      "[117/324] Testing combination 117\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 31\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_117_val_73.7_test_62.5.pth\n",
      "\n",
      "[118/324] Testing combination 118\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 99.9%, Val 63.2%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_118_val_73.7_test_64.6.pth\n",
      "\n",
      "[119/324] Testing combination 119\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 25\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_119_val_73.7_test_64.6.pth\n",
      "\n",
      "[120/324] Testing combination 120\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 99.9%, Val 71.1%\n",
      "    Epoch 40: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 43\n",
      "  ‚úì Val: 73.7%, Test: 68.8%\n",
      "  üíæ Model saved: combo_120_val_73.7_test_68.8.pth\n",
      "\n",
      "[121/324] Testing combination 121\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 60.5%\n",
      "    Early stopping at epoch 48\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_121_val_73.7_test_62.5.pth\n",
      "\n",
      "[122/324] Testing combination 122\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 26\n",
      "  ‚úì Val: 73.7%, Test: 66.7%\n",
      "  üíæ Model saved: combo_122_val_73.7_test_66.7.pth\n",
      "\n",
      "[123/324] Testing combination 123\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Epoch 40: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 40\n",
      "  ‚úì Val: 71.1%, Test: 68.8%\n",
      "  üíæ Model saved: combo_123_val_71.1_test_68.8.pth\n",
      "\n",
      "[124/324] Testing combination 124\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 23\n",
      "  ‚úì Val: 73.7%, Test: 54.2%\n",
      "  üíæ Model saved: combo_124_val_73.7_test_54.2.pth\n",
      "\n",
      "[125/324] Testing combination 125\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 22\n",
      "  ‚úì Val: 76.3%, Test: 66.7%\n",
      "  üíæ Model saved: combo_125_val_76.3_test_66.7.pth\n",
      "\n",
      "[126/324] Testing combination 126\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 26\n",
      "  ‚úì Val: 71.1%, Test: 56.2%\n",
      "  üíæ Model saved: combo_126_val_71.1_test_56.2.pth\n",
      "\n",
      "[127/324] Testing combination 127\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 28\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_127_val_71.1_test_62.5.pth\n",
      "\n",
      "[128/324] Testing combination 128\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 73.7%, Test: 68.8%\n",
      "  üíæ Model saved: combo_128_val_73.7_test_68.8.pth\n",
      "\n",
      "[129/324] Testing combination 129\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Epoch 40: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 50\n",
      "  ‚úì Val: 78.9%, Test: 68.8%\n",
      "  üíæ Model saved: combo_129_val_78.9_test_68.8.pth\n",
      "\n",
      "[130/324] Testing combination 130\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 50\n",
      "  ‚úì Val: 78.9%, Test: 68.8%\n",
      "  üíæ Model saved: combo_130_val_78.9_test_68.8.pth\n",
      "\n",
      "[131/324] Testing combination 131\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 71.1%, Test: 64.6%\n",
      "  üíæ Model saved: combo_131_val_71.1_test_64.6.pth\n",
      "\n",
      "[132/324] Testing combination 132\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 51\n",
      "  ‚úì Val: 76.3%, Test: 66.7%\n",
      "  üíæ Model saved: combo_132_val_76.3_test_66.7.pth\n",
      "\n",
      "[133/324] Testing combination 133\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 26\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_133_val_73.7_test_64.6.pth\n",
      "\n",
      "[134/324] Testing combination 134\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Epoch 40: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 40\n",
      "  ‚úì Val: 73.7%, Test: 70.8%\n",
      "  üíæ Model saved: combo_134_val_73.7_test_70.8.pth\n",
      "\n",
      "[135/324] Testing combination 135\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 28\n",
      "  ‚úì Val: 73.7%, Test: 68.8%\n",
      "  üíæ Model saved: combo_135_val_73.7_test_68.8.pth\n",
      "\n",
      "[136/324] Testing combination 136\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 46\n",
      "  ‚úì Val: 73.7%, Test: 64.6%\n",
      "  üíæ Model saved: combo_136_val_73.7_test_64.6.pth\n",
      "\n",
      "[137/324] Testing combination 137\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 26\n",
      "  ‚úì Val: 73.7%, Test: 70.8%\n",
      "  üíæ Model saved: combo_137_val_73.7_test_70.8.pth\n",
      "\n",
      "[138/324] Testing combination 138\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 35\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_138_val_73.7_test_62.5.pth\n",
      "\n",
      "[139/324] Testing combination 139\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 27\n",
      "  ‚úì Val: 73.7%, Test: 70.8%\n",
      "  üíæ Model saved: combo_139_val_73.7_test_70.8.pth\n",
      "\n",
      "[140/324] Testing combination 140\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 42\n",
      "  ‚úì Val: 73.7%, Test: 66.7%\n",
      "  üíæ Model saved: combo_140_val_73.7_test_66.7.pth\n",
      "\n",
      "[141/324] Testing combination 141\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.3, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 43\n",
      "  ‚úì Val: 71.1%, Test: 68.8%\n",
      "  üíæ Model saved: combo_141_val_71.1_test_68.8.pth\n",
      "\n",
      "[142/324] Testing combination 142\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Epoch 60: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 62\n",
      "  ‚úì Val: 78.9%, Test: 72.9%\n",
      "  üíæ Model saved: combo_142_val_78.9_test_72.9.pth\n",
      "\n",
      "[143/324] Testing combination 143\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 23\n",
      "  ‚úì Val: 76.3%, Test: 60.4%\n",
      "  üíæ Model saved: combo_143_val_76.3_test_60.4.pth\n",
      "\n",
      "[144/324] Testing combination 144\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 34\n",
      "  ‚úì Val: 73.7%, Test: 62.5%\n",
      "  üíæ Model saved: combo_144_val_73.7_test_62.5.pth\n",
      "\n",
      "[145/324] Testing combination 145\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 60.5%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 71.1%, Test: 70.8%\n",
      "  üíæ Model saved: combo_145_val_71.1_test_70.8.pth\n",
      "\n",
      "[146/324] Testing combination 146\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 43\n",
      "  ‚úì Val: 76.3%, Test: 68.8%\n",
      "  üíæ Model saved: combo_146_val_76.3_test_68.8.pth\n",
      "\n",
      "[147/324] Testing combination 147\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 14, Dropout: 0.32, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 60.5%\n",
      "    Early stopping at epoch 39\n",
      "  ‚úì Val: 73.7%, Test: 60.4%\n",
      "  üíæ Model saved: combo_147_val_73.7_test_60.4.pth\n",
      "\n",
      "[148/324] Testing combination 148\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 24\n",
      "  ‚úì Val: 76.3%, Test: 64.6%\n",
      "  üíæ Model saved: combo_148_val_76.3_test_64.6.pth\n",
      "\n",
      "[149/324] Testing combination 149\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 4\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 29\n",
      "  ‚úì Val: 71.1%, Test: 62.5%\n",
      "  üíæ Model saved: combo_149_val_71.1_test_62.5.pth\n",
      "\n",
      "[150/324] Testing combination 150\n",
      "  Optimizer: adamw, LR: 9e-05/0.00045\n",
      "  Batch: 12, Dropout: 0.25, Freeze: 3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_reproducible_seeds(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Ultra-focused GhostNet Hyperparameter Search Space around best results\n",
    "HYPERPARAMETER_GRID = {\n",
    "    'backbone_lr': [0.00009, 0.0001, 0.00011],          # Very tight around 0.0001\n",
    "    'classifier_lr': [0.00045, 0.0005, 0.00055],        # Very tight around 0.0005\n",
    "    'weight_decay': [0.009, 0.01, 0.011],               # Very tight around 0.01\n",
    "    'batch_size': [12, 14],                             # Focus on 12 and 14\n",
    "    'optimizer': ['adamw'],                              # Fixed at adamw\n",
    "    'scheduler': ['cosine', 'plateau'],                  # Keep both\n",
    "    'dropout': [0.25, 0.3, 0.32],                       # Around 0.25-0.3\n",
    "    'label_smoothing': [0.09, 0.1, 0.11],               # Very tight around 0.1\n",
    "    'augmentation_strength': ['medium'],                 # Fixed at medium\n",
    "    'freeze_layers': [3, 4]                             # Keep 3-4\n",
    "}\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUGMENTATION_TARGET = 1000\n",
    "MAX_COMBINATIONS = 324  # Test all combinations in focused grid\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    \"\"\"Detect if image is grayscale and convert to 3-channel RGB\"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    \"\"\"Load data from both color and grayscale folders\"\"\"\n",
    "    color_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "    gray_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*_NDA.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    # Group ages\n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    # Filter classes with enough samples\n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    \n",
    "    return np.array(filtered_images), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image(image, strength='medium'):\n",
    "    \"\"\"Enhanced augmentation with variable strength\"\"\"\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Set probabilities based on strength\n",
    "    if strength == 'light':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.5, 0.3, 0.6, 0.2, 0.1\n",
    "        rot_range, bright_range = 10, (0.8, 1.2)\n",
    "    elif strength == 'medium':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.7, 0.5, 0.8, 0.4, 0.3\n",
    "        rot_range, bright_range = 15, (0.7, 1.3)\n",
    "    else:  # heavy\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.8, 0.6, 0.9, 0.5, 0.4\n",
    "        rot_range, bright_range = 20, (0.6, 1.4)\n",
    "    \n",
    "    # Rotation\n",
    "    if random.random() < rot_prob:\n",
    "        angle = random.uniform(-rot_range, rot_range)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Horizontal flip\n",
    "    if random.random() < flip_prob:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Strategic color conversion (RGB -> Grayscale -> RGB)\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.4:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Brightness/contrast\n",
    "    if random.random() < bright_prob:\n",
    "        alpha = random.uniform(*bright_range)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Gamma correction\n",
    "    if random.random() < gamma_prob:\n",
    "        gamma = random.uniform(0.8, 1.2)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    # Noise\n",
    "    if random.random() < noise_prob:\n",
    "        noise = np.random.normal(0, 7, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_balanced_dataset(X, y, aug_strength='medium'):\n",
    "    \"\"\"Create balanced dataset through augmentation\"\"\"\n",
    "    class_counts = Counter(y)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max(AUGMENTATION_TARGET, max_count)\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for class_idx in range(len(set(y))):\n",
    "        class_mask = np.array(y) == class_idx\n",
    "        class_images = X[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        if current_count == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add originals\n",
    "        X_balanced.extend(class_images)\n",
    "        y_balanced.extend([class_idx] * current_count)\n",
    "        \n",
    "        # Add augmented to reach target\n",
    "        needed = target_count - current_count\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image(class_images[orig_idx].copy(), aug_strength)\n",
    "            X_balanced.append(aug_img)\n",
    "            y_balanced.append(class_idx)\n",
    "    \n",
    "    return np.array(X_balanced), np.array(y_balanced)\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    def __init__(self, X, y, training=True):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.training = training\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class GhostNetHyperparameterTuner:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"ghostnet_focused_tuning_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            torch.backends.cudnn.benchmark = False  # Set to False for reproducibility\n",
    "    \n",
    "    def create_ghostnet_model(self, dropout=0.3, freeze_layers=3):\n",
    "        \"\"\"Create GhostNet model with specified dropout and freezing\"\"\"\n",
    "        model = timm.create_model('ghostnet_100', pretrained=True, num_classes=self.num_classes)\n",
    "        \n",
    "        # Freeze early layers based on freeze_layers parameter\n",
    "        freeze_patterns = [\n",
    "            ['conv_stem'],\n",
    "            ['conv_stem', 'blocks.0'],\n",
    "            ['conv_stem', 'blocks.0', 'blocks.1'],\n",
    "            ['conv_stem', 'blocks.0', 'blocks.1', 'blocks.2']\n",
    "        ]\n",
    "        \n",
    "        if freeze_layers <= len(freeze_patterns):\n",
    "            for name, param in model.named_parameters():\n",
    "                for pattern in freeze_patterns[freeze_layers - 1]:\n",
    "                    if pattern in name:\n",
    "                        param.requires_grad = False\n",
    "                        break\n",
    "        \n",
    "        # Replace classifier with custom dropout\n",
    "        if hasattr(model, 'classifier'):\n",
    "            in_features = model.classifier.in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def get_optimizer(self, model, opt_type, backbone_lr, classifier_lr, weight_decay):\n",
    "        \"\"\"Create optimizer based on hyperparameters\"\"\"\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'classifier' in name:\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        param_groups = [\n",
    "            {'params': backbone_params, 'lr': backbone_lr},\n",
    "            {'params': classifier_params, 'lr': classifier_lr}\n",
    "        ]\n",
    "        \n",
    "        if opt_type == 'adamw':\n",
    "            return optim.AdamW(param_groups, weight_decay=weight_decay)\n",
    "        elif opt_type == 'sgd':\n",
    "            return optim.SGD(param_groups, weight_decay=weight_decay, momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {opt_type}\")\n",
    "    \n",
    "    def get_scheduler(self, optimizer, scheduler_type, max_epochs):\n",
    "        \"\"\"Create learning rate scheduler\"\"\"\n",
    "        if scheduler_type == 'cosine':\n",
    "            return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-6)\n",
    "        elif scheduler_type == 'plateau':\n",
    "            return optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, verbose=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scheduler: {scheduler_type}\")\n",
    "    \n",
    "    def train_with_hyperparams(self, train_loader, val_loader, test_loader, hyperparams, combo_num):\n",
    "        \"\"\"Train model with specific hyperparameters\"\"\"\n",
    "        # Set seed for this specific combination\n",
    "        torch.manual_seed(42 + combo_num)\n",
    "        \n",
    "        model = self.create_ghostnet_model(\n",
    "            dropout=hyperparams['dropout'], \n",
    "            freeze_layers=hyperparams['freeze_layers']\n",
    "        )\n",
    "        \n",
    "        optimizer = self.get_optimizer(\n",
    "            model, hyperparams['optimizer'], \n",
    "            hyperparams['backbone_lr'], hyperparams['classifier_lr'], \n",
    "            hyperparams['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = self.get_scheduler(optimizer, hyperparams['scheduler'], 80)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=hyperparams['label_smoothing'])\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 20\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(80):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Update scheduler\n",
    "            if hyperparams['scheduler'] == 'plateau':\n",
    "                scheduler.step(val_acc)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Print progress every 20 epochs\n",
    "            if epoch % 20 == 0 and epoch > 0:\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load best weights and evaluate on test\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        # Test evaluation with TTA\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Test time augmentation\n",
    "                outputs1 = model(images)\n",
    "                flipped = torch.flip(images, [3])\n",
    "                outputs2 = model(flipped)\n",
    "                outputs = (outputs1 + outputs2) / 2\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        return model, best_val_acc, test_acc\n",
    "    \n",
    "    def generate_best_first_combinations(self):\n",
    "        \"\"\"Test known good combinations first, then systematic exploration\"\"\"\n",
    "        # Your best known combinations first\n",
    "        priority_combinations = [\n",
    "            {\n",
    "                'backbone_lr': 0.0001, 'classifier_lr': 0.0005, 'weight_decay': 0.01,\n",
    "                'batch_size': 12, 'optimizer': 'adamw', 'scheduler': 'cosine',\n",
    "                'dropout': 0.3, 'label_smoothing': 0.1, 'augmentation_strength': 'medium',\n",
    "                'freeze_layers': 4\n",
    "            },\n",
    "            {\n",
    "                'backbone_lr': 0.0001, 'classifier_lr': 0.00055, 'weight_decay': 0.01,\n",
    "                'batch_size': 14, 'optimizer': 'adamw', 'scheduler': 'cosine',\n",
    "                'dropout': 0.25, 'label_smoothing': 0.1, 'augmentation_strength': 'medium',\n",
    "                'freeze_layers': 3\n",
    "            },\n",
    "            {\n",
    "                'backbone_lr': 0.0001, 'classifier_lr': 0.0005, 'weight_decay': 0.01,\n",
    "                'batch_size': 12, 'optimizer': 'adamw', 'scheduler': 'plateau',\n",
    "                'dropout': 0.3, 'label_smoothing': 0.1, 'augmentation_strength': 'medium',\n",
    "                'freeze_layers': 4\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Generate all systematic combinations\n",
    "        keys = list(HYPERPARAMETER_GRID.keys())\n",
    "        values = list(HYPERPARAMETER_GRID.values())\n",
    "        all_combinations = list(itertools.product(*values))\n",
    "        \n",
    "        # Convert to dictionaries\n",
    "        all_combo_dicts = []\n",
    "        for combo in all_combinations:\n",
    "            combo_dict = dict(zip(keys, combo))\n",
    "            # Skip if already in priority list\n",
    "            if combo_dict not in priority_combinations:\n",
    "                all_combo_dicts.append(combo_dict)\n",
    "        \n",
    "        # Return priority first, then systematic exploration\n",
    "        final_combinations = priority_combinations + all_combo_dicts\n",
    "        return final_combinations[:MAX_COMBINATIONS]\n",
    "    \n",
    "    def tune_hyperparameters(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        \"\"\"Main hyperparameter tuning loop\"\"\"\n",
    "        print(f\"Starting Focused GhostNet hyperparameter tuning...\")\n",
    "        \n",
    "        combinations = self.generate_best_first_combinations()\n",
    "        actual_combinations = min(len(combinations), MAX_COMBINATIONS)\n",
    "        print(f\"Testing {actual_combinations} hyperparameter combinations (best-first strategy)\")\n",
    "        \n",
    "        results = []\n",
    "        best_val_acc = 0.0\n",
    "        best_test_acc = 0.0\n",
    "        \n",
    "        for i, hyperparams in enumerate(combinations[:MAX_COMBINATIONS], 1):\n",
    "            print(f\"\\n[{i:3d}/{actual_combinations}] Testing combination {i}\")\n",
    "            print(f\"  Optimizer: {hyperparams['optimizer']}, LR: {hyperparams['backbone_lr']}/{hyperparams['classifier_lr']}\")\n",
    "            print(f\"  Batch: {hyperparams['batch_size']}, Dropout: {hyperparams['dropout']}, Freeze: {hyperparams['freeze_layers']}\")\n",
    "            if i <= 3:\n",
    "                print(f\"  ‚≠ê PRIORITY COMBINATION (known good)\")\n",
    "            \n",
    "            try:\n",
    "                # Set seed for reproducible augmentation\n",
    "                set_reproducible_seeds(42 + i)\n",
    "                \n",
    "                # Create datasets with current augmentation strength\n",
    "                X_train_aug, y_train_aug = create_balanced_dataset(\n",
    "                    X_train, y_train, hyperparams['augmentation_strength']\n",
    "                )\n",
    "                \n",
    "                train_dataset = DeerDataset(X_train_aug, y_train_aug, training=True)\n",
    "                val_dataset = DeerDataset(X_val, y_val, training=False)\n",
    "                test_dataset = DeerDataset(X_test, y_test, training=False)\n",
    "                \n",
    "                train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True, num_workers=0)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'], shuffle=False, num_workers=0)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'], shuffle=False, num_workers=0)\n",
    "                \n",
    "                model, val_acc, test_acc = self.train_with_hyperparams(\n",
    "                    train_loader, val_loader, test_loader, hyperparams, i\n",
    "                )\n",
    "                \n",
    "                result = {\n",
    "                    'combination': i,\n",
    "                    'hyperparams': hyperparams,\n",
    "                    'val_accuracy': val_acc,\n",
    "                    'test_accuracy': test_acc\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"  ‚úì Val: {val_acc:.1f}%, Test: {test_acc:.1f}%\")\n",
    "                \n",
    "                # Save every model\n",
    "                model_path = os.path.join(self.save_dir, f\"ghostnet_combo_{i:03d}_val_{val_acc:.1f}_test_{test_acc:.1f}.pth\")\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'hyperparams': hyperparams,\n",
    "                    'val_accuracy': val_acc,\n",
    "                    'test_accuracy': test_acc,\n",
    "                    'combination': i\n",
    "                }, model_path)\n",
    "                print(f\"  üíæ Model saved: combo_{i:03d}_val_{val_acc:.1f}_test_{test_acc:.1f}.pth\")\n",
    "                \n",
    "                # Track best for summary\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    print(f\"  üèÜ New best validation: {val_acc:.1f}%\")\n",
    "                \n",
    "                if test_acc > best_test_acc:\n",
    "                    best_test_acc = test_acc\n",
    "                    print(f\"  üéØ New best test: {test_acc:.1f}%\")\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Combination {i} failed: {str(e)[:60]}...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "        \n",
    "        # Save all results\n",
    "        results_path = os.path.join(self.save_dir, \"focused_hyperparameter_results.json\")\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set with TTA\"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Test time augmentation\n",
    "            outputs1 = model(images)\n",
    "            flipped = torch.flip(images, [3])\n",
    "            outputs2 = model(flipped)\n",
    "            outputs = (outputs1 + outputs2) / 2\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * test_correct / test_total\n",
    "    return test_acc\n",
    "\n",
    "def main():\n",
    "    # Set reproducible seeds first\n",
    "    set_reproducible_seeds(42)\n",
    "    \n",
    "    print(\"Focused GhostNet Hyperparameter Tuning for Deer Age Prediction\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load combined data\n",
    "    images, ages, sources = load_combined_data()\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    y_indices = np.array([label_mapping[age] for age in ages])\n",
    "    \n",
    "    print(f\"\\nClasses: {len(unique_ages)}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    # Train/test split with fixed random state\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images, y_indices, test_size=0.2, random_state=42, stratify=y_indices\n",
    "    )\n",
    "    \n",
    "    # Further split training into train/val with fixed random state\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"Train: {len(X_train_final)} images\")\n",
    "    print(f\"Val: {len(X_val)} images\") \n",
    "    print(f\"Test: {len(X_test)} images\")\n",
    "    \n",
    "    # Initialize tuner\n",
    "    tuner = GhostNetHyperparameterTuner(num_classes=len(unique_ages))\n",
    "    \n",
    "    # Run hyperparameter tuning\n",
    "    results = tuner.tune_hyperparameters(X_train_final, y_train_final, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    # Analyze results\n",
    "    if results:\n",
    "        # Sort by test accuracy (primary) and validation accuracy (secondary)\n",
    "        sorted_by_test = sorted(results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        sorted_by_val = sorted(results, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "        \n",
    "        elapsed = (time.time() - start_time) / 60\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"FOCUSED GHOSTNET TUNING RESULTS - SORTED BY TEST ACCURACY\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{'Rank':<4} {'Combo':<5} {'Validation':<12} {'Test':<8} {'Key Hyperparams'}\")\n",
    "        print('-' * 70)\n",
    "        \n",
    "        for i, result in enumerate(sorted_by_test[:15], 1):  # Top 15 by test\n",
    "            hp = result['hyperparams']\n",
    "            key_params = f\"lr={hp['backbone_lr']}/{hp['classifier_lr']}, bs={hp['batch_size']}, drop={hp['dropout']}, freeze={hp['freeze_layers']}\"\n",
    "            print(f\"{i:2d}. {result['combination']:4d}       {result['val_accuracy']:5.1f}%      {result['test_accuracy']:5.1f}%    {key_params}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TOP RESULTS BY VALIDATION ACCURACY\")\n",
    "        print(\"=\" * 70)\n",
    "        for i, result in enumerate(sorted_by_val[:10], 1):  # Top 10 by val\n",
    "            hp = result['hyperparams']\n",
    "            key_params = f\"lr={hp['backbone_lr']}/{hp['classifier_lr']}, bs={hp['batch_size']}, drop={hp['dropout']}, freeze={hp['freeze_layers']}\"\n",
    "            print(f\"{i:2d}. {result['combination']:4d}       {result['val_accuracy']:5.1f}%      {result['test_accuracy']:5.1f}%    {key_params}\")\n",
    "        \n",
    "        best_test_result = sorted_by_test[0]\n",
    "        best_val_result = sorted_by_val[0]\n",
    "        \n",
    "        print(f\"\\nüéØ BEST TEST ACCURACY: {best_test_result['test_accuracy']:.1f}% (Combo {best_test_result['combination']})\")\n",
    "        print(f\"üèÜ BEST VALIDATION ACCURACY: {best_val_result['val_accuracy']:.1f}% (Combo {best_val_result['combination']})\")\n",
    "        \n",
    "        print(f\"\\nüìä SUMMARY:\")\n",
    "        print(f\"  Best Test: {best_test_result['test_accuracy']:.1f}%\")\n",
    "        print(f\"  Best Validation: {best_val_result['val_accuracy']:.1f}%\")\n",
    "        print(f\"  Tuning Time: {elapsed:.1f} minutes\")\n",
    "        print(f\"  Results saved to: {tuner.save_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No successful combinations found!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c13841-464e-4971-9a64-def1e12fe3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focused GhostNet Hyperparameter Tuning for Deer Age Prediction\n",
      "======================================================================\n",
      "Loading color images...\n",
      "Loaded 200 color images\n",
      "Loading grayscale images...\n",
      "Loaded 38 grayscale images\n",
      "Total images: 238\n",
      "Final dataset: 238 images\n",
      "Age distribution: {2.5: 40, 3.5: 50, 4.5: 56, 5.5: 60, 1.5: 32}\n",
      "\n",
      "Classes: 5\n",
      "Label mapping: {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
      "\n",
      "Data split:\n",
      "Train: 152 images\n",
      "Val: 38 images\n",
      "Test: 48 images\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Starting Focused GhostNet hyperparameter tuning...\n",
      "Testing 324 hyperparameter combinations (best-first strategy)\n",
      "\n",
      "[  1/324] Testing combination 1\n",
      "  Optimizer: adamw, LR: 0.0001/0.0005\n",
      "  Batch: 12, Dropout: 0.3, Freeze: 4\n",
      "  ‚≠ê PRIORITY COMBINATION (known good)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_reproducible_seeds(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Ultra-focused GhostNet Hyperparameter Search Space around best results\n",
    "HYPERPARAMETER_GRID = {\n",
    "    'backbone_lr': [0.00009, 0.0001, 0.00011],          # Very tight around 0.0001\n",
    "    'classifier_lr': [0.00045, 0.0005, 0.00055],        # Very tight around 0.0005\n",
    "    'weight_decay': [0.009, 0.01, 0.011],               # Very tight around 0.01\n",
    "    'batch_size': [12, 14],                             # Focus on 12 and 14\n",
    "    'optimizer': ['adamw'],                              # Fixed at adamw\n",
    "    'scheduler': ['cosine', 'plateau'],                  # Keep both\n",
    "    'dropout': [0.25, 0.3, 0.32],                       # Around 0.25-0.3\n",
    "    'label_smoothing': [0.09, 0.1, 0.11],               # Very tight around 0.1\n",
    "    'augmentation_strength': ['medium'],                 # Fixed at medium\n",
    "    'freeze_layers': [3, 4]                             # Keep 3-4\n",
    "}\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUGMENTATION_TARGET = 1000\n",
    "MAX_COMBINATIONS = 324  # Test all combinations in focused grid\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    \"\"\"Detect if image is grayscale and convert to 3-channel RGB\"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    \"\"\"Load data from both color and grayscale folders\"\"\"\n",
    "    color_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "    gray_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*_NDA.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    # Group ages\n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    # Filter classes with enough samples\n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    \n",
    "    return np.array(filtered_images), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image(image, strength='medium'):\n",
    "    \"\"\"Enhanced augmentation with variable strength\"\"\"\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Set probabilities based on strength\n",
    "    if strength == 'light':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.5, 0.3, 0.6, 0.2, 0.1\n",
    "        rot_range, bright_range = 10, (0.8, 1.2)\n",
    "    elif strength == 'medium':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.7, 0.5, 0.8, 0.4, 0.3\n",
    "        rot_range, bright_range = 15, (0.7, 1.3)\n",
    "    else:  # heavy\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.8, 0.6, 0.9, 0.5, 0.4\n",
    "        rot_range, bright_range = 20, (0.6, 1.4)\n",
    "    \n",
    "    # Rotation\n",
    "    if random.random() < rot_prob:\n",
    "        angle = random.uniform(-rot_range, rot_range)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Horizontal flip\n",
    "    if random.random() < flip_prob:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Strategic color conversion (RGB -> Grayscale -> RGB)\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.4:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Brightness/contrast\n",
    "    if random.random() < bright_prob:\n",
    "        alpha = random.uniform(*bright_range)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Gamma correction\n",
    "    if random.random() < gamma_prob:\n",
    "        gamma = random.uniform(0.8, 1.2)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    # Noise\n",
    "    if random.random() < noise_prob:\n",
    "        noise = np.random.normal(0, 7, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_balanced_dataset(X, y, aug_strength='medium'):\n",
    "    \"\"\"Create balanced dataset through augmentation\"\"\"\n",
    "    class_counts = Counter(y)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max(AUGMENTATION_TARGET, max_count)\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for class_idx in range(len(set(y))):\n",
    "        class_mask = np.array(y) == class_idx\n",
    "        class_images = X[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        if current_count == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add originals\n",
    "        X_balanced.extend(class_images)\n",
    "        y_balanced.extend([class_idx] * current_count)\n",
    "        \n",
    "        # Add augmented to reach target\n",
    "        needed = target_count - current_count\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image(class_images[orig_idx].copy(), aug_strength)\n",
    "            X_balanced.append(aug_img)\n",
    "            y_balanced.append(class_idx)\n",
    "    \n",
    "    return np.array(X_balanced), np.array(y_balanced)\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    def __init__(self, X, y, training=True):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.training = training\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class GhostNetHyperparameterTuner:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"ghostnet_focused_tuning_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            torch.backends.cudnn.benchmark = False  # Set to False for reproducibility\n",
    "    \n",
    "    def create_ghostnet_model(self, dropout=0.3, freeze_layers=3):\n",
    "        \"\"\"Create GhostNet model with specified dropout and freezing\"\"\"\n",
    "        model = timm.create_model('ghostnet_100', pretrained=True, num_classes=self.num_classes)\n",
    "        \n",
    "        # Freeze early layers based on freeze_layers parameter\n",
    "        freeze_patterns = [\n",
    "            ['conv_stem'],\n",
    "            ['conv_stem', 'blocks.0'],\n",
    "            ['conv_stem', 'blocks.0', 'blocks.1'],\n",
    "            ['conv_stem', 'blocks.0', 'blocks.1', 'blocks.2']\n",
    "        ]\n",
    "        \n",
    "        if freeze_layers <= len(freeze_patterns):\n",
    "            for name, param in model.named_parameters():\n",
    "                for pattern in freeze_patterns[freeze_layers - 1]:\n",
    "                    if pattern in name:\n",
    "                        param.requires_grad = False\n",
    "                        break\n",
    "        \n",
    "        # Replace classifier with custom dropout\n",
    "        if hasattr(model, 'classifier'):\n",
    "            in_features = model.classifier.in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def get_optimizer(self, model, opt_type, backbone_lr, classifier_lr, weight_decay):\n",
    "        \"\"\"Create optimizer based on hyperparameters\"\"\"\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'classifier' in name:\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        param_groups = [\n",
    "            {'params': backbone_params, 'lr': backbone_lr},\n",
    "            {'params': classifier_params, 'lr': classifier_lr}\n",
    "        ]\n",
    "        \n",
    "        if opt_type == 'adamw':\n",
    "            return optim.AdamW(param_groups, weight_decay=weight_decay)\n",
    "        elif opt_type == 'sgd':\n",
    "            return optim.SGD(param_groups, weight_decay=weight_decay, momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {opt_type}\")\n",
    "    \n",
    "    def get_scheduler(self, optimizer, scheduler_type, max_epochs):\n",
    "        \"\"\"Create learning rate scheduler\"\"\"\n",
    "        if scheduler_type == 'cosine':\n",
    "            return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-6)\n",
    "        elif scheduler_type == 'plateau':\n",
    "            return optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, verbose=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scheduler: {scheduler_type}\")\n",
    "    \n",
    "    def train_with_hyperparams(self, train_loader, val_loader, test_loader, hyperparams, combo_num):\n",
    "        \"\"\"Train model with specific hyperparameters\"\"\"\n",
    "        # Set seed for this specific combination\n",
    "        torch.manual_seed(42 + combo_num)\n",
    "        \n",
    "        model = self.create_ghostnet_model(\n",
    "            dropout=hyperparams['dropout'], \n",
    "            freeze_layers=hyperparams['freeze_layers']\n",
    "        )\n",
    "        \n",
    "        optimizer = self.get_optimizer(\n",
    "            model, hyperparams['optimizer'], \n",
    "            hyperparams['backbone_lr'], hyperparams['classifier_lr'], \n",
    "            hyperparams['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = self.get_scheduler(optimizer, hyperparams['scheduler'], 80)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=hyperparams['label_smoothing'])\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 20\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(80):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Update scheduler\n",
    "            if hyperparams['scheduler'] == 'plateau':\n",
    "                scheduler.step(val_acc)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Print progress every 20 epochs\n",
    "            if epoch % 20 == 0 and epoch > 0:\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load best weights and evaluate on test\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        # Test evaluation with TTA\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Test time augmentation\n",
    "                outputs1 = model(images)\n",
    "                flipped = torch.flip(images, [3])\n",
    "                outputs2 = model(flipped)\n",
    "                outputs = (outputs1 + outputs2) / 2\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        return model, best_val_acc, test_acc\n",
    "    \n",
    "    def generate_best_first_combinations(self):\n",
    "        \"\"\"Test known good combinations first, then systematic exploration\"\"\"\n",
    "        # Your best known combinations first\n",
    "        priority_combinations = [\n",
    "            {\n",
    "                'backbone_lr': 0.0001, 'classifier_lr': 0.0005, 'weight_decay': 0.01,\n",
    "                'batch_size': 12, 'optimizer': 'adamw', 'scheduler': 'cosine',\n",
    "                'dropout': 0.3, 'label_smoothing': 0.1, 'augmentation_strength': 'medium',\n",
    "                'freeze_layers': 4\n",
    "            },\n",
    "            {\n",
    "                'backbone_lr': 0.0001, 'classifier_lr': 0.00055, 'weight_decay': 0.01,\n",
    "                'batch_size': 14, 'optimizer': 'adamw', 'scheduler': 'cosine',\n",
    "                'dropout': 0.25, 'label_smoothing': 0.1, 'augmentation_strength': 'medium',\n",
    "                'freeze_layers': 3\n",
    "            },\n",
    "            {\n",
    "                'backbone_lr': 0.0001, 'classifier_lr': 0.0005, 'weight_decay': 0.01,\n",
    "                'batch_size': 12, 'optimizer': 'adamw', 'scheduler': 'plateau',\n",
    "                'dropout': 0.3, 'label_smoothing': 0.1, 'augmentation_strength': 'medium',\n",
    "                'freeze_layers': 4\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Generate all systematic combinations\n",
    "        keys = list(HYPERPARAMETER_GRID.keys())\n",
    "        values = list(HYPERPARAMETER_GRID.values())\n",
    "        all_combinations = list(itertools.product(*values))\n",
    "        \n",
    "        # Convert to dictionaries\n",
    "        all_combo_dicts = []\n",
    "        for combo in all_combinations:\n",
    "            combo_dict = dict(zip(keys, combo))\n",
    "            # Skip if already in priority list\n",
    "            if combo_dict not in priority_combinations:\n",
    "                all_combo_dicts.append(combo_dict)\n",
    "        \n",
    "        # Return priority first, then systematic exploration\n",
    "        final_combinations = priority_combinations + all_combo_dicts\n",
    "        return final_combinations[:MAX_COMBINATIONS]\n",
    "    \n",
    "    def tune_hyperparameters(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        \"\"\"Main hyperparameter tuning loop\"\"\"\n",
    "        print(f\"Starting Focused GhostNet hyperparameter tuning...\")\n",
    "        \n",
    "        combinations = self.generate_best_first_combinations()\n",
    "        actual_combinations = min(len(combinations), MAX_COMBINATIONS)\n",
    "        print(f\"Testing {actual_combinations} hyperparameter combinations (best-first strategy)\")\n",
    "        \n",
    "        results = []\n",
    "        best_val_acc = 0.0\n",
    "        best_test_acc = 0.0\n",
    "        \n",
    "        for i, hyperparams in enumerate(combinations[:MAX_COMBINATIONS], 1):\n",
    "            print(f\"\\n[{i:3d}/{actual_combinations}] Testing combination {i}\")\n",
    "            print(f\"  Optimizer: {hyperparams['optimizer']}, LR: {hyperparams['backbone_lr']}/{hyperparams['classifier_lr']}\")\n",
    "            print(f\"  Batch: {hyperparams['batch_size']}, Dropout: {hyperparams['dropout']}, Freeze: {hyperparams['freeze_layers']}\")\n",
    "            if i <= 3:\n",
    "                print(f\"  ‚≠ê PRIORITY COMBINATION (known good)\")\n",
    "            \n",
    "            try:\n",
    "                # Set seed for reproducible augmentation\n",
    "                set_reproducible_seeds(42 + i)\n",
    "                \n",
    "                # Create datasets with current augmentation strength\n",
    "                X_train_aug, y_train_aug = create_balanced_dataset(\n",
    "                    X_train, y_train, hyperparams['augmentation_strength']\n",
    "                )\n",
    "                \n",
    "                train_dataset = DeerDataset(X_train_aug, y_train_aug, training=True)\n",
    "                val_dataset = DeerDataset(X_val, y_val, training=False)\n",
    "                test_dataset = DeerDataset(X_test, y_test, training=False)\n",
    "                \n",
    "                train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True, num_workers=0)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'], shuffle=False, num_workers=0)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'], shuffle=False, num_workers=0)\n",
    "                \n",
    "                model, val_acc, test_acc = self.train_with_hyperparams(\n",
    "                    train_loader, val_loader, test_loader, hyperparams, i\n",
    "                )\n",
    "                \n",
    "                result = {\n",
    "                    'combination': i,\n",
    "                    'hyperparams': hyperparams,\n",
    "                    'val_accuracy': val_acc,\n",
    "                    'test_accuracy': test_acc\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"  ‚úì Val: {val_acc:.1f}%, Test: {test_acc:.1f}%\")\n",
    "                \n",
    "                # Save every model\n",
    "                model_path = os.path.join(self.save_dir, f\"ghostnet_combo_{i:03d}_val_{val_acc:.1f}_test_{test_acc:.1f}.pth\")\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'hyperparams': hyperparams,\n",
    "                    'val_accuracy': val_acc,\n",
    "                    'test_accuracy': test_acc,\n",
    "                    'combination': i\n",
    "                }, model_path)\n",
    "                print(f\"  üíæ Model saved: combo_{i:03d}_val_{val_acc:.1f}_test_{test_acc:.1f}.pth\")\n",
    "                \n",
    "                # Track best for summary\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    print(f\"  üèÜ New best validation: {val_acc:.1f}%\")\n",
    "                \n",
    "                if test_acc > best_test_acc:\n",
    "                    best_test_acc = test_acc\n",
    "                    print(f\"  üéØ New best test: {test_acc:.1f}%\")\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Combination {i} failed: {str(e)[:60]}...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "        \n",
    "        # Save all results\n",
    "        results_path = os.path.join(self.save_dir, \"focused_hyperparameter_results.json\")\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set with TTA\"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Test time augmentation\n",
    "            outputs1 = model(images)\n",
    "            flipped = torch.flip(images, [3])\n",
    "            outputs2 = model(flipped)\n",
    "            outputs = (outputs1 + outputs2) / 2\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * test_correct / test_total\n",
    "    return test_acc\n",
    "\n",
    "def main():\n",
    "    # Set reproducible seeds first\n",
    "    set_reproducible_seeds(42)\n",
    "    \n",
    "    print(\"Focused GhostNet Hyperparameter Tuning for Deer Age Prediction\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load combined data\n",
    "    images, ages, sources = load_combined_data()\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    y_indices = np.array([label_mapping[age] for age in ages])\n",
    "    \n",
    "    print(f\"\\nClasses: {len(unique_ages)}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    # Train/test split with fixed random state\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images, y_indices, test_size=0.2, random_state=42, stratify=y_indices\n",
    "    )\n",
    "    \n",
    "    # Further split training into train/val with fixed random state\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"Train: {len(X_train_final)} images\")\n",
    "    print(f\"Val: {len(X_val)} images\") \n",
    "    print(f\"Test: {len(X_test)} images\")\n",
    "    \n",
    "    # Initialize tuner\n",
    "    tuner = GhostNetHyperparameterTuner(num_classes=len(unique_ages))\n",
    "    \n",
    "    # Run hyperparameter tuning\n",
    "    results = tuner.tune_hyperparameters(X_train_final, y_train_final, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    # Analyze results\n",
    "    if results:\n",
    "        # Sort by test accuracy (primary) and validation accuracy (secondary)\n",
    "        sorted_by_test = sorted(results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        sorted_by_val = sorted(results, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "        \n",
    "        elapsed = (time.time() - start_time) / 60\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"FOCUSED GHOSTNET TUNING RESULTS - SORTED BY TEST ACCURACY\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{'Rank':<4} {'Combo':<5} {'Validation':<12} {'Test':<8} {'Key Hyperparams'}\")\n",
    "        print('-' * 70)\n",
    "        \n",
    "        for i, result in enumerate(sorted_by_test[:15], 1):  # Top 15 by test\n",
    "            hp = result['hyperparams']\n",
    "            key_params = f\"lr={hp['backbone_lr']}/{hp['classifier_lr']}, bs={hp['batch_size']}, drop={hp['dropout']}, freeze={hp['freeze_layers']}\"\n",
    "            print(f\"{i:2d}. {result['combination']:4d}       {result['val_accuracy']:5.1f}%      {result['test_accuracy']:5.1f}%    {key_params}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TOP RESULTS BY VALIDATION ACCURACY\")\n",
    "        print(\"=\" * 70)\n",
    "        for i, result in enumerate(sorted_by_val[:10], 1):  # Top 10 by val\n",
    "            hp = result['hyperparams']\n",
    "            key_params = f\"lr={hp['backbone_lr']}/{hp['classifier_lr']}, bs={hp['batch_size']}, drop={hp['dropout']}, freeze={hp['freeze_layers']}\"\n",
    "            print(f\"{i:2d}. {result['combination']:4d}       {result['val_accuracy']:5.1f}%      {result['test_accuracy']:5.1f}%    {key_params}\")\n",
    "        \n",
    "        best_test_result = sorted_by_test[0]\n",
    "        best_val_result = sorted_by_val[0]\n",
    "        \n",
    "        print(f\"\\nüéØ BEST TEST ACCURACY: {best_test_result['test_accuracy']:.1f}% (Combo {best_test_result['combination']})\")\n",
    "        print(f\"üèÜ BEST VALIDATION ACCURACY: {best_val_result['val_accuracy']:.1f}% (Combo {best_val_result['combination']})\")\n",
    "        \n",
    "        print(f\"\\nüìä SUMMARY:\")\n",
    "        print(f\"  Best Test: {best_test_result['test_accuracy']:.1f}%\")\n",
    "        print(f\"  Best Validation: {best_val_result['val_accuracy']:.1f}%\")\n",
    "        print(f\"  Tuning Time: {elapsed:.1f} minutes\")\n",
    "        print(f\"  Results saved to: {tuner.save_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No successful combinations found!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dcd71e-cba6-4297-8727-d76a9bbc97f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
