{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3eb0f-fc0c-4f79-bbd0-64a5fe58961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from buck.analysis.basics import ingest_images\n",
    "\n",
    "# Your existing ingestion\n",
    "fpath = \"C:\\\\Users\\\\aaron\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*.png\"\n",
    "images, ages = ingest_images(fpath)\n",
    "print(len(images),'images found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e9606-63d3-422f-8f58-21a2e4ef579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from buck.analysis.basics import split_data\n",
    "\n",
    "Xtr_og, ytr_og, Xval, yval, Xte, yte_onehot, ages, l_map = split_data(images, ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eec27e-965b-411c-8296-27762189ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from buck.analysis.basics import homogenize_data\n",
    "\n",
    "augment_multiplier = 30\n",
    "X_train, y_train, X_test, y_true, label_mapping, num_classes = homogenize_data(Xtr_og, ytr_og, Xte,yte_onehot, l_map, augment_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2dad4-961a-479e-9619-1a78b5561d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class OptimizedEfficientNetTester:\n",
    "    \"\"\"Optimized EfficientNet training with better hyperparameters\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"🚀 Optimized EfficientNet Tester\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Classes: {num_classes}\")\n",
    "    \n",
    "    def create_optimized_model(self, model_name, freeze_backbone=True):\n",
    "        \"\"\"Create EfficientNet with optimized settings\"\"\"\n",
    "        try:\n",
    "            print(f\"   🔧 Creating optimized {model_name}...\")\n",
    "            \n",
    "            # Create model\n",
    "            model = timm.create_model(model_name, pretrained=True, num_classes=self.num_classes)\n",
    "            \n",
    "            # Option 1: Freeze backbone, only train classifier\n",
    "            if freeze_backbone:\n",
    "                print(f\"      🧊 Freezing backbone layers...\")\n",
    "                for name, param in model.named_parameters():\n",
    "                    if 'classifier' not in name and 'head' not in name:\n",
    "                        param.requires_grad = False\n",
    "                \n",
    "                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"      ✅ Loaded: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "            else:\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"      ✅ Loaded: {total_params:,} parameters (all trainable)\")\n",
    "            \n",
    "            model = model.to(self.device)\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ Failed: {str(e)[:50]}...\")\n",
    "            return None\n",
    "    \n",
    "    def train_optimized(self, model, arch_name, train_loader, val_loader, test_loader):\n",
    "        \"\"\"Optimized training with better hyperparameters\"\"\"\n",
    "        print(f\"   🚀 Training {arch_name} (OPTIMIZED)...\")\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing helps\n",
    "        \n",
    "        # Better optimizer settings\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=0.01,  # Higher learning rate for faster convergence\n",
    "            weight_decay=0.01,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        # Cosine annealing scheduler\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        patience = 8  # More patience\n",
    "        max_epochs = 25  # More epochs\n",
    "        \n",
    "        print(f\"      📊 Training setup: {max_epochs} max epochs, patience={patience}\")\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                improvement = \"🔥\"\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                improvement = \"\"\n",
    "            \n",
    "            # More frequent progress updates\n",
    "            if epoch % 2 == 0 or epoch < 5 or improvement:\n",
    "                gap = train_acc - val_acc\n",
    "                print(f\"      Epoch {epoch:2d}: Train {train_acc:.1f}%, Val {val_acc:.1f}% (gap: {gap:+.1f}%), LR: {current_lr:.2e} {improvement}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"      Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Restore best model and evaluate on test\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Test evaluation\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        print(f\"      🎯 {arch_name} FINAL: Val {best_val_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "        \n",
    "        return best_val_acc, test_acc\n",
    "    \n",
    "    def test_efficientnet_series(self, train_loader, val_loader, test_loader):\n",
    "        \"\"\"Test EfficientNet B0-B7 with optimized training\"\"\"\n",
    "        print(\"🚀 OPTIMIZED EFFICIENTNET B0-B7 TESTING\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"🎯 IMPROVEMENTS:\")\n",
    "        print(\"   • Higher learning rate (0.01 vs 0.001)\")\n",
    "        print(\"   • Cosine annealing scheduler\") \n",
    "        print(\"   • Label smoothing (0.1)\")\n",
    "        print(\"   • Gradient clipping\")\n",
    "        print(\"   • More epochs (25 vs 12)\")\n",
    "        print(\"   • More patience (8 vs 4)\")\n",
    "        print(\"   • Option to freeze backbone\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # EfficientNet models to test\n",
    "        models = [\n",
    "            ('EfficientNet-B0', 'efficientnet_b0'),\n",
    "            ('EfficientNet-B1', 'efficientnet_b1'),\n",
    "            ('EfficientNet-B2', 'efficientnet_b2'),\n",
    "            ('EfficientNet-B3', 'efficientnet_b3'),\n",
    "            ('EfficientNet-B4', 'efficientnet_b4'),\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, (arch_name, model_name) in enumerate(models, 1):\n",
    "            print(f\"\\n[{i}/{len(models)}] 🎯 OPTIMIZED {arch_name}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Test with frozen backbone first\n",
    "            print(\"   🧊 FROZEN BACKBONE VERSION:\")\n",
    "            model_frozen = self.create_optimized_model(model_name, freeze_backbone=True)\n",
    "            \n",
    "            if model_frozen is not None:\n",
    "                try:\n",
    "                    val_acc_frozen, test_acc_frozen = self.train_optimized(\n",
    "                        model_frozen, f\"{arch_name}-Frozen\", train_loader, val_loader, test_loader\n",
    "                    )\n",
    "                    \n",
    "                    results.append({\n",
    "                        'name': f\"{arch_name}-Frozen\",\n",
    "                        'val_accuracy': val_acc_frozen,\n",
    "                        'test_accuracy': test_acc_frozen,\n",
    "                        'training_time': time.time() - start_time\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      ❌ Frozen training failed: {str(e)[:50]}...\")\n",
    "            \n",
    "            # Test with full fine-tuning if frozen version works well\n",
    "            if 'val_acc_frozen' in locals() and val_acc_frozen > 40:\n",
    "                print(f\"   🔥 FULL FINE-TUNING VERSION (frozen got {val_acc_frozen:.1f}%):\")\n",
    "                model_full = self.create_optimized_model(model_name, freeze_backbone=False)\n",
    "                \n",
    "                if model_full is not None:\n",
    "                    try:\n",
    "                        # Use lower learning rate for full fine-tuning\n",
    "                        optimizer_full = optim.AdamW(model_full.parameters(), lr=0.001, weight_decay=0.01)\n",
    "                        \n",
    "                        val_acc_full, test_acc_full = self.train_optimized(\n",
    "                            model_full, f\"{arch_name}-FullFT\", train_loader, val_loader, test_loader\n",
    "                        )\n",
    "                        \n",
    "                        results.append({\n",
    "                            'name': f\"{arch_name}-FullFT\", \n",
    "                            'val_accuracy': val_acc_full,\n",
    "                            'test_accuracy': test_acc_full,\n",
    "                            'training_time': time.time() - start_time\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"      ❌ Full fine-tuning failed: {str(e)[:50]}...\")\n",
    "            \n",
    "            print(f\"   ⏱️ Total time for {arch_name}: {time.time() - start_time:.1f}s\")\n",
    "        \n",
    "        # Sort results\n",
    "        results.sort(key=lambda x: x['test_accuracy'], reverse=True)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n🏆 OPTIMIZED EFFICIENTNET RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"{'Rank':<4} {'Architecture':<25} {'Val%':<8} {'Test%':<8} {'vs 54.2%'}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            val_acc = result['val_accuracy']\n",
    "            test_acc = result['test_accuracy']\n",
    "            \n",
    "            if test_acc >= 70.0:\n",
    "                status = \"🎉 BREAKTHROUGH!\"\n",
    "            elif test_acc > 54.2:\n",
    "                status = \"🔥 NEW BEST!\"\n",
    "            elif test_acc > 45.0:\n",
    "                status = \"📈 Good\"\n",
    "            else:\n",
    "                status = \"📉 Poor\"\n",
    "            \n",
    "            print(f\"{i:<4} {result['name']:<25} {val_acc:<7.1f} {test_acc:<7.1f} {status}\")\n",
    "        \n",
    "        if results:\n",
    "            best = results[0]\n",
    "            print(f\"\\n🎉 BEST OPTIMIZED EFFICIENTNET:\")\n",
    "            print(f\"   🏆 {best['name']}: {best['test_accuracy']:.1f}% test accuracy\")\n",
    "            \n",
    "            if best['test_accuracy'] > 54.2:\n",
    "                improvement = best['test_accuracy'] - 54.2\n",
    "                print(f\"   🚀 IMPROVEMENT: +{improvement:.1f}% over baseline!\")\n",
    "            else:\n",
    "                print(f\"   💡 Still below 54.2% baseline - trying other optimizations...\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        return results\n",
    "\n",
    "def run_optimized_efficientnet_test(train_loader, val_loader, test_loader):\n",
    "    \"\"\"Run optimized EfficientNet testing\"\"\"\n",
    "    tester = OptimizedEfficientNetTester(num_classes=5)\n",
    "    return tester.test_efficientnet_series(train_loader, val_loader, test_loader)\n",
    "\n",
    "# To use this with your existing data loaders:\n",
    "print(\"🚀 READY FOR OPTIMIZED EFFICIENTNET TESTING!\")\n",
    "print(\"Run: optimized_results = run_optimized_efficientnet_test(train_loader, val_loader, test_loader)\")\n",
    "optimized_results = run_optimized_efficientnet_test(train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915e676-4b9d-4e01-847d-72414c725aa9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CUDA Debugger\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def test_cuda_setup():\n",
    "    \"\"\"Test CUDA setup thoroughly\"\"\"\n",
    "    print(\"🔍 CUDA DIAGNOSTICS:\")\n",
    "    print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"   Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"   Device name: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"   Device memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "        # Test basic tensor operations\n",
    "        print(\"   Testing GPU tensor operations...\")\n",
    "        try:\n",
    "            x = torch.randn(100, 100).cuda()\n",
    "            y = torch.randn(100, 100).cuda()\n",
    "            z = torch.mm(x, y)\n",
    "            print(\"   ✅ Basic GPU operations working\")\n",
    "            \n",
    "            # Check memory usage\n",
    "            print(f\"   GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.3f} GB\")\n",
    "            print(f\"   GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.3f} GB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ GPU test failed: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"   ❌ CUDA not available - will use CPU\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "class DebugDeerDataset(Dataset):\n",
    "    \"\"\"Debug version with extensive logging\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, is_training=True):\n",
    "        print(f\"🔍 Creating dataset...\")\n",
    "        print(f\"   Input X type: {type(X)}, shape: {X.shape if hasattr(X, 'shape') else 'unknown'}\")\n",
    "        print(f\"   Input y type: {type(y)}, shape: {y.shape if hasattr(y, 'shape') else 'unknown'}\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        if isinstance(X, np.ndarray):\n",
    "            print(\"   Converting X from numpy to tensor...\")\n",
    "            X = torch.FloatTensor(X)\n",
    "        if isinstance(y, np.ndarray):\n",
    "            print(\"   Converting y from numpy to tensor...\")\n",
    "            y = torch.LongTensor(y)\n",
    "            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        print(f\"   Final X tensor shape: {X.shape}\")\n",
    "        print(f\"   Final y tensor shape: {y.shape}\")\n",
    "        print(f\"   Dataset length: {len(X)}\")\n",
    "        print(f\"   Is training: {is_training}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:  # Log first item details\n",
    "            print(f\"🔍 Getting first item (idx={idx})...\")\n",
    "            \n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(f\"   Original image shape: {image.shape}\")\n",
    "            print(f\"   Image dtype: {image.dtype}\")\n",
    "            print(f\"   Image min/max: {image.min():.3f}/{image.max():.3f}\")\n",
    "            print(f\"   Label: {label}\")\n",
    "        \n",
    "        # Normalize\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "            \n",
    "        # Permute to CHW\n",
    "        image = image.permute(2, 0, 1)  # (3, 288, 288)\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(f\"   After permute shape: {image.shape}\")\n",
    "        \n",
    "        # Simple augmentation\n",
    "        if self.is_training and torch.rand(1) > 0.5:\n",
    "            image = torch.flip(image, dims=[2])\n",
    "            \n",
    "        # Resize to 224x224\n",
    "        image = image.unsqueeze(0)  # (1, 3, 288, 288)\n",
    "        image = F.interpolate(image, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        image = image.squeeze(0)  # (3, 224, 224)\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(f\"   After resize shape: {image.shape}\")\n",
    "        \n",
    "        # Normalize\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        image = (image - mean) / std\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(f\"   Final normalized shape: {image.shape}\")\n",
    "            print(f\"   Final min/max: {image.min():.3f}/{image.max():.3f}\")\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class DebugEfficientNet:\n",
    "    \"\"\"Debug version with step-by-step logging\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        print(f\"🔍 Creating EfficientNet model...\")\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        \n",
    "        self.create_model()\n",
    "        \n",
    "    def create_model(self):\n",
    "        print(\"   Loading EfficientNet-B0...\")\n",
    "        try:\n",
    "            self.model = timm.create_model(\n",
    "                'efficientnet_b0',\n",
    "                pretrained=True,\n",
    "                num_classes=self.num_classes,\n",
    "                drop_rate=0.1\n",
    "            )\n",
    "            print(\"   ✅ Model created successfully\")\n",
    "            \n",
    "            print(\"   Moving model to device...\")\n",
    "            self.model = self.model.to(self.device)\n",
    "            print(\"   ✅ Model moved to device\")\n",
    "            \n",
    "            # Test model with dummy input\n",
    "            print(\"   Testing model with dummy input...\")\n",
    "            dummy_input = torch.randn(2, 3, 224, 224).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                dummy_output = self.model(dummy_input)\n",
    "            print(f\"   ✅ Model test successful. Output shape: {dummy_output.shape}\")\n",
    "            \n",
    "            param_count = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f\"   Model parameters: {param_count:,}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Model creation failed: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def test_dataloader(self, train_loader):\n",
    "        \"\"\"Test if dataloader works\"\"\"\n",
    "        print(\"🔍 Testing dataloader...\")\n",
    "        try:\n",
    "            print(\"   Getting first batch...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                print(f\"   Batch {batch_idx}: images {images.shape}, labels {labels.shape}\")\n",
    "                print(f\"   Images dtype: {images.dtype}, Labels dtype: {labels.dtype}\")\n",
    "                print(f\"   Moving to device...\")\n",
    "                \n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                print(f\"   ✅ Successfully moved batch to {self.device}\")\n",
    "                print(f\"   Memory allocated: {torch.cuda.memory_allocated() / 1e9:.3f} GB\")\n",
    "                \n",
    "                # Test model forward pass\n",
    "                print(\"   Testing model forward pass...\")\n",
    "                start_forward = time.time()\n",
    "                outputs = self.model(images)\n",
    "                forward_time = time.time() - start_forward\n",
    "                \n",
    "                print(f\"   ✅ Forward pass successful in {forward_time:.3f}s\")\n",
    "                print(f\"   Output shape: {outputs.shape}\")\n",
    "                print(f\"   Memory after forward: {torch.cuda.memory_allocated() / 1e9:.3f} GB\")\n",
    "                \n",
    "                if batch_idx >= 1:  # Test 2 batches\n",
    "                    break\n",
    "                    \n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"   ✅ Dataloader test completed in {total_time:.3f}s\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Dataloader test failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def prepare_data_loaders(self, X_train, y_train, X_val, y_val, batch_size=16):\n",
    "        print(\"🔍 Preparing data loaders...\")\n",
    "        \n",
    "        train_dataset = DebugDeerDataset(X_train, y_train, is_training=True)\n",
    "        val_dataset = DebugDeerDataset(X_val, y_val, is_training=False)\n",
    "        \n",
    "        print(f\"   Creating train loader with batch_size={batch_size}...\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                num_workers=0, drop_last=True)\n",
    "        \n",
    "        print(f\"   Creating val loader...\")\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                              num_workers=0)\n",
    "        \n",
    "        print(f\"   ✅ Loaders created:\")\n",
    "        print(f\"      Train: {len(train_loader)} batches\")\n",
    "        print(f\"      Val: {len(val_loader)} batches\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def quick_train_test(self, train_loader, val_loader):\n",
    "        \"\"\"Quick training test - just 2 epochs\"\"\"\n",
    "        print(\"🔍 Quick training test (2 epochs)...\")\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(2):\n",
    "            print(f\"   Epoch {epoch+1}/2...\")\n",
    "            self.model.train()\n",
    "            \n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                start_batch = time.time()\n",
    "                \n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                batch_time = time.time() - start_batch\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    acc = 100. * correct / total\n",
    "                    print(f\"      Batch {batch_idx}: Loss {loss.item():.4f}, Acc {acc:.1f}%, Time {batch_time:.3f}s\")\n",
    "                    print(f\"      GPU Memory: {torch.cuda.memory_allocated() / 1e9:.3f} GB\")\n",
    "                \n",
    "                if batch_idx >= 20:  # Test just 20 batches\n",
    "                    break\n",
    "            \n",
    "            epoch_acc = 100. * correct / total\n",
    "            epoch_loss = total_loss / min(21, len(train_loader))\n",
    "            print(f\"   ✅ Epoch {epoch+1} completed: Loss {epoch_loss:.4f}, Acc {epoch_acc:.1f}%\")\n",
    "        \n",
    "        print(\"   ✅ Quick training test successful!\")\n",
    "\n",
    "def debug_deer_classification(X_train, y_train, X_val, y_val, X_test, y_true, label_mapping):\n",
    "    \"\"\"Complete debug run\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🔍 DEBUGGING DEER AGE CLASSIFICATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Test CUDA\n",
    "    cuda_works = test_cuda_setup()\n",
    "    \n",
    "    # Step 2: Check data\n",
    "    print(f\"\\n🔍 DATA SHAPES:\")\n",
    "    print(f\"   Train: {X_train.shape}\")\n",
    "    print(f\"   Val: {X_val.shape}\")\n",
    "    print(f\"   Test: {X_test.shape}\")\n",
    "    print(f\"   Classes: {len(label_mapping)}\")\n",
    "    \n",
    "    # Step 3: Fix labels\n",
    "    print(f\"\\n🔍 FIXING LABELS:\")\n",
    "    if len(y_train.shape) == 2:\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "        print(\"   Fixed y_train from one-hot\")\n",
    "    if len(y_val.shape) == 2:\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "        print(\"   Fixed y_val from one-hot\")\n",
    "    if len(y_true.shape) == 2:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        print(\"   Fixed y_true from one-hot\")\n",
    "    \n",
    "    print(f\"   Final label shapes: train {y_train.shape}, val {y_val.shape}, test {y_true.shape}\")\n",
    "    \n",
    "    # Step 4: Create model\n",
    "    print(f\"\\n🔍 CREATING MODEL:\")\n",
    "    classifier = DebugEfficientNet(num_classes=len(label_mapping))\n",
    "    \n",
    "    # Step 5: Create data loaders\n",
    "    print(f\"\\n🔍 CREATING DATA LOADERS:\")\n",
    "    train_loader, val_loader = classifier.prepare_data_loaders(\n",
    "        X_train, y_train, X_val, y_val, batch_size=16\n",
    "    )\n",
    "    \n",
    "    # Step 6: Test data loader\n",
    "    print(f\"\\n🔍 TESTING DATA LOADING:\")\n",
    "    dataloader_works = classifier.test_dataloader(train_loader)\n",
    "    \n",
    "    if dataloader_works:\n",
    "        # Step 7: Quick training test\n",
    "        print(f\"\\n🔍 TESTING TRAINING:\")\n",
    "        classifier.quick_train_test(train_loader, val_loader)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"🎉 ALL TESTS PASSED! System is working correctly.\")\n",
    "        print(\"   You can now run full training with confidence.\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"❌ ISSUE FOUND IN DATA LOADING\")\n",
    "        print(\"   Check the error messages above to identify the problem.\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Run the debug version\n",
    "debug_deer_classification(X_train, y_train, Xval, yval, X_test, y_true, label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069baa4-1877-4af5-af40-80130e7a3711",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Corrected model test\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import timm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_trained_model(model_path='best_deer_efficientnet.pth', num_classes=5):\n",
    "    \"\"\"\n",
    "    Load the saved deer age classification model\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Loading trained model from {model_path}...\")\n",
    "    \n",
    "    # Recreate the model architecture (same as in training)\n",
    "    model = timm.create_model(\n",
    "        'efficientnet_b4',\n",
    "        pretrained=False,  # Don't download pretrained weights\n",
    "        num_classes=num_classes,\n",
    "        drop_rate=0.3,\n",
    "        drop_path_rate=0.2\n",
    "    )\n",
    "    \n",
    "    # Replace classifier (same as in training)\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(p=0.4, inplace=True),\n",
    "        torch.nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Load the saved weights\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✅ Model loaded successfully on {device}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model_correct_preprocessing(model, X_test, y_true, label_mapping):\n",
    "    \"\"\"\n",
    "    Evaluate model with CORRECT preprocessing (no ImageNet normalization)\n",
    "    \"\"\"\n",
    "    print(\"🎯 EVALUATING MODEL ON TEST SET - CORRECTED PREPROCESSING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Convert one-hot to class indices if needed\n",
    "    if len(y_true.shape) == 2:\n",
    "        print(f\"Converting one-hot labels {y_true.shape} to class indices...\")\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        print(f\"✅ Labels now: {y_true.shape}\")\n",
    "    \n",
    "    # Create reverse mapping\n",
    "    class_to_age = {v: k for k, v in label_mapping.items()}\n",
    "    \n",
    "    print(f\"Test set size: {len(X_test)} images\")\n",
    "    print(f\"Number of classes: {len(label_mapping)}\")\n",
    "    print(f\"Age mapping: {label_mapping}\")\n",
    "    print(f\"Class to age: {class_to_age}\")\n",
    "    \n",
    "    # Get device\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    print(\"\\n🔄 Making predictions with CORRECT preprocessing...\")\n",
    "    print(\"   ✅ Using raw 0-1 normalized pixels (NO ImageNet normalization)\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 16\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            batch_end = min(i + batch_size, len(X_test))\n",
    "            batch_images = X_test[i:batch_end]\n",
    "            \n",
    "            # Convert to tensor - same as training\n",
    "            batch_tensor = torch.FloatTensor(batch_images).permute(0, 3, 1, 2).to(device)\n",
    "            \n",
    "            # Resize to model input size (384x384 for EfficientNet-B4)\n",
    "            batch_tensor = F.interpolate(batch_tensor, size=(384, 384), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # ✅ NO IMAGENET NORMALIZATION - just use raw 0-1 pixel values\n",
    "            # This matches what your model was actually trained on!\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = model(batch_tensor)\n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            predicted_classes = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # Convert to Python lists\n",
    "            all_predictions.extend(predicted_classes.tolist())\n",
    "            all_probabilities.extend(probabilities.tolist())\n",
    "            \n",
    "            if i % (batch_size * 5) == 0:\n",
    "                print(f\"   Processed {batch_end}/{len(X_test)} images...\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    y_pred = np.array(all_predictions)\n",
    "    y_proba = np.array(all_probabilities)\n",
    "    \n",
    "    print(f\"✅ Predictions complete!\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\n📊 CALCULATING METRICS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"🎯 Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # F1 Scores\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    print(f\"📈 F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"📈 F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"📈 F1 Score (Micro): {f1_micro:.4f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(f\"\\n📋 DETAILED CLASSIFICATION REPORT:\")\n",
    "    age_labels = [class_to_age[i] for i in range(len(label_mapping))]\n",
    "    target_names = [f\"{age} years\" for age in age_labels]\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(f\"\\n🔍 CONFUSION MATRIX:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"Predicted →\")\n",
    "    print(\"True ↓   \", \"  \".join([f\"{age:4.1f}\" for age in age_labels]))\n",
    "    for i, age in enumerate(age_labels):\n",
    "        row_str = \" \".join([f\"{cm[i][j]:4d}\" for j in range(len(age_labels))])\n",
    "        print(f\"{age:4.1f}     {row_str}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[f\"{age}y\" for age in age_labels],\n",
    "                yticklabels=[f\"{age}y\" for age in age_labels])\n",
    "    plt.title('Confusion Matrix - Deer Age Classification (Corrected)')\n",
    "    plt.xlabel('Predicted Age')\n",
    "    plt.ylabel('True Age')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Age-based accuracy analysis\n",
    "    print(f\"\\n🦌 AGE-SPECIFIC ANALYSIS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for class_idx in range(len(label_mapping)):\n",
    "        age = class_to_age[class_idx]\n",
    "        \n",
    "        # Get indices for this age class\n",
    "        true_class_mask = (y_true == class_idx)\n",
    "        if np.sum(true_class_mask) > 0:\n",
    "            class_accuracy = np.sum((y_true == class_idx) & (y_pred == class_idx)) / np.sum(true_class_mask)\n",
    "            class_count = np.sum(true_class_mask)\n",
    "            class_f1 = report[f\"{age} years\"]['f1-score']\n",
    "            class_precision = report[f\"{age} years\"]['precision']\n",
    "            class_recall = report[f\"{age} years\"]['recall']\n",
    "            \n",
    "            print(f\"Age {age} years ({class_count} samples):\")\n",
    "            print(f\"   Accuracy: {class_accuracy:.3f} ({class_accuracy*100:.1f}%)\")\n",
    "            print(f\"   Precision: {class_precision:.3f}\")\n",
    "            print(f\"   Recall: {class_recall:.3f}\")\n",
    "            print(f\"   F1-Score: {class_f1:.3f}\")\n",
    "    \n",
    "    # Tolerance-based accuracy\n",
    "    print(f\"\\n🎯 TOLERANCE-BASED ACCURACY:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Convert class indices back to actual ages\n",
    "    y_true_ages = [class_to_age[idx] for idx in y_true]\n",
    "    y_pred_ages = [class_to_age[idx] for idx in y_pred]\n",
    "    \n",
    "    exact_matches = sum(1 for true, pred in zip(y_true_ages, y_pred_ages) if true == pred)\n",
    "    within_1_year = sum(1 for true, pred in zip(y_true_ages, y_pred_ages) if abs(true - pred) <= 1.0)\n",
    "    \n",
    "    exact_accuracy = exact_matches / len(y_true_ages)\n",
    "    tolerance_accuracy = within_1_year / len(y_true_ages)\n",
    "    \n",
    "    print(f\"Exact age accuracy: {exact_accuracy:.3f} ({exact_accuracy*100:.1f}%)\")\n",
    "    print(f\"Within ±1 year accuracy: {tolerance_accuracy:.3f} ({tolerance_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # Show prediction distribution\n",
    "    print(f\"\\n📊 PREDICTION DISTRIBUTION:\")\n",
    "    print(\"-\" * 30)\n",
    "    pred_counts = {}\n",
    "    for pred in y_pred:\n",
    "        age = class_to_age[pred]\n",
    "        pred_counts[age] = pred_counts.get(age, 0) + 1\n",
    "    \n",
    "    for age in sorted(pred_counts.keys()):\n",
    "        count = pred_counts[age]\n",
    "        print(f\"   Predicted {age} years: {count} times ({count/len(y_pred)*100:.1f}%)\")\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(f\"\\n🔍 SAMPLE PREDICTIONS:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i in range(min(15, len(y_true))):\n",
    "        true_age = y_true_ages[i]\n",
    "        pred_age = y_pred_ages[i]\n",
    "        confidence = max(y_proba[i])\n",
    "        status = \"✅\" if true_age == pred_age else \"❌\"\n",
    "        print(f\"{status} Sample {i+1}: True {true_age}y → Predicted {pred_age}y (confidence: {confidence:.2%})\")\n",
    "    \n",
    "    # Return results\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_micro': f1_micro,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_proba,\n",
    "        'exact_accuracy': exact_accuracy,\n",
    "        'tolerance_accuracy': tolerance_accuracy\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_corrected_evaluation(X_test, y_true, label_mapping, model_path='best_deer_efficientnet.pth'):\n",
    "    \"\"\"\n",
    "    Complete corrected evaluation pipeline\n",
    "    \"\"\"\n",
    "    print(\"🦌 DEER AGE CLASSIFICATION: CORRECTED MODEL EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"🔧 Using CORRECT preprocessing (no ImageNet normalization)\")\n",
    "    print(\"   This matches what your model was actually trained on!\")\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = load_trained_model(model_path, num_classes=len(label_mapping))\n",
    "    \n",
    "    # Evaluate the model with correct preprocessing\n",
    "    results = evaluate_model_correct_preprocessing(model, X_test, y_true, label_mapping)\n",
    "    \n",
    "    print(\"\\n✅ CORRECTED EVALUATION COMPLETE!\")\n",
    "    print(f\"📊 Final Results Summary:\")\n",
    "    print(f\"   Test Accuracy: {results['accuracy']:.3f} ({results['accuracy']*100:.1f}%)\")\n",
    "    print(f\"   F1 Score (Macro): {results['f1_macro']:.3f}\")\n",
    "    print(f\"   F1 Score (Weighted): {results['f1_weighted']:.3f}\")\n",
    "    print(f\"   Exact Age Accuracy: {results['exact_accuracy']:.3f} ({results['exact_accuracy']*100:.1f}%)\")\n",
    "    print(f\"   Within ±1 Year Accuracy: {results['tolerance_accuracy']:.3f} ({results['tolerance_accuracy']*100:.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the CORRECTED evaluation\n",
    "results = run_corrected_evaluation(X_test, y_true, label_mapping, 'best_deer_efficientnet.pth')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0acd6f-d97b-407a-859f-c400767d63d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imaging where the model is looking\n",
    "'''\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import timm\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Grad-CAM implementation for visualizing what features the model focuses on\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer_name):\n",
    "        self.model = model\n",
    "        self.target_layer_name = target_layer_name\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        self.register_hooks()\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register forward and backward hooks to capture gradients and activations\"\"\"\n",
    "        \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "        \n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "        \n",
    "        # Find the target layer\n",
    "        target_layer = None\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name == self.target_layer_name:\n",
    "                target_layer = module\n",
    "                break\n",
    "        \n",
    "        if target_layer is None:\n",
    "            raise ValueError(f\"Layer {self.target_layer_name} not found in model\")\n",
    "        \n",
    "        # Register hooks\n",
    "        target_layer.register_forward_hook(forward_hook)\n",
    "        target_layer.register_backward_hook(backward_hook)\n",
    "    \n",
    "    def generate_cam(self, image_tensor, class_idx=None):\n",
    "        \"\"\"\n",
    "        Generate Grad-CAM for a given image\n",
    "        \n",
    "        Args:\n",
    "            image_tensor: Input image tensor (1, 3, H, W)\n",
    "            class_idx: Target class index (if None, uses predicted class)\n",
    "        \n",
    "        Returns:\n",
    "            cam: Grad-CAM heatmap\n",
    "            prediction: Model prediction\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        self.model.eval()\n",
    "        output = self.model(image_tensor)\n",
    "        \n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        target = output[0][class_idx]\n",
    "        target.backward()\n",
    "        \n",
    "        # Get gradients and activations\n",
    "        gradients = self.gradients[0]  # (C, H, W)\n",
    "        activations = self.activations[0]  # (C, H, W)\n",
    "        \n",
    "        # Calculate weights (global average pooling of gradients)\n",
    "        weights = torch.mean(gradients, dim=(1, 2))  # (C,)\n",
    "        \n",
    "        # Generate CAM\n",
    "        cam = torch.zeros(activations.shape[1:], dtype=torch.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "        \n",
    "        # Apply ReLU and normalize\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam / cam.max() if cam.max() > 0 else cam\n",
    "        \n",
    "        return cam.detach().cpu().numpy(), output.detach().cpu().numpy()\n",
    "\n",
    "def load_trained_model(model_path='best_deer_efficientnet.pth', num_classes=5):\n",
    "    \"\"\"\n",
    "    Load the saved deer age classification model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to saved model weights\n",
    "        num_classes: Number of age classes\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded model ready for inference\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Loading trained model from {model_path}...\")\n",
    "    \n",
    "    # Recreate the model architecture (same as in training)\n",
    "    model = timm.create_model(\n",
    "        'efficientnet_b4',\n",
    "        pretrained=False,  # Don't download pretrained weights\n",
    "        num_classes=num_classes,\n",
    "        drop_rate=0.3,\n",
    "        drop_path_rate=0.2\n",
    "    )\n",
    "    \n",
    "    # Replace classifier (same as in training)\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(p=0.4, inplace=True),\n",
    "        torch.nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Load the saved weights\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✅ Model loaded successfully on {device}\")\n",
    "    return model\n",
    "\n",
    "def visualize_deer_features(model, X_test, y_true, label_mapping, num_images=6):\n",
    "    \"\"\"\n",
    "    Visualize which features the model focuses on for deer age prediction\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded trained model\n",
    "        X_test: Test images\n",
    "        y_true: True labels (as class indices, not one-hot)\n",
    "        label_mapping: Age to class mapping\n",
    "        num_images: Number of images to visualize\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert one-hot to class indices if needed\n",
    "    if len(y_true.shape) == 2:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    # Create reverse mapping\n",
    "    class_to_age = {v: k for k, v in label_mapping.items()}\n",
    "    \n",
    "    # Initialize Grad-CAM (target the last convolutional layer)\n",
    "    grad_cam = GradCAM(model, target_layer_name='features.8.2.block.2')\n",
    "    \n",
    "    # Select random test images\n",
    "    indices = np.random.choice(len(X_test), num_images, replace=False)\n",
    "    \n",
    "    # Create custom colormap for heatmap\n",
    "    colors = ['blue', 'cyan', 'yellow', 'orange', 'red']\n",
    "    n_bins = 256\n",
    "    cmap = LinearSegmentedColormap.from_list('grad_cam', colors, N=n_bins)\n",
    "    \n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(20, 8))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Prepare image\n",
    "        original_image = X_test[idx]\n",
    "        true_age = class_to_age[y_true[idx]]\n",
    "        \n",
    "        # Convert to tensor and add batch dimension\n",
    "        image_tensor = torch.FloatTensor(original_image).permute(2, 0, 1).unsqueeze(0)\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        \n",
    "        # Resize to model input size (384x384 for EfficientNet-B4)\n",
    "        image_tensor = F.interpolate(image_tensor, size=(384, 384), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Normalize (ImageNet normalization)\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "        image_tensor = (image_tensor - mean) / std\n",
    "        \n",
    "        # Generate Grad-CAM\n",
    "        cam, prediction = grad_cam.generate_cam(image_tensor)\n",
    "        predicted_class = np.argmax(prediction[0])\n",
    "        predicted_age = class_to_age[predicted_class]\n",
    "        confidence = F.softmax(torch.tensor(prediction[0]), dim=0)[predicted_class].item()\n",
    "        \n",
    "        # Resize CAM to original image size\n",
    "        cam_resized = cv2.resize(cam, (original_image.shape[1], original_image.shape[0]))\n",
    "        \n",
    "        # Display original image\n",
    "        axes[0, i].imshow(original_image)\n",
    "        axes[0, i].set_title(f'Original Image\\nTrue Age: {true_age} years', fontsize=12)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Display Grad-CAM overlay\n",
    "        axes[1, i].imshow(original_image)\n",
    "        axes[1, i].imshow(cam_resized, cmap=cmap, alpha=0.5, vmin=0, vmax=1)\n",
    "        axes[1, i].set_title(f'Predicted: {predicted_age} years\\nConfidence: {confidence:.2%}', fontsize=12)\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        print(f\"Image {i+1}: True={true_age}y, Predicted={predicted_age}y, Confidence={confidence:.1%}\")\n",
    "    \n",
    "    plt.suptitle('Deer Age Classification: Feature Importance Visualization', fontsize=16, y=0.95)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a colorbar legend\n",
    "    fig, ax = plt.subplots(figsize=(8, 1))\n",
    "    gradient = np.linspace(0, 1, 256).reshape(1, -1)\n",
    "    ax.imshow(gradient, aspect='auto', cmap=cmap)\n",
    "    ax.set_xlim(0, 256)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([0, 64, 128, 192, 256])\n",
    "    ax.set_xticklabels(['Low', 'Medium-Low', 'Medium', 'Medium-High', 'High'])\n",
    "    ax.set_xlabel('Feature Importance for Age Prediction')\n",
    "    ax.set_title('Grad-CAM Heatmap Legend: Red = High Importance, Blue = Low Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_feature_patterns(model, X_test, y_true, label_mapping, samples_per_age=3):\n",
    "    \"\"\"\n",
    "    Analyze what features the model focuses on for each age group\n",
    "    \"\"\"\n",
    "    # Convert one-hot to class indices if needed\n",
    "    if len(y_true.shape) == 2:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    class_to_age = {v: k for k, v in label_mapping.items()}\n",
    "    grad_cam = GradCAM(model, target_layer_name='features.8.2.block.2')\n",
    "    \n",
    "    print(\"🔍 ANALYZING FEATURE PATTERNS BY AGE GROUP\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Group images by age\n",
    "    age_groups = {}\n",
    "    for i, label in enumerate(y_true):\n",
    "        age = class_to_age[label]\n",
    "        if age not in age_groups:\n",
    "            age_groups[age] = []\n",
    "        age_groups[age].append(i)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    for age in sorted(age_groups.keys()):\n",
    "        print(f\"\\n📊 AGE GROUP: {age} YEARS\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Sample images from this age group\n",
    "        indices = np.random.choice(age_groups[age], min(samples_per_age, len(age_groups[age])), replace=False)\n",
    "        \n",
    "        cam_averages = []\n",
    "        \n",
    "        for idx in indices:\n",
    "            # Prepare image\n",
    "            original_image = X_test[idx]\n",
    "            image_tensor = torch.FloatTensor(original_image).permute(2, 0, 1).unsqueeze(0)\n",
    "            image_tensor = image_tensor.to(device)\n",
    "            image_tensor = F.interpolate(image_tensor, size=(384, 384), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Normalize\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "            image_tensor = (image_tensor - mean) / std\n",
    "            \n",
    "            # Generate CAM\n",
    "            cam, prediction = grad_cam.generate_cam(image_tensor)\n",
    "            predicted_class = np.argmax(prediction[0])\n",
    "            predicted_age = class_to_age[predicted_class]\n",
    "            confidence = F.softmax(torch.tensor(prediction[0]), dim=0)[predicted_class].item()\n",
    "            \n",
    "            cam_averages.append(cam)\n",
    "            \n",
    "            print(f\"   Sample {len(cam_averages)}: Predicted {predicted_age}y (confidence: {confidence:.1%})\")\n",
    "        \n",
    "        # Calculate average attention pattern for this age group\n",
    "        avg_cam = np.mean(cam_averages, axis=0)\n",
    "        \n",
    "        # Analyze where the model focuses\n",
    "        height, width = avg_cam.shape\n",
    "        \n",
    "        # Divide image into regions and calculate average attention\n",
    "        regions = {\n",
    "            'Top (Head/Antlers)': avg_cam[:height//3, :].mean(),\n",
    "            'Middle (Body)': avg_cam[height//3:2*height//3, :].mean(), \n",
    "            'Bottom (Legs)': avg_cam[2*height//3:, :].mean(),\n",
    "            'Left Side': avg_cam[:, :width//2].mean(),\n",
    "            'Right Side': avg_cam[:, width//2:].mean(),\n",
    "            'Center': avg_cam[height//4:3*height//4, width//4:3*width//4].mean()\n",
    "        }\n",
    "        \n",
    "        print(f\"   🎯 Key focus areas:\")\n",
    "        for region, attention in sorted(regions.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"      {region}: {attention:.3f}\")\n",
    "\n",
    "def run_gradcam_analysis(X_test, y_true, label_mapping, model_path='best_deer_efficientnet.pth'):\n",
    "    \"\"\"\n",
    "    Complete pipeline to load model and run Grad-CAM analysis\n",
    "    \n",
    "    Args:\n",
    "        X_test: Test images\n",
    "        y_true: True labels\n",
    "        label_mapping: Age to class mapping dict\n",
    "        model_path: Path to saved model weights\n",
    "    \"\"\"\n",
    "    print(\"🦌 DEER AGE CLASSIFICATION: GRAD-CAM FEATURE ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = load_trained_model(model_path, num_classes=len(label_mapping))\n",
    "    \n",
    "    # Run Grad-CAM visualization\n",
    "    print(\"\\n📸 Generating Grad-CAM visualizations...\")\n",
    "    visualize_deer_features(model, X_test, y_true, label_mapping, num_images=6)\n",
    "    \n",
    "    # Analyze patterns by age group\n",
    "    print(\"\\n🔍 Analyzing feature patterns by age group...\")\n",
    "    analyze_feature_patterns(model, X_test, y_true, label_mapping, samples_per_age=3)\n",
    "    \n",
    "    print(\"\\n✅ Grad-CAM analysis complete!\")\n",
    "    print(\"\\nKey insights to look for:\")\n",
    "    print(\"🔍 Young deer (1.5-2.5y): Model may focus on body size, facial features\")\n",
    "    print(\"🔍 Middle-aged (3.5y): Model may focus on antler development, body mass\")  \n",
    "    print(\"🔍 Older deer (4.5-5.5y): Model may focus on antler size, body structure\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# USAGE: Run this after training to analyze your saved model\n",
    "model = run_gradcam_analysis(X_test, y_true, label_mapping, 'best_deer_efficientnet.pth')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502ac84-f86f-49be-8aec-250cb172d572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from buck.classifiers.autotune import optimize_all\n",
    "#\n",
    "#optimize_all(X_train, y_train, X_test, y_true, cycles=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
