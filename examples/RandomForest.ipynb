{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3eb0f-fc0c-4f79-bbd0-64a5fe58961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from buck.analysis.basics import ingest_images\n",
    "\n",
    "# Your existing ingestion\n",
    "fpath = \"C:\\\\Users\\\\aaron\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*.png\"\n",
    "images, ages = ingest_images(fpath)\n",
    "print(len(images),'images found:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e9606-63d3-422f-8f58-21a2e4ef579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from buck.analysis.basics import split_data\n",
    "\n",
    "Xtr_og, ytr_og, Xval, yval, Xte, yte_onehot, ages, l_map = split_data(images, ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eec27e-965b-411c-8296-27762189ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from buck.analysis.basics import homogenize_data\n",
    "\n",
    "augment_multiplier = 20\n",
    "X_train, y_train, X_test, y_true, label_mapping, num_classes = homogenize_data(Xtr_og, ytr_og, Xte,yte_onehot, l_map, augment_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502ac84-f86f-49be-8aec-250cb172d572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from buck.classifiers.autotune import optimize_all\n",
    "\n",
    "#optimize_all(X_train, y_train, X_test, y_true, cycles=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d816e-a5b9-496c-9e6b-7e4e13704d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install timm\n",
    "#!pip install torch\n",
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2dad4-961a-479e-9619-1a78b5561d12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DeerDatasetFromArrays(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class that works with your existing numpy arrays\n",
    "    Converts grayscale to RGB for transfer learning models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, transform=None, is_training=True):\n",
    "        \"\"\"\n",
    "        X: numpy array of shape (N, 288, 288, 1) - your grayscale images\n",
    "        y: numpy array of labels\n",
    "        transform: torchvision transforms\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        print(f\"Dataset created with {len(X)} samples, shape: {X.shape}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and label\n",
    "        image = self.X[idx]  # Shape: (288, 288, 1)\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        # Convert grayscale to RGB for transfer learning\n",
    "        # Squeeze the channel dimension and repeat 3 times\n",
    "        image_gray = image.squeeze(-1)  # (288, 288)\n",
    "        image_rgb = np.stack([image_gray, image_gray, image_gray], axis=-1)  # (288, 288, 3)\n",
    "        \n",
    "        # Normalize to 0-1 range if not already\n",
    "        if image_rgb.max() > 1.0:\n",
    "            image_rgb = image_rgb.astype(np.float32) / 255.0\n",
    "        else:\n",
    "            image_rgb = image_rgb.astype(np.float32)\n",
    "        \n",
    "        # Convert to PIL Image for torchvision transforms\n",
    "        from PIL import Image\n",
    "        image_pil = Image.fromarray((image_rgb * 255).astype(np.uint8))\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_pil)\n",
    "        else:\n",
    "            # Basic conversion to tensor\n",
    "            image_tensor = transforms.ToTensor()(image_pil)\n",
    "        \n",
    "        return image_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class DeerAgeEfficientNet:\n",
    "    \"\"\"\n",
    "    EfficientNet implementation adapted for your existing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = None\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Create EfficientNet model for deer age classification\n",
    "        \"\"\"\n",
    "        # Load pre-trained EfficientNet-B4\n",
    "        self.model = timm.create_model(\n",
    "            'efficientnet_b4',\n",
    "            pretrained=True,\n",
    "            num_classes=self.num_classes,\n",
    "            drop_rate=0.3,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        print(f\"‚úÖ EfficientNet-B4 created with {sum(p.numel() for p in self.model.parameters()):,} parameters\")\n",
    "        return self.model\n",
    "    \n",
    "    def get_transforms(self):\n",
    "        \"\"\"\n",
    "        Data transforms optimized for your deer images\n",
    "        Since you're already doing augmentation, these are minimal\n",
    "        \"\"\"\n",
    "        \n",
    "        # Training transforms - minimal since you already augment\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((384, 384)),  # Resize to EfficientNet optimal size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            # Optional: small additional augmentations\n",
    "            transforms.RandomErasing(p=0.1, scale=(0.02, 0.1))\n",
    "        ])\n",
    "        \n",
    "        # Validation transforms - no augmentation\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((384, 384)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        return train_transform, val_transform\n",
    "    \n",
    "    def prepare_data_loaders(self, X_train, y_train, X_val, y_val, batch_size=16):\n",
    "        \"\"\"\n",
    "        Create data loaders from your existing arrays\n",
    "        \"\"\"\n",
    "        print(\"üîÑ Preparing data loaders from your arrays...\")\n",
    "        \n",
    "        # Get transforms\n",
    "        train_transform, val_transform = self.get_transforms()\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = DeerDatasetFromArrays(X_train, y_train, train_transform, is_training=True)\n",
    "        val_dataset = DeerDatasetFromArrays(X_val, y_val, val_transform, is_training=False)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size * 2,  # Larger batch for validation\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Training loader: {len(train_loader)} batches\")\n",
    "        print(f\"‚úÖ Validation loader: {len(val_loader)} batches\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def train_model(self, train_loader, val_loader, epochs=50, learning_rate=1e-4):\n",
    "        \"\"\"\n",
    "        Train the model with your data\n",
    "        \"\"\"\n",
    "        print(f\"üöÄ Starting training for {epochs} epochs...\")\n",
    "        \n",
    "        # Optimizer and scheduler\n",
    "        optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=epochs, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Loss function with label smoothing\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Training history\n",
    "        history = {\n",
    "            'train_loss': [], 'train_acc': [],\n",
    "            'val_loss': [], 'val_acc': []\n",
    "        }\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 15\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                # Progress update every 20 batches\n",
    "                if batch_idx % 20 == 0:\n",
    "                    current_acc = 100. * train_correct / train_total\n",
    "                    print(f'Epoch {epoch+1}/{epochs} | Batch {batch_idx}/{len(train_loader)} | '\n",
    "                          f'Loss: {loss.item():.4f} | Acc: {current_acc:.1f}%')\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_acc = self._validate(val_loader, criterion)\n",
    "            \n",
    "            # Update history\n",
    "            train_acc = 100. * train_correct / train_total\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f'\\nüìä Epoch {epoch+1}/{epochs} Results:')\n",
    "            print(f'   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "            print(f'   Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "            print(f'   Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(self.model.state_dict(), 'best_deer_efficientnet.pth')\n",
    "                patience_counter = 0\n",
    "                print(f'   üéØ New best model! Validation Accuracy: {val_acc:.2f}%')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f'\\n‚è∞ Early stopping after {epoch+1} epochs (patience: {patience})')\n",
    "                break\n",
    "            \n",
    "            scheduler.step()\n",
    "            print('-' * 60)\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(torch.load('best_deer_efficientnet.pth'))\n",
    "        print(f'\\nüèÜ Training completed! Best validation accuracy: {best_val_acc:.2f}%')\n",
    "        \n",
    "        return history, best_val_acc\n",
    "    \n",
    "    def _validate(self, val_loader, criterion):\n",
    "        \"\"\"\n",
    "        Validation function\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * correct / total\n",
    "        \n",
    "        return val_loss, val_acc\n",
    "    \n",
    "    def evaluate_on_test(self, X_test, y_test, label_mapping):\n",
    "        \"\"\"\n",
    "        Final evaluation on your test set\n",
    "        \"\"\"\n",
    "        print(\"üéØ Final evaluation on test set...\")\n",
    "        \n",
    "        # Create test dataset and loader\n",
    "        _, val_transform = self.get_transforms()\n",
    "        test_dataset = DeerDatasetFromArrays(X_test, y_test, val_transform, is_training=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Get predictions\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                _, predicted = outputs.max(1)\n",
    "                \n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probabilities.extend(probabilities.cpu().numpy())\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        \n",
    "        # Create age labels from your mapping\n",
    "        age_labels = [f\"Age {list(label_mapping.keys())[i]}\" for i in range(len(label_mapping))]\n",
    "        \n",
    "        print(f\"\\nüéØ FINAL TEST RESULTS:\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"Improvement over your CNN baseline: +{(accuracy-0.31)*100:.1f}%\")\n",
    "        \n",
    "        if accuracy > 0.578:\n",
    "            print(\"üéâ SUCCESS! You've beaten your tree model baseline (57.8%)!\")\n",
    "        \n",
    "        # Detailed classification report\n",
    "        print(f\"\\nüìä Detailed Classification Report:\")\n",
    "        print(classification_report(all_labels, all_predictions, target_names=age_labels))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_predictions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=age_labels, yticklabels=age_labels)\n",
    "        plt.title(f'Confusion Matrix - Test Accuracy: {accuracy*100:.1f}%')\n",
    "        plt.ylabel('True Age')\n",
    "        plt.xlabel('Predicted Age')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return accuracy, all_predictions, all_probabilities\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"\n",
    "        Plot training curves\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss curves\n",
    "        ax1.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "        ax1.plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "        ax1.set_title('Model Loss', fontsize=14)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy curves\n",
    "        ax2.plot(history['train_acc'], label='Training Accuracy', linewidth=2)\n",
    "        ax2.plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "        ax2.set_title('Model Accuracy', fontsize=14)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print final stats\n",
    "        final_train_acc = history['train_acc'][-1]\n",
    "        final_val_acc = history['val_acc'][-1]\n",
    "        best_val_acc = max(history['val_acc'])\n",
    "        \n",
    "        print(f\"üìà Training Summary:\")\n",
    "        print(f\"   Final Training Accuracy: {final_train_acc:.2f}%\")\n",
    "        print(f\"   Final Validation Accuracy: {final_val_acc:.2f}%\")\n",
    "        print(f\"   Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "def run_efficientnet_on_your_data(X_train, y_train, X_val, y_val, X_test, y_test, label_mapping):\n",
    "    \"\"\"\n",
    "    Complete pipeline using your existing data\n",
    "    \"\"\"\n",
    "    print(\"ü¶å DEER AGE CLASSIFICATION WITH EFFICIENTNET\")\n",
    "    print(\"=\"*55)\n",
    "    print(f\"Training data: {X_train.shape}\")\n",
    "    print(f\"Validation data: {X_val.shape}\")\n",
    "    print(f\"Test data: {X_test.shape}\")\n",
    "    print(f\"Number of classes: {len(label_mapping)}\")\n",
    "    print(f\"Age mapping: {label_mapping}\")\n",
    "    print(\"=\"*55)\n",
    "    \n",
    "    # Initialize classifier\n",
    "    num_classes = len(label_mapping)\n",
    "    classifier = DeerAgeEfficientNet(num_classes=num_classes)\n",
    "    \n",
    "    # Create model\n",
    "    model = classifier.create_model()\n",
    "    \n",
    "    # Prepare data loaders\n",
    "    train_loader, val_loader = classifier.prepare_data_loaders(\n",
    "        X_train, y_train, X_val, y_val, batch_size=16\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history, best_val_acc = classifier.train_model(\n",
    "        train_loader, val_loader, epochs=50, learning_rate=1e-4\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    classifier.plot_training_history(history)\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    test_accuracy, predictions, probabilities = classifier.evaluate_on_test(\n",
    "        X_test, y_test, label_mapping\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéØ FINAL COMPARISON:\")\n",
    "    print(f\"Your previous CNN: 31.0%\")\n",
    "    print(f\"Your best tree model: 57.8%\")\n",
    "    print(f\"EfficientNet result: {test_accuracy*100:.1f}%\")\n",
    "    print(f\"Improvement: +{(test_accuracy-0.578)*100:.1f}% over tree models\")\n",
    "    \n",
    "    return classifier, history, test_accuracy\n",
    "\n",
    "# Debug function to visualize data conversion\n",
    "def debug_data_conversion(X_sample, y_sample, num_samples=3):\n",
    "    \"\"\"\n",
    "    Debug function to see how your grayscale images are converted\n",
    "    \"\"\"\n",
    "    print(\"üîç DEBUGGING DATA CONVERSION\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    classifier = DeerAgeEfficientNet(num_classes=5)\n",
    "    train_transform, _ = classifier.get_transforms()\n",
    "    \n",
    "    # Create sample dataset\n",
    "    dataset = DeerDatasetFromArrays(X_sample[:num_samples], y_sample[:num_samples], \n",
    "                                   train_transform, is_training=False)\n",
    "    \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        # Get original and processed\n",
    "        original = X_sample[i].squeeze()  # Remove channel dimension for display\n",
    "        processed_tensor, label = dataset[i]\n",
    "        \n",
    "        # Denormalize processed tensor for visualization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        denorm = processed_tensor * std + mean\n",
    "        denorm = torch.clamp(denorm, 0, 1)\n",
    "        \n",
    "        # Convert back to numpy for display\n",
    "        processed_rgb = denorm.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(original, cmap='gray')\n",
    "        plt.title(f'Original Grayscale\\nLabel: {label.item()}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(processed_rgb)\n",
    "        plt.title(f'Converted to RGB\\nShape: {processed_tensor.shape}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(processed_rgb[:,:,0], cmap='gray')  # Show one channel\n",
    "        plt.title('RGB Channel (same as original)')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Original shape: {original.shape}\")\n",
    "        print(f\"  Processed shape: {processed_tensor.shape}\")\n",
    "        print(f\"  Label: {label.item()}\")\n",
    "        print(f\"  Tensor range: [{processed_tensor.min():.3f}, {processed_tensor.max():.3f}]\")\n",
    "        print()\n",
    "\n",
    "print(\"‚úÖ EfficientNet implementation ready for your pipeline!\")\n",
    "print(\"Next: Run debug_data_conversion() to verify data handling\")\n",
    "print(\"Then: Run run_efficientnet_on_your_data() for full training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8923aaa-3d0e-4603-9080-ea891f75441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_efficientnet_on_your_data(X_train, y_train, Xval, yval, X_test, y_true, label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967871c-bd87-4300-94aa-d29a25dc2757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall CPU version\n",
    "!pip uninstall torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd97444-03e5-4cdb-95ad-f12fc9ec2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CUDA version (for RTX 2060)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab5908-149b-4163-b1aa-59936e32fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"PyTorch CUDA device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# If using TensorFlow instead:\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow GPU devices: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39810c9-54ef-45b3-a433-adc370f81d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
