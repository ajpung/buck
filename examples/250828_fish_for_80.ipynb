{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a5b6e2-ade6-40ef-9121-92a94d2a855e",
   "metadata": {},
   "source": [
    "## What changed?\n",
    "\n",
    "This notebook takes the output result of `250813_nda_all` and attempts to optimize a single model instead of an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62247f7c-827d-4f33-99dd-bc10db9c0700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 2060\n",
      "GPU memory: 6.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not detected by PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1083fe-4efc-4cf3-874a-b0056cb1eafa",
   "metadata": {},
   "source": [
    "### Fine-tuning hyperparameters of ghostnet_100\n",
    "\n",
    "- Trying to recapture  ‚úì Val: 84.2%, Test: 81.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb90355-602e-495d-a537-d567709b2de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GhostNet Hyperparameter Tuning for Deer Age Prediction\n",
      "============================================================\n",
      "Loading color images...\n",
      "Loaded 200 color images\n",
      "Loading grayscale images...\n",
      "Loaded 38 grayscale images\n",
      "Total images: 238\n",
      "Final dataset: 238 images\n",
      "Age distribution: {2.5: 40, 3.5: 50, 4.5: 56, 5.5: 60, 1.5: 32}\n",
      "\n",
      "Classes: 5\n",
      "Label mapping: {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
      "\n",
      "Data split:\n",
      "Train: 152 images\n",
      "Val: 38 images\n",
      "Test: 48 images\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Starting GhostNet hyperparameter tuning...\n",
      "Testing 30 hyperparameter combinations\n",
      "\n",
      "[ 1/30] Testing combination 1\n",
      "  Optimizer: sgd, LR: 0.0005/0.002\n",
      "  Batch: 12, Dropout: 0.2, Freeze: 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GhostNet Hyperparameter Search Space\n",
    "HYPERPARAMETER_GRID = {\n",
    "    'backbone_lr': [0.0001, 0.0003, 0.0005],\n",
    "    'classifier_lr': [0.0005, 0.001, 0.002],\n",
    "    'weight_decay': [0.01, 0.03, 0.05],\n",
    "    'batch_size': [8, 12, 16],\n",
    "    'optimizer': ['adamw', 'sgd'],\n",
    "    'scheduler': ['cosine', 'plateau'],\n",
    "    'dropout': [0.2, 0.3, 0.4, 0.5],\n",
    "    'label_smoothing': [0.05, 0.1, 0.15],\n",
    "    'augmentation_strength': ['light', 'medium', 'heavy'],\n",
    "    'freeze_layers': [2, 3, 4]  # How many early block groups to freeze\n",
    "}\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUGMENTATION_TARGET = 1000\n",
    "MAX_COMBINATIONS = 30  # Test 30 combinations\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    \"\"\"Detect if image is grayscale and convert to 3-channel RGB\"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    \"\"\"Load data from both color and grayscale folders\"\"\"\n",
    "    color_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "    gray_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*_NDA.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    # Group ages\n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    # Filter classes with enough samples\n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    \n",
    "    return np.array(filtered_images), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image(image, strength='medium'):\n",
    "    \"\"\"Enhanced augmentation with variable strength\"\"\"\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Set probabilities based on strength\n",
    "    if strength == 'light':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.5, 0.3, 0.6, 0.2, 0.1\n",
    "        rot_range, bright_range = 10, (0.8, 1.2)\n",
    "    elif strength == 'medium':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.7, 0.5, 0.8, 0.4, 0.3\n",
    "        rot_range, bright_range = 15, (0.7, 1.3)\n",
    "    else:  # heavy\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.8, 0.6, 0.9, 0.5, 0.4\n",
    "        rot_range, bright_range = 20, (0.6, 1.4)\n",
    "    \n",
    "    # Rotation\n",
    "    if random.random() < rot_prob:\n",
    "        angle = random.uniform(-rot_range, rot_range)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Horizontal flip\n",
    "    if random.random() < flip_prob:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Strategic color conversion (RGB -> Grayscale -> RGB)\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.4:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Brightness/contrast\n",
    "    if random.random() < bright_prob:\n",
    "        alpha = random.uniform(*bright_range)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Gamma correction\n",
    "    if random.random() < gamma_prob:\n",
    "        gamma = random.uniform(0.8, 1.2)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    # Noise\n",
    "    if random.random() < noise_prob:\n",
    "        noise = np.random.normal(0, 7, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_balanced_dataset(X, y, aug_strength='medium'):\n",
    "    \"\"\"Create balanced dataset through augmentation\"\"\"\n",
    "    class_counts = Counter(y)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max(AUGMENTATION_TARGET, max_count)\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for class_idx in range(len(set(y))):\n",
    "        class_mask = np.array(y) == class_idx\n",
    "        class_images = X[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        if current_count == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add originals\n",
    "        X_balanced.extend(class_images)\n",
    "        y_balanced.extend([class_idx] * current_count)\n",
    "        \n",
    "        # Add augmented to reach target\n",
    "        needed = target_count - current_count\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image(class_images[orig_idx].copy(), aug_strength)\n",
    "            X_balanced.append(aug_img)\n",
    "            y_balanced.append(class_idx)\n",
    "    \n",
    "    return np.array(X_balanced), np.array(y_balanced)\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    def __init__(self, X, y, training=True):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.training = training\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class GhostNetHyperparameterTuner:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"ghostnet_tuning_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    def create_ghostnet_model(self, dropout=0.3, freeze_layers=3):\n",
    "        \"\"\"Create GhostNet model with specified dropout and freezing\"\"\"\n",
    "        model = timm.create_model('ghostnet_100', pretrained=True, num_classes=self.num_classes)\n",
    "        \n",
    "        # Freeze early layers based on freeze_layers parameter\n",
    "        freeze_patterns = [\n",
    "            ['conv_stem'],\n",
    "            ['conv_stem', 'blocks.0'],\n",
    "            ['conv_stem', 'blocks.0', 'blocks.1'],\n",
    "            ['conv_stem', 'blocks.0', 'blocks.1', 'blocks.2']\n",
    "        ]\n",
    "        \n",
    "        if freeze_layers <= len(freeze_patterns):\n",
    "            for name, param in model.named_parameters():\n",
    "                for pattern in freeze_patterns[freeze_layers - 1]:\n",
    "                    if pattern in name:\n",
    "                        param.requires_grad = False\n",
    "                        break\n",
    "        \n",
    "        # Replace classifier with custom dropout\n",
    "        if hasattr(model, 'classifier'):\n",
    "            in_features = model.classifier.in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def get_optimizer(self, model, opt_type, backbone_lr, classifier_lr, weight_decay):\n",
    "        \"\"\"Create optimizer based on hyperparameters\"\"\"\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'classifier' in name:\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        param_groups = [\n",
    "            {'params': backbone_params, 'lr': backbone_lr},\n",
    "            {'params': classifier_params, 'lr': classifier_lr}\n",
    "        ]\n",
    "        \n",
    "        if opt_type == 'adamw':\n",
    "            return optim.AdamW(param_groups, weight_decay=weight_decay)\n",
    "        elif opt_type == 'sgd':\n",
    "            return optim.SGD(param_groups, weight_decay=weight_decay, momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {opt_type}\")\n",
    "    \n",
    "    def get_scheduler(self, optimizer, scheduler_type, max_epochs):\n",
    "        \"\"\"Create learning rate scheduler\"\"\"\n",
    "        if scheduler_type == 'cosine':\n",
    "            return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-6)\n",
    "        elif scheduler_type == 'plateau':\n",
    "            return optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, verbose=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scheduler: {scheduler_type}\")\n",
    "    \n",
    "    def train_with_hyperparams(self, train_loader, val_loader, test_loader, hyperparams, combo_num):\n",
    "        \"\"\"Train model with specific hyperparameters\"\"\"\n",
    "        model = self.create_ghostnet_model(\n",
    "            dropout=hyperparams['dropout'], \n",
    "            freeze_layers=hyperparams['freeze_layers']\n",
    "        )\n",
    "        \n",
    "        optimizer = self.get_optimizer(\n",
    "            model, hyperparams['optimizer'], \n",
    "            hyperparams['backbone_lr'], hyperparams['classifier_lr'], \n",
    "            hyperparams['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = self.get_scheduler(optimizer, hyperparams['scheduler'], 80)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=hyperparams['label_smoothing'])\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 20\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(80):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Update scheduler\n",
    "            if hyperparams['scheduler'] == 'plateau':\n",
    "                scheduler.step(val_acc)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Print progress every 20 epochs\n",
    "            if epoch % 20 == 0 and epoch > 0:\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load best weights and evaluate on test\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        # Test evaluation with TTA\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Test time augmentation\n",
    "                outputs1 = model(images)\n",
    "                flipped = torch.flip(images, [3])\n",
    "                outputs2 = model(flipped)\n",
    "                outputs = (outputs1 + outputs2) / 2\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        return model, best_val_acc, test_acc\n",
    "    \n",
    "    def generate_hyperparameter_combinations(self):\n",
    "        \"\"\"Generate smart hyperparameter combinations\"\"\"\n",
    "        # Create all possible combinations\n",
    "        keys = list(HYPERPARAMETER_GRID.keys())\n",
    "        values = list(HYPERPARAMETER_GRID.values())\n",
    "        all_combinations = list(itertools.product(*values))\n",
    "        \n",
    "        # Shuffle and limit\n",
    "        random.shuffle(all_combinations)\n",
    "        selected_combinations = all_combinations[:MAX_COMBINATIONS]\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        combinations = []\n",
    "        for combo in selected_combinations:\n",
    "            hyperparams = dict(zip(keys, combo))\n",
    "            combinations.append(hyperparams)\n",
    "        \n",
    "        return combinations\n",
    "    \n",
    "    def tune_hyperparameters(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        \"\"\"Main hyperparameter tuning loop\"\"\"\n",
    "        print(f\"Starting GhostNet hyperparameter tuning...\")\n",
    "        print(f\"Testing {MAX_COMBINATIONS} hyperparameter combinations\")\n",
    "        \n",
    "        combinations = self.generate_hyperparameter_combinations()\n",
    "        results = []\n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        for i, hyperparams in enumerate(combinations, 1):\n",
    "            print(f\"\\n[{i:2d}/{MAX_COMBINATIONS}] Testing combination {i}\")\n",
    "            print(f\"  Optimizer: {hyperparams['optimizer']}, LR: {hyperparams['backbone_lr']}/{hyperparams['classifier_lr']}\")\n",
    "            print(f\"  Batch: {hyperparams['batch_size']}, Dropout: {hyperparams['dropout']}, Freeze: {hyperparams['freeze_layers']}\")\n",
    "            \n",
    "            try:\n",
    "                # Create datasets with current augmentation strength\n",
    "                X_train_aug, y_train_aug = create_balanced_dataset(\n",
    "                    X_train, y_train, hyperparams['augmentation_strength']\n",
    "                )\n",
    "                \n",
    "                train_dataset = DeerDataset(X_train_aug, y_train_aug, training=True)\n",
    "                val_dataset = DeerDataset(X_val, y_val, training=False)\n",
    "                test_dataset = DeerDataset(X_test, y_test, training=False)\n",
    "                \n",
    "                train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True, num_workers=0)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'], shuffle=False, num_workers=0)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'], shuffle=False, num_workers=0)\n",
    "                \n",
    "                model, val_acc, test_acc = self.train_with_hyperparams(\n",
    "                    train_loader, val_loader, test_loader, hyperparams, i\n",
    "                )\n",
    "                \n",
    "                result = {\n",
    "                    'combination': i,\n",
    "                    'hyperparams': hyperparams,\n",
    "                    'val_accuracy': val_acc,\n",
    "                    'test_accuracy': test_acc\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"  ‚úì Val: {val_acc:.1f}%, Test: {test_acc:.1f}%\")\n",
    "                \n",
    "                # Save every model with unique name based on test accuracy\n",
    "                model_path = os.path.join(self.save_dir, f\"ghostnet_combo_{i:02d}_test_{test_acc:.1f}.pth\")\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'hyperparams': hyperparams,\n",
    "                    'val_accuracy': val_acc,\n",
    "                    'test_accuracy': test_acc,\n",
    "                    'combination': i\n",
    "                }, model_path)\n",
    "                print(f\"  üíæ Model saved: ghostnet_combo_{i:02d}_test_{test_acc:.1f}.pth\")\n",
    "                \n",
    "                # Save best model so far\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_path = os.path.join(self.save_dir, f\"ghostnet_best_val_{val_acc:.1f}.pth\")\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'hyperparams': hyperparams,\n",
    "                        'val_accuracy': val_acc,\n",
    "                        'test_accuracy': test_acc,\n",
    "                        'combination': i\n",
    "                    }, best_path)\n",
    "                    print(f\"  üíæ New best model saved: {val_acc:.1f}%\")\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Combination {i} failed: {str(e)[:60]}...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "        \n",
    "        # Save all results\n",
    "        results_path = os.path.join(self.save_dir, \"hyperparameter_results.json\")\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set with TTA\"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Test time augmentation\n",
    "            outputs1 = model(images)\n",
    "            flipped = torch.flip(images, [3])\n",
    "            outputs2 = model(flipped)\n",
    "            outputs = (outputs1 + outputs2) / 2\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * test_correct / test_total\n",
    "    return test_acc\n",
    "\n",
    "def main():\n",
    "    print(\"GhostNet Hyperparameter Tuning for Deer Age Prediction\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load combined data\n",
    "    images, ages, sources = load_combined_data()\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    y_indices = np.array([label_mapping[age] for age in ages])\n",
    "    \n",
    "    print(f\"\\nClasses: {len(unique_ages)}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images, y_indices, test_size=0.2, random_state=42, stratify=y_indices\n",
    "    )\n",
    "    \n",
    "    # Further split training into train/val\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"Train: {len(X_train_final)} images\")\n",
    "    print(f\"Val: {len(X_val)} images\") \n",
    "    print(f\"Test: {len(X_test)} images\")\n",
    "    \n",
    "    # Initialize tuner\n",
    "    tuner = GhostNetHyperparameterTuner(num_classes=len(unique_ages))\n",
    "    \n",
    "    # Run hyperparameter tuning\n",
    "    results = tuner.tune_hyperparameters(X_train_final, y_train_final, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    # Analyze results\n",
    "    if results:\n",
    "        # Sort by validation accuracy\n",
    "        sorted_results = sorted(results, key=lambda x: x['val_accuracy'], reverse=True)\n",
    "        \n",
    "        elapsed = (time.time() - start_time) / 60\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"GHOSTNET HYPERPARAMETER TUNING RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"{'Rank':<4} {'Combination':<6} {'Validation':<12} {'Test':<8} {'Key Hyperparams'}\")\n",
    "        print('-' * 75)\n",
    "        \n",
    "        for i, result in enumerate(sorted_results[:10], 1):  # Top 10\n",
    "            hp = result['hyperparams']\n",
    "            key_params = f\"opt={hp['optimizer']}, lr={hp['backbone_lr']}, bs={hp['batch_size']}, drop={hp['dropout']}\"\n",
    "            print(f\"{i:2d}. {result['combination']:4d}       {result['val_accuracy']:5.1f}%      {result['test_accuracy']:5.1f}%    {key_params}\")\n",
    "        \n",
    "        best_result = sorted_results[0]\n",
    "        print(f\"\\nüèÜ BEST HYPERPARAMETERS:\")\n",
    "        for key, value in best_result['hyperparams'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\nüìä PERFORMANCE:\")\n",
    "        print(f\"  Best Validation: {best_result['val_accuracy']:.1f}%\")\n",
    "        print(f\"  Best Test: {best_result['test_accuracy']:.1f}%\")\n",
    "        print(f\"  Tuning Time: {elapsed:.1f} minutes\")\n",
    "        print(f\"  Results saved to: {tuner.save_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No successful combinations found!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc62768-7360-4eb3-bc38-18b6eb6b11be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
