{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a5b6e2-ade6-40ef-9121-92a94d2a855e",
   "metadata": {},
   "source": [
    "## What changed?\n",
    "\n",
    "This notebook takes the output result of `250813_nda_all` and attempts to optimize a single model instead of an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62247f7c-827d-4f33-99dd-bc10db9c0700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 2060\n",
      "GPU memory: 6.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not detected by PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccd28f-ace7-4023-989c-f597892f9d2b",
   "metadata": {},
   "source": [
    "### Initial color/grayscale exploration\n",
    "\n",
    "(ghostnet_100 wins, Val 84.2%, Test 72.9%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa4928b-d228-4d97-b29e-b0b0abf0737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Optimized Deer Age Prediction Model\n",
      "==================================================\n",
      "Loading color images...\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 361\n",
      "Loaded 363 color images\n",
      "Loading grayscale images...\n",
      "Loaded 108 grayscale images\n",
      "Total images: 471\n",
      "Final dataset: 471 images\n",
      "Age distribution: {5.5: 121, 4.5: 89, 2.5: 83, 3.5: 111, 1.5: 67}\n",
      "Source distribution: {'color': 363, 'grayscale': 108}\n",
      "\n",
      "Classes: 5\n",
      "Label mapping: {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
      "\n",
      "Data split:\n",
      "Train: 300 images\n",
      "Val: 76 images\n",
      "Test: 95 images\n",
      "\n",
      "Creating balanced training set...\n",
      "\n",
      "Original class distribution:\n",
      "  Class 0: 42 images\n",
      "  Class 1: 53 images\n",
      "  Class 2: 71 images\n",
      "  Class 3: 57 images\n",
      "  Class 4: 77 images\n",
      "\n",
      "Target samples per class: 1000\n",
      "  Class 0: 42 original + 958 augmented = 1000 total\n",
      "  Class 1: 53 original + 947 augmented = 1000 total\n",
      "  Class 2: 71 original + 929 augmented = 1000 total\n",
      "  Class 3: 57 original + 943 augmented = 1000 total\n",
      "  Class 4: 77 original + 923 augmented = 1000 total\n",
      "\n",
      "Final balanced class distribution:\n",
      "  Class 0: 1000 images\n",
      "  Class 1: 1000 images\n",
      "  Class 2: 1000 images\n",
      "  Class 3: 1000 images\n",
      "  Class 4: 1000 images\n",
      "Total training images after balancing: 5000\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Testing 30 diverse architectural families...\n",
      "[ 1/30] Testing efficientnet_b1...\n",
      "Training efficientnet_b1...\n",
      "    Epoch 20: Train 100.0%, Val 64.5%\n",
      "    Early stopping at epoch 30 (patience reached)\n",
      "  efficientnet_b1 best validation: 67.1%\n",
      "  ‚úì efficientnet_b1: Val 67.1%, Test 51.6%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_efficientnet_b1_51.6pct.pth\n",
      "\n",
      "[ 2/30] Testing resnet34...\n",
      "Training resnet34...\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 35 (patience reached)\n",
      "  resnet34 best validation: 72.4%\n",
      "  ‚úì resnet34: Val 72.4%, Test 60.0%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_resnet34_60.0pct.pth\n",
      "\n",
      "[ 3/30] Testing densenet169...\n",
      "Training densenet169...\n",
      "    Epoch 20: Train 99.8%, Val 61.8%\n",
      "    Epoch 40: Train 98.7%, Val 61.8%\n",
      "    Early stopping at epoch 54 (patience reached)\n",
      "  densenet169 best validation: 72.4%\n",
      "  ‚úì densenet169: Val 72.4%, Test 58.9%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_densenet169_58.9pct.pth\n",
      "\n",
      "[ 4/30] Testing hrnet_w32...\n",
      "Training hrnet_w32...\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 37 (patience reached)\n",
      "  hrnet_w32 best validation: 80.3%\n",
      "  ‚úì hrnet_w32: Val 80.3%, Test 58.9%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_hrnet_w32_58.9pct.pth\n",
      "\n",
      "[ 5/30] Testing mobilenetv3_large_100...\n",
      "Training mobilenetv3_large_100...\n",
      "    Epoch 20: Train 100.0%, Val 69.7%\n",
      "    Early stopping at epoch 27 (patience reached)\n",
      "  mobilenetv3_large_100 best validation: 72.4%\n",
      "  ‚úì mobilenetv3_large_100: Val 72.4%, Test 54.7%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_mobilenetv3_large_100_54.7pct.pth\n",
      "\n",
      "[ 6/30] Testing vit_small_patch16_224...\n",
      "Training vit_small_patch16_224...\n",
      "    Epoch 20: Train 99.1%, Val 61.8%\n",
      "    Early stopping at epoch 32 (patience reached)\n",
      "  vit_small_patch16_224 best validation: 65.8%\n",
      "  ‚úì vit_small_patch16_224: Val 65.8%, Test 51.6%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_vit_small_patch16_224_51.6pct.pth\n",
      "\n",
      "[ 7/30] Testing regnetx_004...\n",
      "Training regnetx_004...\n",
      "  ‚úó regnetx_004 failed: mat1 and mat2 shapes cannot be multiplied (32256x7...\n",
      "\n",
      "[ 8/30] Testing convnext_tiny...\n",
      "Training convnext_tiny...\n",
      "  ‚úó convnext_tiny failed: mat1 and mat2 shapes cannot be multiplied (64512x7...\n",
      "\n",
      "[ 9/30] Testing swin_tiny_patch4_window7_224...\n",
      "Training swin_tiny_patch4_window7_224...\n",
      "  ‚úó swin_tiny_patch4_window7_224 failed: only batches of spatial targets supported (3D tens...\n",
      "\n",
      "[10/30] Testing maxvit_tiny_tf_224...\n",
      "Training maxvit_tiny_tf_224...\n",
      "  ‚úó maxvit_tiny_tf_224 failed: mat1 and mat2 shapes cannot be multiplied (43008x7...\n",
      "\n",
      "[11/30] Testing repvgg_b1...\n",
      "Training repvgg_b1...\n",
      "  ‚úó repvgg_b1 failed: mat1 and mat2 shapes cannot be multiplied (172032x...\n",
      "\n",
      "[12/30] Testing ghostnet_100...\n",
      "Training ghostnet_100...\n",
      "    Epoch 20: Train 99.9%, Val 73.7%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Epoch 60: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 64 (patience reached)\n",
      "  ghostnet_100 best validation: 75.0%\n",
      "  ‚úì ghostnet_100: Val 75.0%, Test 51.6%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_ghostnet_100_51.6pct.pth\n",
      "\n",
      "[13/30] Testing mobilevit_s...\n",
      "Training mobilevit_s...\n",
      "  ‚úó mobilevit_s failed: mat1 and mat2 shapes cannot be multiplied (53760x7...\n",
      "\n",
      "[14/30] Testing resnext50_32x4d...\n",
      "Training resnext50_32x4d...\n",
      "    Epoch 20: Train 99.7%, Val 59.2%\n",
      "    Epoch 40: Train 100.0%, Val 63.2%\n",
      "    Early stopping at epoch 50 (patience reached)\n",
      "  resnext50_32x4d best validation: 71.1%\n",
      "  ‚úì resnext50_32x4d: Val 71.1%, Test 56.8%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_resnext50_32x4d_56.8pct.pth\n",
      "\n",
      "[15/30] Testing seresnet50...\n",
      "Training seresnet50...\n",
      "    Epoch 20: Train 99.8%, Val 64.5%\n",
      "    Early stopping at epoch 36 (patience reached)\n",
      "  seresnet50 best validation: 71.1%\n",
      "  ‚úì seresnet50: Val 71.1%, Test 51.6%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_seresnet50_51.6pct.pth\n",
      "\n",
      "[16/30] Testing efficientnet_b0...\n",
      "Training efficientnet_b0...\n",
      "    Epoch 20: Train 99.9%, Val 73.7%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 50 (patience reached)\n",
      "  efficientnet_b0 best validation: 78.9%\n",
      "  ‚úì efficientnet_b0: Val 78.9%, Test 53.7%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_efficientnet_b0_53.7pct.pth\n",
      "\n",
      "[17/30] Testing efficientnet_b2...\n",
      "Training efficientnet_b2...\n",
      "    Epoch 20: Train 99.9%, Val 69.7%\n",
      "    Epoch 40: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 40 (patience reached)\n",
      "  efficientnet_b2 best validation: 76.3%\n",
      "  ‚úì efficientnet_b2: Val 76.3%, Test 61.1%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_efficientnet_b2_61.1pct.pth\n",
      "\n",
      "[18/30] Testing efficientnetv2_s...\n",
      "Training efficientnetv2_s...\n",
      "  ‚úó efficientnetv2_s failed: No pretrained weights exist for efficientnetv2_s. ...\n",
      "\n",
      "[19/30] Testing resnet18...\n",
      "Training resnet18...\n",
      "    Epoch 20: Train 100.0%, Val 63.2%\n",
      "    Epoch 40: Train 100.0%, Val 64.5%\n",
      "    Early stopping at epoch 55 (patience reached)\n",
      "  resnet18 best validation: 71.1%\n",
      "  ‚úì resnet18: Val 71.1%, Test 61.1%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_resnet18_61.1pct.pth\n",
      "\n",
      "[20/30] Testing resnet50...\n",
      "Training resnet50...\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 32 (patience reached)\n",
      "  resnet50 best validation: 72.4%\n",
      "  ‚úì resnet50: Val 72.4%, Test 56.8%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_resnet50_56.8pct.pth\n",
      "\n",
      "[21/30] Testing densenet121...\n",
      "Training densenet121...\n",
      "    Epoch 20: Train 99.9%, Val 68.4%\n",
      "    Epoch 40: Train 99.6%, Val 59.2%\n",
      "    Early stopping at epoch 47 (patience reached)\n",
      "  densenet121 best validation: 72.4%\n",
      "  ‚úì densenet121: Val 72.4%, Test 55.8%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_densenet121_55.8pct.pth\n",
      "\n",
      "[22/30] Testing mobilenetv3_small_100...\n",
      "Training mobilenetv3_small_100...\n",
      "    Epoch 20: Train 99.3%, Val 57.9%\n",
      "    Epoch 40: Train 100.0%, Val 57.9%\n",
      "    Early stopping at epoch 54 (patience reached)\n",
      "  mobilenetv3_small_100 best validation: 72.4%\n",
      "  ‚úì mobilenetv3_small_100: Val 72.4%, Test 51.6%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_mobilenetv3_small_100_51.6pct.pth\n",
      "\n",
      "[23/30] Testing regnetx_002...\n",
      "Training regnetx_002...\n",
      "  ‚úó regnetx_002 failed: mat1 and mat2 shapes cannot be multiplied (30912x7...\n",
      "\n",
      "[24/30] Testing regnetx_008...\n",
      "Training regnetx_008...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9d57f7e7754c45a8675a3d1f5c992d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/29.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úó regnetx_008 failed: mat1 and mat2 shapes cannot be multiplied (56448x7...\n",
      "\n",
      "[25/30] Testing convnext_small...\n",
      "Training convnext_small...\n",
      "  ‚úó convnext_small failed: mat1 and mat2 shapes cannot be multiplied (64512x7...\n",
      "\n",
      "[26/30] Testing swin_small_patch4_window7_224...\n",
      "Training swin_small_patch4_window7_224...\n",
      "  ‚úó swin_small_patch4_window7_224 failed: only batches of spatial targets supported (3D tens...\n",
      "\n",
      "[27/30] Testing vit_tiny_patch16_224...\n",
      "Training vit_tiny_patch16_224...\n",
      "    Epoch 20: Train 98.6%, Val 48.7%\n",
      "    Early stopping at epoch 28 (patience reached)\n",
      "  vit_tiny_patch16_224 best validation: 63.2%\n",
      "  ‚úì vit_tiny_patch16_224: Val 63.2%, Test 50.5%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_vit_tiny_patch16_224_50.5pct.pth\n",
      "\n",
      "[28/30] Testing mobilevit_xs...\n",
      "Training mobilevit_xs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bc8eab9e6d406babc0a3d43c51e077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/9.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úó mobilevit_xs failed: mat1 and mat2 shapes cannot be multiplied (32256x7...\n",
      "\n",
      "[29/30] Testing resnext101_32x8d...\n",
      "Training resnext101_32x8d...\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 30 (patience reached)\n",
      "  resnext101_32x8d best validation: 73.7%\n",
      "  ‚úì resnext101_32x8d: Val 73.7%, Test 60.0%\n",
      "  Model saved: single_model_20250829_074605\\deer_age_model_resnext101_32x8d_60.0pct.pth\n",
      "\n",
      "[30/30] Testing dla60...\n",
      "Training dla60...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228a82ebe996477c9883e45d5d4172ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/88.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úó dla60 failed: 'Conv2d' object has no attribute 'in_features'...\n",
      "\n",
      "\n",
      "============================================================\n",
      "ARCHITECTURE COMPARISON RESULTS\n",
      "============================================================\n",
      "Rank Architecture                   Validation   Test    \n",
      "------------------------------------------------------------\n",
      " 1. üèÜ hrnet_w32                     80.3%       58.9%\n",
      " 2.    efficientnet_b0               78.9%       53.7%\n",
      " 3.    efficientnet_b2               76.3%       61.1%\n",
      " 4.    ghostnet_100                  75.0%       51.6%\n",
      " 5.    resnext101_32x8d              73.7%       60.0%\n",
      " 6.    resnet34                      72.4%       60.0%\n",
      " 7.    densenet169                   72.4%       58.9%\n",
      " 8.    mobilenetv3_large_100         72.4%       54.7%\n",
      " 9.    resnet50                      72.4%       56.8%\n",
      "10.    densenet121                   72.4%       55.8%\n",
      "11.    mobilenetv3_small_100         72.4%       51.6%\n",
      "12.    resnext50_32x4d               71.1%       56.8%\n",
      "13.    seresnet50                    71.1%       51.6%\n",
      "14.    resnet18                      71.1%       61.1%\n",
      "15.    efficientnet_b1               67.1%       51.6%\n",
      "16.    vit_small_patch16_224         65.8%       51.6%\n",
      "17.    vit_tiny_patch16_224          63.2%       50.5%\n",
      "\n",
      "Failed architectures (13): regnetx_004, convnext_tiny, swin_tiny_patch4_window7_224, maxvit_tiny_tf_224, repvgg_b1, mobilevit_s, efficientnetv2_s, regnetx_002, regnetx_008, convnext_small, swin_small_patch4_window7_224, mobilevit_xs, dla60\n",
      "\n",
      "üèÜ WINNER: hrnet_w32 (Val: 80.3%, Test: 58.9%)\n",
      "Note: Selection based on validation performance to avoid test contamination\n",
      "\n",
      "Final optimization of hrnet_w32...\n",
      "  Epoch 0: Train 99.6%, Val 57.9%\n",
      "  Epoch 10: Train 100.0%, Val 65.8%\n",
      "  Epoch 20: Train 99.5%, Val 63.2%\n",
      "  Final optimization complete: 72.4%\n",
      "\n",
      "==================================================\n",
      "FINAL RESULTS\n",
      "==================================================\n",
      "Best architecture: hrnet_w32\n",
      "Validation accuracy: 72.4%\n",
      "Test accuracy: 62.1%\n",
      "Training time: 493.1 minutes\n",
      "Final model saved: single_model_20250829_074605\\deer_age_model_hrnet_w32_62.1pct_FINAL.pth\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 30 different architectural families optimized for trail camera deer images\n",
    "TOP_ARCHITECTURES = [\n",
    "    # Original 15 architectures\n",
    "    'efficientnet_b1',           # 1. EfficientNet (Google)\n",
    "    'resnet34',                  # 2. ResNet (Microsoft Research)\n",
    "    'densenet169',               # 3. DenseNet (Cornell/Tsinghua)\n",
    "    'hrnet_w32',                 # 4. HRNet (Microsoft Research)\n",
    "    'mobilenetv3_large_100',     # 5. MobileNet (Google)\n",
    "    'vit_small_patch16_224',     # 6. Vision Transformer (Google)\n",
    "    'regnetx_004',               # 7. RegNet (Facebook)\n",
    "    'convnext_tiny',             # 8. ConvNeXt (Facebook)\n",
    "    'swin_tiny_patch4_window7_224', # 9. Swin Transformer (Microsoft)\n",
    "    'maxvit_tiny_tf_224',        # 10. MaxViT (Google)\n",
    "    'repvgg_b1',                 # 11. RepVGG (Tsinghua)\n",
    "    'ghostnet_100',              # 12. GhostNet (Huawei)\n",
    "    'mobilevit_s',               # 13. MobileViT (Apple)\n",
    "    'resnext50_32x4d',           # 14. ResNeXt (Facebook)\n",
    "    'seresnet50',                # 15. SENet (WMW)\n",
    "    \n",
    "    # Additional 15 architectures optimized for trail camera imagery\n",
    "    'efficientnet_b0',           # 16. Smaller EfficientNet for speed\n",
    "    'efficientnet_b2',           # 17. Larger EfficientNet for accuracy\n",
    "    'efficientnetv2_s',          # 18. Newer EfficientNet with training improvements\n",
    "    'resnet18',                  # 19. Lightweight ResNet for efficiency\n",
    "    'resnet50',                  # 20. Standard ResNet workhorse\n",
    "    'densenet121',               # 21. Efficient DenseNet variant\n",
    "    'mobilenetv3_small_100',     # 22. Very efficient for edge deployment\n",
    "    'regnetx_002',               # 23. Smaller RegNet for speed\n",
    "    'regnetx_008',               # 24. Larger RegNet for accuracy\n",
    "    'convnext_small',            # 25. Larger ConvNeXt for better features\n",
    "    'swin_small_patch4_window7_224', # 26. Larger Swin for hierarchical features\n",
    "    'vit_tiny_patch16_224',      # 27. Tiny ViT for efficiency\n",
    "    'mobilevit_xs',              # 28. Extra small MobileViT\n",
    "    'resnext101_32x8d',          # 29. Large ResNeXt for maximum accuracy\n",
    "    'dla60'                      # 30. Deep Layer Aggregation (good for fine-grained tasks)\n",
    "]\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUGMENTATION_TARGET = 1000\n",
    "BATCH_SIZE = 12  # RTX 2060 friendly\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    \"\"\"Detect if image is grayscale and convert to 3-channel RGB\"\"\"\n",
    "    if len(image.shape) == 2:  # Grayscale\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:  # Single channel\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:  # Already RGB\n",
    "            return image\n",
    "        elif image.shape[2] == 4:  # RGBA\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    \"\"\"Load data from both color and grayscale folders\"\"\"\n",
    "    color_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*.png\"\n",
    "    gray_path = \"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []  # Track if image came from color or grayscale\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    # Group ages\n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    # Filter classes with enough samples\n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    print(f\"Source distribution: {dict(Counter(filtered_sources))}\")\n",
    "    \n",
    "    return np.array(filtered_images), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image(image):\n",
    "    \"\"\"Enhanced augmentation for deer images with strategic color conversion\"\"\"\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Rotation\n",
    "    if random.random() < 0.7:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Strategic color conversion (RGB -> Grayscale -> RGB)\n",
    "    # Based on ensemble results showing grayscale superiority\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.4:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Brightness/contrast\n",
    "    if random.random() < 0.8:\n",
    "        alpha = random.uniform(0.7, 1.3)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Gamma correction\n",
    "    if random.random() < 0.4:\n",
    "        gamma = random.uniform(0.8, 1.2)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    # Noise\n",
    "    if random.random() < 0.3:\n",
    "        noise = np.random.normal(0, 7, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_balanced_dataset(X, y):\n",
    "    \"\"\"Create balanced dataset through augmentation\"\"\"\n",
    "    print(f\"\\nOriginal class distribution:\")\n",
    "    class_counts = Counter(y)\n",
    "    for class_idx, count in sorted(class_counts.items()):\n",
    "        print(f\"  Class {class_idx}: {count} images\")\n",
    "    \n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max(AUGMENTATION_TARGET, max_count)\n",
    "    \n",
    "    print(f\"\\nTarget samples per class: {target_count}\")\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for class_idx in range(len(set(y))):\n",
    "        class_mask = np.array(y) == class_idx\n",
    "        class_images = X[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        if current_count == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add originals\n",
    "        X_balanced.extend(class_images)\n",
    "        y_balanced.extend([class_idx] * current_count)\n",
    "        \n",
    "        # Add augmented to reach target\n",
    "        needed = target_count - current_count\n",
    "        augmented_for_class = 0\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image(class_images[orig_idx].copy())\n",
    "            X_balanced.append(aug_img)\n",
    "            y_balanced.append(class_idx)\n",
    "            augmented_for_class += 1\n",
    "        \n",
    "        print(f\"  Class {class_idx}: {current_count} original + {augmented_for_class} augmented = {current_count + augmented_for_class} total\")\n",
    "    \n",
    "    # Verify final balance\n",
    "    final_counts = Counter(y_balanced)\n",
    "    print(f\"\\nFinal balanced class distribution:\")\n",
    "    for class_idx, count in sorted(final_counts.items()):\n",
    "        print(f\"  Class {class_idx}: {count} images\")\n",
    "    \n",
    "    print(f\"Total training images after balancing: {len(X_balanced)}\")\n",
    "    \n",
    "    return np.array(X_balanced), np.array(y_balanced)\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    def __init__(self, X, y, training=True):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.training = training\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Ensure CHW format\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Test time augmentation for validation\n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class SingleModelTrainer:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"single_model_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    def create_model(self, architecture):\n",
    "        \"\"\"Create model with optimized head for diverse architectures\"\"\"\n",
    "        model = timm.create_model(architecture, pretrained=True, num_classes=self.num_classes)\n",
    "        \n",
    "        # Architecture-specific layer freezing\n",
    "        if any(arch in architecture for arch in ['resnet', 'resnext', 'seresnet']):\n",
    "            frozen_layers = ['conv1', 'bn1', 'layer1', 'layer2']\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in frozen_layers):\n",
    "                    param.requires_grad = False\n",
    "        elif 'efficientnet' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(block in name for block in ['blocks.0', 'blocks.1', 'blocks.2']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'densenet' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['features.conv0', 'features.norm0', 'features.denseblock1']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'hrnet' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['conv1', 'bn1', 'stage1']):\n",
    "                    param.requires_grad = False\n",
    "        elif any(arch in architecture for arch in ['mobilenet', 'ghostnet']):\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['features.0', 'features.1', 'features.2']):\n",
    "                    param.requires_grad = False\n",
    "        elif any(arch in architecture for arch in ['vit', 'swin', 'mobilevit']):\n",
    "            # Freeze patch embedding and early transformer blocks\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['patch_embed', 'blocks.0', 'blocks.1', 'layers.0']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'regnet' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['stem', 's1']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'convnext' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['stem', 'stages.0']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'maxvit' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['stem', 'stages.0']):\n",
    "                    param.requires_grad = False\n",
    "        elif 'repvgg' in architecture:\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['stage0', 'stage1']):\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # Replace classifier head based on architecture\n",
    "        if hasattr(model, 'fc'):\n",
    "            in_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        elif hasattr(model, 'classifier'):\n",
    "            if hasattr(model.classifier, 'in_features'):\n",
    "                in_features = model.classifier.in_features\n",
    "            else:\n",
    "                in_features = model.classifier[-1].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        elif hasattr(model, 'head'):\n",
    "            if hasattr(model.head, 'in_features'):\n",
    "                in_features = model.head.in_features\n",
    "            else:\n",
    "                in_features = model.head[-1].in_features if hasattr(model.head, '__getitem__') else 512\n",
    "            model.head = nn.Sequential(\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(in_features, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def train_single_architecture(self, train_loader, val_loader, architecture):\n",
    "        \"\"\"Train a single architecture\"\"\"\n",
    "        print(f\"Training {architecture}...\")\n",
    "        \n",
    "        model = self.create_model(architecture)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Separate learning rates\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if any(head in name for head in ['fc', 'classifier', 'head']):\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': 0.0002},\n",
    "            {'params': classifier_params, 'lr': 0.0008}\n",
    "        ], weight_decay=0.02)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80, eta_min=1e-6)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 25  # Increased patience\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(80):  # Increased max epochs\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % 20 == 0 and epoch > 0:\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"    Early stopping at epoch {epoch} (patience reached)\")\n",
    "                break\n",
    "            \n",
    "            # Memory management for RTX 2060\n",
    "            if epoch % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load best weights\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        print(f\"  {architecture} best validation: {best_val_acc:.1f}%\")\n",
    "        return model, best_val_acc\n",
    "    \n",
    "    def find_best_architecture(self, train_loader, val_loader, test_loader, label_mapping):\n",
    "        \"\"\"Test all architectures and return the best\"\"\"\n",
    "        results = {}\n",
    "        failed_archs = []\n",
    "        \n",
    "        print(f\"Testing {len(TOP_ARCHITECTURES)} diverse architectural families...\")\n",
    "        for i, arch in enumerate(TOP_ARCHITECTURES, 1):\n",
    "            try:\n",
    "                print(f\"[{i:2d}/{len(TOP_ARCHITECTURES)}] Testing {arch}...\")\n",
    "                model, val_acc = self.train_single_architecture(train_loader, val_loader, arch)\n",
    "                \n",
    "                # Evaluate on test set for comparison (not selection)\n",
    "                test_acc = evaluate_model(model, test_loader, self.device)\n",
    "                \n",
    "                results[arch] = (model, val_acc, test_acc)\n",
    "                print(f\"  ‚úì {arch}: Val {val_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "                \n",
    "                # Save each model\n",
    "                save_path = os.path.join(self.save_dir, f\"deer_age_model_{arch}_{test_acc:.1f}pct.pth\")\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'architecture': arch,\n",
    "                    'num_classes': self.num_classes,\n",
    "                    'label_mapping': label_mapping,\n",
    "                    'test_accuracy': test_acc,\n",
    "                    'val_accuracy': val_acc,\n",
    "                    'input_size': IMAGE_SIZE\n",
    "                }, save_path)\n",
    "                print(f\"  Model saved: {save_path}\")\n",
    "                \n",
    "                print()  # Add blank line between architectures\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó {arch} failed: {str(e)[:50]}...\")\n",
    "                print()  # Add blank line for failed architectures too\n",
    "                failed_archs.append(arch)\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "        \n",
    "        if not results:\n",
    "            raise ValueError(\"All architectures failed to train!\")\n",
    "        \n",
    "        # Find best based on VALIDATION (not test) to avoid contamination\n",
    "        best_arch = max(results.keys(), key=lambda x: results[x][1])\n",
    "        best_model, best_val_acc, best_test_acc = results[best_arch]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ARCHITECTURE COMPARISON RESULTS\")\n",
    "        print('='*60)\n",
    "        print(f\"{'Rank':<4} {'Architecture':<30} {'Validation':<12} {'Test':<8}\")\n",
    "        print('-'*60)\n",
    "        \n",
    "        # Sort by validation performance for ranking\n",
    "        sorted_results = sorted(results.items(), key=lambda x: x[1][1], reverse=True)\n",
    "        for i, (arch, (_, val_acc, test_acc)) in enumerate(sorted_results, 1):\n",
    "            marker = \"üèÜ\" if arch == best_arch else \"  \"\n",
    "            print(f\"{i:2d}. {marker} {arch:<28} {val_acc:5.1f}%      {test_acc:5.1f}%\")\n",
    "        \n",
    "        if failed_archs:\n",
    "            print(f\"\\nFailed architectures ({len(failed_archs)}): {', '.join(failed_archs)}\")\n",
    "        \n",
    "        print(f\"\\nüèÜ WINNER: {best_arch} (Val: {best_val_acc:.1f}%, Test: {best_test_acc:.1f}%)\")\n",
    "        print(\"Note: Selection based on validation performance to avoid test contamination\")\n",
    "        \n",
    "        return best_model, best_arch, best_val_acc\n",
    "    \n",
    "    def final_optimization(self, model, train_loader, val_loader, architecture):\n",
    "        \"\"\"Final optimization of the best model\"\"\"\n",
    "        print(f\"\\nFinal optimization of {architecture}...\")\n",
    "        \n",
    "        # Unfreeze more layers for fine-tuning\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Lower learning rate for fine-tuning\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-7)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 15\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    # Test time augmentation\n",
    "                    outputs1 = model(images)\n",
    "                    flipped = torch.flip(images, [3])\n",
    "                    outputs2 = model(flipped)\n",
    "                    outputs = (outputs1 + outputs2) / 2\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"  Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        print(f\"  Final optimization complete: {best_val_acc:.1f}%\")\n",
    "        return model, best_val_acc\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Test time augmentation\n",
    "            outputs1 = model(images)\n",
    "            flipped = torch.flip(images, [3])\n",
    "            outputs2 = model(flipped)\n",
    "            outputs = (outputs1 + outputs2) / 2\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * test_correct / test_total\n",
    "    return test_acc\n",
    "\n",
    "def main():\n",
    "    print(\"Single Optimized Deer Age Prediction Model\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load combined data\n",
    "    images, ages, sources = load_combined_data()\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    y_indices = np.array([label_mapping[age] for age in ages])\n",
    "    \n",
    "    print(f\"\\nClasses: {len(unique_ages)}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images, y_indices, test_size=0.2, random_state=42, stratify=y_indices\n",
    "    )\n",
    "    \n",
    "    # Further split training into train/val\n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"Train: {len(X_train_final)} images\")\n",
    "    print(f\"Val: {len(X_val)} images\") \n",
    "    print(f\"Test: {len(X_test)} images\")\n",
    "    \n",
    "    # Create balanced training set\n",
    "    print(\"\\nCreating balanced training set...\")\n",
    "    X_train_balanced, y_train_balanced = create_balanced_dataset(X_train_final, y_train_final)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DeerDataset(X_train_balanced, y_train_balanced, training=True)\n",
    "    val_dataset = DeerDataset(X_val, y_val, training=False)\n",
    "    test_dataset = DeerDataset(X_test, y_test, training=False)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = SingleModelTrainer(num_classes=len(unique_ages))\n",
    "    \n",
    "    # Find best architecture\n",
    "    best_model, best_arch, val_acc = trainer.find_best_architecture(train_loader, val_loader, test_loader, label_mapping)\n",
    "    \n",
    "    # Final optimization\n",
    "    optimized_model, final_val_acc = trainer.final_optimization(best_model, train_loader, val_loader, best_arch)\n",
    "    \n",
    "    # Test evaluation\n",
    "    test_acc = evaluate_model(optimized_model, test_loader, trainer.device)\n",
    "    \n",
    "    # Save final model\n",
    "    save_path = os.path.join(trainer.save_dir, f\"deer_age_model_{best_arch}_{test_acc:.1f}pct_FINAL.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': optimized_model.state_dict(),\n",
    "        'architecture': best_arch,\n",
    "        'num_classes': len(unique_ages),\n",
    "        'label_mapping': label_mapping,\n",
    "        'test_accuracy': test_acc,\n",
    "        'val_accuracy': final_val_acc,\n",
    "        'input_size': IMAGE_SIZE\n",
    "    }, save_path)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Best architecture: {best_arch}\")\n",
    "    print(f\"Validation accuracy: {final_val_acc:.1f}%\")\n",
    "    print(f\"Test accuracy: {test_acc:.1f}%\")\n",
    "    print(f\"Training time: {elapsed:.1f} minutes\")\n",
    "    print(f\"Final model saved: {save_path}\")\n",
    "    \n",
    "    return optimized_model, best_arch, test_acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, architecture, accuracy = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9eeae-fac6-4e5c-8470-2c2ad0050976",
   "metadata": {},
   "source": [
    "### Addressing algorithm robustness / deer detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4cb65a-1514-4cc1-8243-f14e4b812477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Method Deer Age Prediction Comparison\n",
      "============================================================\n",
      "Loading NDA_color images...\n",
      "  Loaded 363 images from NDA_color\n",
      "Loading NDA_grayscale images...\n",
      "  Loaded 108 images from NDA_grayscale\n",
      "Total images: 471\n",
      "Final dataset: 471 images\n",
      "Age distribution: {5.5: 121, 4.5: 89, 2.5: 83, 3.5: 111, 1.5: 67}\n",
      "Source distribution: {'NDA_color': 363, 'NDA_grayscale': 108}\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Classes: 5\n",
      "Label mapping: {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
      "Data split - Train: 300, Val: 76, Test: 95\n",
      "\n",
      "============================================================\n",
      "TESTING METHOD: BASELINE\n",
      "============================================================\n",
      "\n",
      "[baseline] Testing efficientnet_b0...\n",
      "\n",
      "Creating balanced dataset for method: baseline\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 79.5%, Val 55.3%\n",
      "    Epoch 15: Train 100.0%, Val 67.1%\n",
      "    Epoch 30: Train 100.0%, Val 69.7%\n",
      "  efficientnet_b0: Val 72.4%, Test 55.8%\n",
      "\n",
      "[baseline] Testing resnet34...\n",
      "\n",
      "Creating balanced dataset for method: baseline\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 60.2%, Val 60.5%\n",
      "    Epoch 15: Train 100.0%, Val 71.1%\n",
      "  resnet34: Val 71.1%, Test 64.2%\n",
      "\n",
      "[baseline] Testing mobilenetv3_large_100...\n",
      "\n",
      "Creating balanced dataset for method: baseline\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 79.5%, Val 56.6%\n",
      "    Epoch 15: Train 100.0%, Val 67.1%\n",
      "  mobilenetv3_large_100: Val 71.1%, Test 50.5%\n",
      "\n",
      "[baseline] Testing ghostnet_100...\n",
      "\n",
      "Creating balanced dataset for method: baseline\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 77.2%, Val 72.4%\n",
      "    Epoch 15: Train 100.0%, Val 69.7%\n",
      "  ghostnet_100: Val 73.7%, Test 57.9%\n",
      "\n",
      "============================================================\n",
      "TESTING METHOD: ROBUST_TRAINING\n",
      "============================================================\n",
      "\n",
      "[robust_training] Testing efficientnet_b0...\n",
      "\n",
      "Creating balanced dataset for method: robust_training\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 70.3%, Val 64.5%\n",
      "    Epoch 15: Train 100.0%, Val 72.4%\n",
      "  efficientnet_b0: Val 72.4%, Test 58.9%\n",
      "\n",
      "[robust_training] Testing resnet34...\n",
      "\n",
      "Creating balanced dataset for method: robust_training\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 58.5%, Val 63.2%\n",
      "    Epoch 15: Train 99.4%, Val 68.4%\n",
      "  resnet34: Val 72.4%, Test 60.0%\n",
      "\n",
      "[robust_training] Testing mobilenetv3_large_100...\n",
      "\n",
      "Creating balanced dataset for method: robust_training\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 68.8%, Val 52.6%\n",
      "    Epoch 15: Train 100.0%, Val 65.8%\n",
      "    Epoch 30: Train 100.0%, Val 72.4%\n",
      "    Epoch 45: Train 100.0%, Val 76.3%\n",
      "  mobilenetv3_large_100: Val 78.9%, Test 64.2%\n",
      "\n",
      "[robust_training] Testing ghostnet_100...\n",
      "\n",
      "Creating balanced dataset for method: robust_training\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 73.5%, Val 68.4%\n",
      "    Epoch 15: Train 100.0%, Val 68.4%\n",
      "    Epoch 30: Train 100.0%, Val 68.4%\n",
      "  ghostnet_100: Val 76.3%, Test 54.7%\n",
      "\n",
      "============================================================\n",
      "TESTING METHOD: UNCERTAINTY_AWARE\n",
      "============================================================\n",
      "\n",
      "[uncertainty_aware] Testing efficientnet_b0...\n",
      "\n",
      "Creating balanced dataset for method: uncertainty_aware\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 82.7%, Val 65.8%\n",
      "    Epoch 15: Train 99.9%, Val 68.4%\n",
      "    Epoch 30: Train 99.9%, Val 69.7%\n",
      "    Epoch 45: Train 100.0%, Val 69.7%\n",
      "  efficientnet_b0: Val 75.0%, Test 63.2%\n",
      "\n",
      "[uncertainty_aware] Testing resnet34...\n",
      "\n",
      "Creating balanced dataset for method: uncertainty_aware\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 54.3%, Val 57.9%\n",
      "    Epoch 15: Train 99.5%, Val 65.8%\n",
      "  resnet34: Val 71.1%, Test 56.8%\n",
      "\n",
      "[uncertainty_aware] Testing mobilenetv3_large_100...\n",
      "\n",
      "Creating balanced dataset for method: uncertainty_aware\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 79.8%, Val 65.8%\n",
      "    Epoch 15: Train 99.6%, Val 71.1%\n",
      "    Epoch 30: Train 100.0%, Val 68.4%\n",
      "  mobilenetv3_large_100: Val 71.1%, Test 53.7%\n",
      "\n",
      "[uncertainty_aware] Testing ghostnet_100...\n",
      "\n",
      "Creating balanced dataset for method: uncertainty_aware\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 70.5%, Val 64.5%\n",
      "    Epoch 15: Train 99.7%, Val 71.1%\n",
      "  ghostnet_100: Val 71.1%, Test 57.9%\n",
      "\n",
      "============================================================\n",
      "TESTING METHOD: CROP_ROBUST\n",
      "============================================================\n",
      "\n",
      "[crop_robust] Testing efficientnet_b0...\n",
      "\n",
      "Creating balanced dataset for method: crop_robust\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 71.2%, Val 55.3%\n",
      "    Epoch 15: Train 100.0%, Val 69.7%\n",
      "    Epoch 30: Train 100.0%, Val 69.7%\n",
      "  efficientnet_b0: Val 73.7%, Test 52.6%\n",
      "\n",
      "[crop_robust] Testing resnet34...\n",
      "\n",
      "Creating balanced dataset for method: crop_robust\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 43.6%, Val 51.3%\n",
      "    Epoch 15: Train 99.9%, Val 63.2%\n",
      "  resnet34: Val 67.1%, Test 57.9%\n",
      "\n",
      "[crop_robust] Testing mobilenetv3_large_100...\n",
      "\n",
      "Creating balanced dataset for method: crop_robust\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 69.3%, Val 52.6%\n",
      "    Epoch 15: Train 100.0%, Val 61.8%\n",
      "    Epoch 30: Train 100.0%, Val 69.7%\n",
      "    Epoch 45: Train 100.0%, Val 68.4%\n",
      "  mobilenetv3_large_100: Val 72.4%, Test 52.6%\n",
      "\n",
      "[crop_robust] Testing ghostnet_100...\n",
      "\n",
      "Creating balanced dataset for method: crop_robust\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 66.3%, Val 57.9%\n",
      "    Epoch 15: Train 99.6%, Val 68.4%\n",
      "  ghostnet_100: Val 72.4%, Test 56.8%\n",
      "\n",
      "============================================================\n",
      "TESTING METHOD: MIXED_APPROACH\n",
      "============================================================\n",
      "\n",
      "[mixed_approach] Testing efficientnet_b0...\n",
      "\n",
      "Creating balanced dataset for method: mixed_approach\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 60.5%, Val 55.3%\n",
      "    Epoch 15: Train 100.0%, Val 68.4%\n",
      "  efficientnet_b0: Val 72.4%, Test 52.6%\n",
      "\n",
      "[mixed_approach] Testing resnet34...\n",
      "\n",
      "Creating balanced dataset for method: mixed_approach\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 37.5%, Val 44.7%\n",
      "    Epoch 15: Train 100.0%, Val 61.8%\n",
      "    Epoch 30: Train 100.0%, Val 63.2%\n",
      "  resnet34: Val 76.3%, Test 49.5%\n",
      "\n",
      "[mixed_approach] Testing mobilenetv3_large_100...\n",
      "\n",
      "Creating balanced dataset for method: mixed_approach\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 60.9%, Val 56.6%\n",
      "    Epoch 15: Train 100.0%, Val 67.1%\n",
      "    Epoch 30: Train 100.0%, Val 69.7%\n",
      "    Epoch 45: Train 100.0%, Val 68.4%\n",
      "  mobilenetv3_large_100: Val 75.0%, Test 52.6%\n",
      "\n",
      "[mixed_approach] Testing ghostnet_100...\n",
      "\n",
      "Creating balanced dataset for method: mixed_approach\n",
      "Total balanced images: 4000\n",
      "    Epoch 0: Train 58.6%, Val 69.7%\n",
      "    Epoch 15: Train 99.5%, Val 68.4%\n",
      "    Epoch 30: Train 99.9%, Val 65.8%\n",
      "  ghostnet_100: Val 73.7%, Test 52.6%\n",
      "\n",
      "================================================================================\n",
      "FINAL METHOD COMPARISON RESULTS\n",
      "================================================================================\n",
      "Method               Architecture         Val Acc    Test Acc  \n",
      "--------------------------------------------------------------------------------\n",
      "üèÜ baseline           resnet34               71.1%      64.2%\n",
      "   robust_training    mobilenetv3_large_100   78.9%      64.2%\n",
      "   uncertainty_aware  efficientnet_b0        75.0%      63.2%\n",
      "   robust_training    resnet34               72.4%      60.0%\n",
      "   robust_training    efficientnet_b0        72.4%      58.9%\n",
      "   baseline           ghostnet_100           73.7%      57.9%\n",
      "   uncertainty_aware  ghostnet_100           71.1%      57.9%\n",
      "   crop_robust        resnet34               67.1%      57.9%\n",
      "   uncertainty_aware  resnet34               71.1%      56.8%\n",
      "   crop_robust        ghostnet_100           72.4%      56.8%\n",
      "   baseline           efficientnet_b0        72.4%      55.8%\n",
      "   robust_training    ghostnet_100           76.3%      54.7%\n",
      "   uncertainty_aware  mobilenetv3_large_100   71.1%      53.7%\n",
      "   crop_robust        efficientnet_b0        73.7%      52.6%\n",
      "   crop_robust        mobilenetv3_large_100   72.4%      52.6%\n",
      "   mixed_approach     efficientnet_b0        72.4%      52.6%\n",
      "   mixed_approach     mobilenetv3_large_100   75.0%      52.6%\n",
      "   mixed_approach     ghostnet_100           73.7%      52.6%\n",
      "   baseline           mobilenetv3_large_100   71.1%      50.5%\n",
      "   mixed_approach     resnet34               76.3%      49.5%\n",
      "\n",
      "============================================================\n",
      "METHOD SUMMARIES (Best result per method):\n",
      "============================================================\n",
      "baseline            :  64.2% (using resnet34)\n",
      "robust_training     :  64.2% (using mobilenetv3_large_100)\n",
      "uncertainty_aware   :  63.2% (using efficientnet_b0)\n",
      "crop_robust         :  57.9% (using resnet34)\n",
      "mixed_approach      :  52.6% (using efficientnet_b0)\n",
      "\n",
      "Total comparison time: 204.1 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Test different approaches to handle multi-source label inconsistency\n",
    "METHODS_TO_TEST = [\n",
    "    'baseline',                    # Original approach\n",
    "    'robust_training',            # Heavy label smoothing + high dropout\n",
    "    'uncertainty_aware',          # Predict confidence alongside age\n",
    "    'crop_robust',               # Multiple crop augmentation\n",
    "    'mixed_approach'             # Combination of techniques\n",
    "]\n",
    "\n",
    "# Efficient architectures for comparison\n",
    "TEST_ARCHITECTURES = [\n",
    "    'efficientnet_b0',\n",
    "    'resnet34', \n",
    "    'mobilenetv3_large_100',\n",
    "    'ghostnet_100'\n",
    "]\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUGMENTATION_TARGET = 800  # Reduced for faster training during testing\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    \"\"\"Detect if image is grayscale and convert to 3-channel RGB\"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_multi_source_data():\n",
    "    \"\"\"Load data from multiple sources - modify paths as needed\"\"\"\n",
    "    # Update these paths to your actual data locations\n",
    "    data_sources = [\n",
    "        (\"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*.png\", \"NDA_color\"),\n",
    "        (\"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*.png\", \"NDA_grayscale\"),\n",
    "        # Add more sources here:\n",
    "        # (\"path\\\\to\\\\source2\\\\*.jpg\", \"source2\"),\n",
    "        # (\"path\\\\to\\\\source3\\\\*.png\", \"source3\"),\n",
    "    ]\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    for path_pattern, source_name in data_sources:\n",
    "        print(f\"Loading {source_name} images...\")\n",
    "        files = glob.glob(path_pattern)\n",
    "        source_count = 0\n",
    "        \n",
    "        for img_path in files:\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                \n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = detect_and_convert_image(img)\n",
    "                img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "                \n",
    "                filename = os.path.basename(img_path)\n",
    "                filename_no_ext = os.path.splitext(filename)[0]\n",
    "                parts = filename_no_ext.split('_')\n",
    "                \n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                \n",
    "                age_part = parts[3]\n",
    "                if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    age_value = float(age_part.replace('p', '.'))\n",
    "                    images.append(img_resized)\n",
    "                    ages.append(age_value)\n",
    "                    sources.append(source_name)\n",
    "                    source_count += 1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"  Loaded {source_count} images from {source_name}\")\n",
    "    \n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    # Group ages and filter\n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    print(f\"Source distribution: {dict(Counter(filtered_sources))}\")\n",
    "    \n",
    "    return np.array(filtered_images), filtered_ages, filtered_sources\n",
    "\n",
    "def smart_crop_augmentation(image, crop_probability=0.7):\n",
    "    \"\"\"Intelligent cropping that tries to keep the deer in frame\"\"\"\n",
    "    if random.random() > crop_probability:\n",
    "        return image\n",
    "    \n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Different crop strategies - some more conservative, some more aggressive\n",
    "    crop_strategies = [\n",
    "        (0.8, 0.8),   # Mild crop\n",
    "        (0.7, 0.7),   # Moderate crop  \n",
    "        (0.6, 0.8),   # Horizontal crop\n",
    "        (0.8, 0.6),   # Vertical crop\n",
    "        (0.6, 0.6),   # Aggressive crop\n",
    "    ]\n",
    "    \n",
    "    crop_h_factor, crop_w_factor = random.choice(crop_strategies)\n",
    "    \n",
    "    new_h = int(h * crop_h_factor)\n",
    "    new_w = int(w * crop_w_factor)\n",
    "    \n",
    "    # Random starting position, but avoid extreme edges\n",
    "    max_start_y = h - new_h\n",
    "    max_start_x = w - new_w\n",
    "    \n",
    "    # Bias towards center (where deer likely is)\n",
    "    start_y = random.randint(max(0, max_start_y//4), max(max_start_y//4, max_start_y*3//4))\n",
    "    start_x = random.randint(max(0, max_start_x//4), max(max_start_x//4, max_start_x*3//4))\n",
    "    \n",
    "    cropped = image[start_y:start_y+new_h, start_x:start_x+new_w]\n",
    "    \n",
    "    # Resize back to original size\n",
    "    return cv2.resize(cropped, IMAGE_SIZE[::-1])\n",
    "\n",
    "def enhanced_augment_image(image, method='baseline'):\n",
    "    \"\"\"Method-specific augmentation strategies\"\"\"\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Base augmentations for all methods\n",
    "    if random.random() < 0.7:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Crop-robust augmentation\n",
    "    if method in ['crop_robust', 'mixed_approach']:\n",
    "        image = smart_crop_augmentation(image, crop_probability=0.8)\n",
    "    \n",
    "    # Color/contrast augmentation\n",
    "    if random.random() < 0.8:\n",
    "        alpha = random.uniform(0.7, 1.3)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Strategic grayscale conversion\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.4:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Additional heavy augmentation for robust methods\n",
    "    if method in ['robust_training', 'mixed_approach']:\n",
    "        # More aggressive noise and gamma\n",
    "        if random.random() < 0.5:\n",
    "            gamma = random.uniform(0.7, 1.4)\n",
    "            inv_gamma = 1.0 / gamma\n",
    "            table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "            image = cv2.LUT(image, table)\n",
    "        \n",
    "        if random.random() < 0.4:\n",
    "            noise = np.random.normal(0, 10, image.shape).astype(np.int16)\n",
    "            image_int16 = image.astype(np.int16)\n",
    "            noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "            image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_balanced_dataset(X, y, sources, method='baseline'):\n",
    "    \"\"\"Create balanced dataset with method-specific augmentation\"\"\"\n",
    "    print(f\"\\nCreating balanced dataset for method: {method}\")\n",
    "    \n",
    "    class_counts = Counter(y)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max(AUGMENTATION_TARGET, max_count)\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    sources_balanced = []\n",
    "    \n",
    "    for class_idx in range(len(set(y))):\n",
    "        class_mask = np.array(y) == class_idx\n",
    "        class_images = X[class_mask]\n",
    "        class_sources = np.array(sources)[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        if current_count == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add originals\n",
    "        X_balanced.extend(class_images)\n",
    "        y_balanced.extend([class_idx] * current_count)\n",
    "        sources_balanced.extend(class_sources)\n",
    "        \n",
    "        # Add augmented\n",
    "        needed = target_count - current_count\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image(class_images[orig_idx].copy(), method=method)\n",
    "            X_balanced.append(aug_img)\n",
    "            y_balanced.append(class_idx)\n",
    "            sources_balanced.append(class_sources[orig_idx])\n",
    "    \n",
    "    print(f\"Total balanced images: {len(X_balanced)}\")\n",
    "    return np.array(X_balanced), np.array(y_balanced), sources_balanced\n",
    "\n",
    "class RobustDeerDataset(Dataset):\n",
    "    def __init__(self, X, y, sources=None, training=True, method='baseline'):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.sources = sources\n",
    "        self.training = training\n",
    "        self.method = method\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Test time augmentation for validation/test\n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        # Return source info for methods that need it\n",
    "        if self.sources is not None:\n",
    "            return image, label, self.sources[idx]\n",
    "        return image, label\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"Standard classification model\"\"\"\n",
    "    def __init__(self, architecture, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(architecture, pretrained=True, num_classes=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "class UncertaintyAwareModel(nn.Module):\n",
    "    \"\"\"Model that predicts both age and confidence\"\"\"\n",
    "    def __init__(self, architecture, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(architecture, pretrained=True, num_classes=1000)\n",
    "        \n",
    "        # Get feature dimension\n",
    "        if hasattr(self.backbone, 'fc'):\n",
    "            feat_dim = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        elif hasattr(self.backbone, 'classifier'):\n",
    "            feat_dim = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif hasattr(self.backbone, 'head'):\n",
    "            feat_dim = self.backbone.head.in_features\n",
    "            self.backbone.head = nn.Identity()\n",
    "        else:\n",
    "            feat_dim = 1000\n",
    "        \n",
    "        self.age_head = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(feat_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.confidence_head = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(feat_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        age_logits = self.age_head(features)\n",
    "        confidence = self.confidence_head(features)\n",
    "        return age_logits, confidence\n",
    "\n",
    "class MethodTrainer:\n",
    "    def __init__(self, method, architecture, num_classes, device):\n",
    "        self.method = method\n",
    "        self.architecture = architecture\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Create model based on method\n",
    "        if method == 'uncertainty_aware':\n",
    "            self.model = UncertaintyAwareModel(architecture, num_classes)\n",
    "        else:\n",
    "            self.model = BaselineModel(architecture, num_classes)\n",
    "        \n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        # Method-specific parameters\n",
    "        if method == 'robust_training':\n",
    "            self.label_smoothing = 0.25\n",
    "            self.dropout_rate = 0.5\n",
    "        elif method == 'mixed_approach':\n",
    "            self.label_smoothing = 0.2\n",
    "            self.dropout_rate = 0.4\n",
    "        else:\n",
    "            self.label_smoothing = 0.1\n",
    "            self.dropout_rate = 0.3\n",
    "    \n",
    "    def get_criterion(self):\n",
    "        if self.method == 'uncertainty_aware':\n",
    "            return self.uncertainty_loss\n",
    "        else:\n",
    "            return nn.CrossEntropyLoss(label_smoothing=self.label_smoothing)\n",
    "    \n",
    "    def uncertainty_loss(self, predictions, targets):\n",
    "        age_logits, confidence = predictions\n",
    "        age_loss = F.cross_entropy(age_logits, targets, reduction='none')\n",
    "        \n",
    "        # Weight loss by confidence (lower confidence = higher weight for learning)\n",
    "        weighted_loss = age_loss * (2.0 - confidence.squeeze())\n",
    "        \n",
    "        # Add confidence regularization\n",
    "        conf_loss = -torch.mean(torch.log(confidence + 1e-8))  # Encourage confidence\n",
    "        \n",
    "        return weighted_loss.mean() + 0.1 * conf_loss\n",
    "    \n",
    "    def train_model(self, train_loader, val_loader, max_epochs=60):\n",
    "        criterion = self.get_criterion()\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=0.0003, weight_decay=0.02)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 15\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                if len(batch) == 3:  # Has source info\n",
    "                    images, labels, sources = batch\n",
    "                else:\n",
    "                    images, labels = batch\n",
    "                \n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                \n",
    "                if self.method == 'uncertainty_aware':\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    age_logits, _ = outputs\n",
    "                    _, predicted = torch.max(age_logits, 1)\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            val_acc = self.evaluate(val_loader)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = self.model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % 15 == 0:\n",
    "                train_acc = 100 * train_correct / train_total\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        # Load best weights\n",
    "        if best_state is not None:\n",
    "            self.model.load_state_dict(best_state)\n",
    "        \n",
    "        return best_val_acc\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                if len(batch) == 3:\n",
    "                    images, labels, sources = batch\n",
    "                else:\n",
    "                    images, labels = batch\n",
    "                \n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                \n",
    "                if self.method == 'uncertainty_aware':\n",
    "                    age_logits, _ = outputs\n",
    "                    _, predicted = torch.max(age_logits, 1)\n",
    "                else:\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return 100 * correct / total\n",
    "\n",
    "class MultiMethodComparison:\n",
    "    def __init__(self, save_dir=None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"multi_method_comparison_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    \n",
    "    def run_comparison(self, images, ages, sources):\n",
    "        unique_ages = sorted(list(set(ages)))\n",
    "        label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "        y_indices = np.array([label_mapping[age] for age in ages])\n",
    "        \n",
    "        print(f\"Classes: {len(unique_ages)}\")\n",
    "        print(f\"Label mapping: {label_mapping}\")\n",
    "        \n",
    "        # Single train/test split for fair comparison\n",
    "        X_train, X_test, y_train, y_test, sources_train, sources_test = train_test_split(\n",
    "            images, y_indices, sources, test_size=0.2, random_state=42, stratify=y_indices\n",
    "        )\n",
    "        \n",
    "        X_train_final, X_val, y_train_final, y_val, sources_train_final, sources_val = train_test_split(\n",
    "            X_train, y_train, sources_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(f\"Data split - Train: {len(X_train_final)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Test each method with each architecture\n",
    "        for method in METHODS_TO_TEST:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"TESTING METHOD: {method.upper()}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            method_results = {}\n",
    "            \n",
    "            for arch in TEST_ARCHITECTURES:\n",
    "                print(f\"\\n[{method}] Testing {arch}...\")\n",
    "                \n",
    "                try:\n",
    "                    # Create method-specific balanced dataset\n",
    "                    X_balanced, y_balanced, sources_balanced = create_balanced_dataset(\n",
    "                        X_train_final, y_train_final, sources_train_final, method=method\n",
    "                    )\n",
    "                    \n",
    "                    # Create datasets\n",
    "                    train_dataset = RobustDeerDataset(X_balanced, y_balanced, sources_balanced, training=True, method=method)\n",
    "                    val_dataset = RobustDeerDataset(X_val, y_val, sources_val, training=False, method=method)\n",
    "                    test_dataset = RobustDeerDataset(X_test, y_test, sources_test, training=False, method=method)\n",
    "                    \n",
    "                    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "                    \n",
    "                    # Train model\n",
    "                    trainer = MethodTrainer(method, arch, len(unique_ages), self.device)\n",
    "                    val_acc = trainer.train_model(train_loader, val_loader)\n",
    "                    test_acc = trainer.evaluate(test_loader)\n",
    "                    \n",
    "                    method_results[arch] = {\n",
    "                        'val_acc': val_acc,\n",
    "                        'test_acc': test_acc,\n",
    "                        'model_state': trainer.model.state_dict().copy()\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  {arch}: Val {val_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "                    \n",
    "                    # Save model\n",
    "                    save_path = os.path.join(self.save_dir, f\"{method}_{arch}_{test_acc:.1f}pct.pth\")\n",
    "                    torch.save({\n",
    "                        'model_state_dict': trainer.model.state_dict(),\n",
    "                        'method': method,\n",
    "                        'architecture': arch,\n",
    "                        'num_classes': len(unique_ages),\n",
    "                        'label_mapping': label_mapping,\n",
    "                        'test_accuracy': test_acc,\n",
    "                        'val_accuracy': val_acc\n",
    "                    }, save_path)\n",
    "                    \n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  {arch} FAILED: {str(e)[:60]}...\")\n",
    "                    continue\n",
    "            \n",
    "            results[method] = method_results\n",
    "        \n",
    "        # Print final comparison\n",
    "        self.print_final_results(results)\n",
    "        \n",
    "        # Save results\n",
    "        results_path = os.path.join(self.save_dir, \"method_comparison_results.json\")\n",
    "        with open(results_path, 'w') as f:\n",
    "            # Convert to serializable format\n",
    "            serializable_results = {}\n",
    "            for method, arch_results in results.items():\n",
    "                serializable_results[method] = {}\n",
    "                for arch, metrics in arch_results.items():\n",
    "                    serializable_results[method][arch] = {\n",
    "                        'val_acc': metrics['val_acc'],\n",
    "                        'test_acc': metrics['test_acc']\n",
    "                    }\n",
    "            json.dump(serializable_results, f, indent=2)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_final_results(self, results):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"FINAL METHOD COMPARISON RESULTS\")\n",
    "        print('='*80)\n",
    "        print(f\"{'Method':<20} {'Architecture':<20} {'Val Acc':<10} {'Test Acc':<10}\")\n",
    "        print('-'*80)\n",
    "        \n",
    "        all_results = []\n",
    "        for method, arch_results in results.items():\n",
    "            for arch, metrics in arch_results.items():\n",
    "                all_results.append((method, arch, metrics['val_acc'], metrics['test_acc']))\n",
    "        \n",
    "        # Sort by test accuracy\n",
    "        all_results.sort(key=lambda x: x[3], reverse=True)\n",
    "        \n",
    "        for i, (method, arch, val_acc, test_acc) in enumerate(all_results):\n",
    "            marker = \"üèÜ\" if i == 0 else \"  \"\n",
    "            print(f\"{marker} {method:<18} {arch:<20} {val_acc:6.1f}%    {test_acc:6.1f}%\")\n",
    "        \n",
    "        # Method summaries\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"METHOD SUMMARIES (Best result per method):\")\n",
    "        print('='*60)\n",
    "        \n",
    "        for method in METHODS_TO_TEST:\n",
    "            if method in results and results[method]:\n",
    "                best_result = max(results[method].items(), key=lambda x: x[1]['test_acc'])\n",
    "                arch, metrics = best_result\n",
    "                print(f\"{method:<20}: {metrics['test_acc']:5.1f}% (using {arch})\")\n",
    "\n",
    "def main():\n",
    "    print(\"Multi-Method Deer Age Prediction Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    images, ages, sources = load_multi_source_data()\n",
    "    \n",
    "    # Run comparison\n",
    "    comparator = MultiMethodComparison()\n",
    "    results = comparator.run_comparison(images, ages, sources)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nTotal comparison time: {elapsed:.1f} minutes\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867854b-f9b3-49e3-bb1e-715d2f33ca4f",
   "metadata": {},
   "source": [
    "### Fine-tune the robust model\n",
    "\n",
    "```\n",
    "================================================================================\n",
    "FINAL METHOD COMPARISON RESULTS\n",
    "================================================================================\n",
    "Method               Architecture         Val Acc    Test Acc  \n",
    "--------------------------------------------------------------------------------\n",
    "üèÜ baseline           resnet34               71.1%      64.2%\n",
    "   robust_training    mobilenetv3_large_100   78.9%      64.2%\n",
    "   uncertainty_aware  efficientnet_b0        75.0%      63.2%\n",
    "```\n",
    "The output from the previou code suggests `robust_training` is learning more generalizable traits than the `baseline` model. Let's try to fine-tune its hyperparameters.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257180d5-1096-4764-8535-4b787e4678eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust Training Optimization for Deer Age Prediction\n",
      "============================================================\n",
      "Loading NDA_color images...\n",
      "  Loaded 363 images from NDA_color\n",
      "Loading NDA_grayscale images...\n",
      "  Loaded 108 images from NDA_grayscale\n",
      "Total images: 471\n",
      "Final dataset: 471 images\n",
      "Age distribution: {5.5: 121, 4.5: 89, 2.5: 83, 3.5: 111, 1.5: 67}\n",
      "Source distribution: {'NDA_color': 363, 'NDA_grayscale': 108}\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Classes: 5\n",
      "Label mapping: {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
      "Data split - Train: 300, Val: 76, Test: 95\n",
      "\n",
      "======================================================================\n",
      "OPTIMIZING ROBUST TRAINING WITH RESNET34\n",
      "======================================================================\n",
      "\n",
      "[resnet34] Testing config: original\n",
      "Training with config: original\n",
      "  Label smoothing: 0.25, Dropout: 0.5\n",
      "  LR: 0.0003, Weight decay: 0.02, Max epochs: 60\n",
      "    Epoch 0: Train 44.0%, Val 55.3%\n",
      "    Epoch 20: Train 99.9%, Val 67.1%\n",
      "    Early stopping at epoch 36\n",
      "  Result: Val 73.7%, Test 58.9%\n",
      "\n",
      "[resnet34] Testing config: less_smoothing\n",
      "Training with config: less_smoothing\n",
      "  Label smoothing: 0.15, Dropout: 0.5\n",
      "  LR: 0.0003, Weight decay: 0.02, Max epochs: 80\n",
      "    Epoch 0: Train 43.0%, Val 47.4%\n",
      "    Epoch 20: Train 100.0%, Val 65.8%\n",
      "    Epoch 40: Train 100.0%, Val 67.1%\n",
      "    Early stopping at epoch 44\n",
      "  Result: Val 72.4%, Test 56.8%\n",
      "\n",
      "[resnet34] Testing config: moderate_reg\n",
      "Training with config: moderate_reg\n",
      "  Label smoothing: 0.2, Dropout: 0.4\n",
      "  LR: 0.0003, Weight decay: 0.02, Max epochs: 80\n",
      "    Epoch 0: Train 47.5%, Val 50.0%\n",
      "    Epoch 20: Train 99.8%, Val 67.1%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Epoch 60: Train 100.0%, Val 72.4%\n",
      "    Early stopping at epoch 67\n",
      "  Result: Val 72.4%, Test 57.9%\n",
      "\n",
      "[resnet34] Testing config: lower_lr\n",
      "Training with config: lower_lr\n",
      "  Label smoothing: 0.25, Dropout: 0.5\n",
      "  LR: 0.0001, Weight decay: 0.02, Max epochs: 100\n",
      "    Epoch 0: Train 28.7%, Val 32.9%\n",
      "    Epoch 20: Train 100.0%, Val 64.5%\n",
      "    Early stopping at epoch 30\n",
      "  Result: Val 71.1%, Test 58.9%\n",
      "\n",
      "[resnet34] Testing config: higher_lr\n",
      "Training with config: higher_lr\n",
      "  Label smoothing: 0.25, Dropout: 0.5\n",
      "  LR: 0.0005, Weight decay: 0.01, Max epochs: 60\n",
      "    Epoch 0: Train 54.0%, Val 55.3%\n",
      "    Epoch 20: Train 99.8%, Val 61.8%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 46\n",
      "  Result: Val 68.4%, Test 56.8%\n",
      "\n",
      "[resnet34] Testing config: extended_training\n",
      "Training with config: extended_training\n",
      "  Label smoothing: 0.2, Dropout: 0.4\n",
      "  LR: 0.0002, Weight decay: 0.015, Max epochs: 120\n",
      "    Epoch 0: Train 38.9%, Val 44.7%\n",
      "    Epoch 20: Train 100.0%, Val 69.7%\n",
      "    Epoch 40: Train 100.0%, Val 65.8%\n",
      "    Early stopping at epoch 46\n",
      "  Result: Val 75.0%, Test 63.2%\n",
      "\n",
      "[resnet34] Testing config: less_decay\n",
      "Training with config: less_decay\n",
      "  Label smoothing: 0.25, Dropout: 0.5\n",
      "  LR: 0.0003, Weight decay: 0.01, Max epochs: 80\n",
      "    Epoch 0: Train 42.3%, Val 51.3%\n",
      "    Epoch 20: Train 99.9%, Val 75.0%\n",
      "    Early stopping at epoch 28\n",
      "  Result: Val 75.0%, Test 54.7%\n",
      "\n",
      "[resnet34] Testing config: more_decay\n",
      "Training with config: more_decay\n",
      "  Label smoothing: 0.25, Dropout: 0.5\n",
      "  LR: 0.0003, Weight decay: 0.03, Max epochs: 80\n",
      "    Epoch 0: Train 42.6%, Val 59.2%\n",
      "    Epoch 20: Train 99.8%, Val 69.7%\n",
      "    Early stopping at epoch 35\n",
      "  Result: Val 73.7%, Test 64.2%\n",
      "\n",
      "[resnet34] Testing config: conservative\n",
      "Training with config: conservative\n",
      "  Label smoothing: 0.1, Dropout: 0.3\n",
      "  LR: 0.0003, Weight decay: 0.02, Max epochs: 80\n",
      "    Epoch 0: Train 49.2%, Val 55.3%\n",
      "    Epoch 20: Train 99.9%, Val 65.8%\n",
      "    Early stopping at epoch 32\n",
      "  Result: Val 72.4%, Test 57.9%\n",
      "\n",
      "[resnet34] Testing config: aggressive\n",
      "Training with config: aggressive\n",
      "  Label smoothing: 0.35, Dropout: 0.6\n",
      "  LR: 0.0002, Weight decay: 0.025, Max epochs: 100\n",
      "    Epoch 0: Train 28.9%, Val 31.6%\n",
      "    Epoch 20: Train 100.0%, Val 56.6%\n",
      "    Early stopping at epoch 36\n",
      "  Result: Val 68.4%, Test 54.7%\n",
      "\n",
      "======================================================================\n",
      "OPTIMIZING ROBUST TRAINING WITH MOBILENETV3_LARGE_100\n",
      "======================================================================\n",
      "\n",
      "[mobilenetv3_large_100] Testing config: original\n",
      "Training with config: original\n",
      "  Label smoothing: 0.25, Dropout: 0.5\n",
      "  LR: 0.0003, Weight decay: 0.02, Max epochs: 60\n",
      "    Epoch 0: Train 74.5%, Val 64.5%\n",
      "    Epoch 20: Train 99.9%, Val 68.4%\n",
      "    Early stopping at epoch 39\n",
      "  Result: Val 72.4%, Test 55.8%\n",
      "\n",
      "[mobilenetv3_large_100] Testing config: less_smoothing\n",
      "Training with config: less_smoothing\n",
      "  Label smoothing: 0.15, Dropout: 0.5\n",
      "  LR: 0.0003, Weight decay: 0.02, Max epochs: 80\n",
      "    Epoch 0: Train 76.0%, Val 63.2%\n",
      "    Epoch 20: Train 99.8%, Val 59.2%\n",
      "    Epoch 40: Train 100.0%, Val 67.1%\n",
      "    Early stopping at epoch 50\n",
      "  Result: Val 75.0%, Test 55.8%\n",
      "\n",
      "[mobilenetv3_large_100] Testing config: moderate_reg\n",
      "Training with config: moderate_reg\n",
      "  Label smoothing: 0.2, Dropout: 0.4\n",
      "  LR: 0.0003, Weight decay: 0.02, Max epochs: 80\n",
      "    Epoch 0: Train 76.5%, Val 67.1%\n",
      "    Epoch 20: Train 99.9%, Val 64.5%\n",
      "    Early stopping at epoch 21\n",
      "  Result: Val 72.4%, Test 52.6%\n",
      "\n",
      "[mobilenetv3_large_100] Testing config: lower_lr\n",
      "Training with config: lower_lr\n",
      "  Label smoothing: 0.25, Dropout: 0.5\n",
      "  LR: 0.0001, Weight decay: 0.02, Max epochs: 100\n",
      "    Epoch 0: Train 57.0%, Val 56.6%\n",
      "    Epoch 20: Train 100.0%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 57\n",
      "  Result: Val 71.1%, Test 51.6%\n",
      "\n",
      "[mobilenetv3_large_100] Testing config: higher_lr\n",
      "Training with config: higher_lr\n",
      "  Label smoothing: 0.25, Dropout: 0.5\n",
      "  LR: 0.0005, Weight decay: 0.01, Max epochs: 60\n",
      "    Epoch 0: Train 79.8%, Val 71.1%\n",
      "    Epoch 20: Train 99.6%, Val 64.5%\n",
      "    Early stopping at epoch 24\n",
      "  Result: Val 73.7%, Test 53.7%\n",
      "\n",
      "[mobilenetv3_large_100] Testing config: extended_training\n",
      "Training with config: extended_training\n",
      "  Label smoothing: 0.2, Dropout: 0.4\n",
      "  LR: 0.0002, Weight decay: 0.015, Max epochs: 120\n",
      "    Epoch 0: Train 73.9%, Val 65.8%\n",
      "    Epoch 20: Train 99.8%, Val 71.1%\n",
      "    Early stopping at epoch 36\n",
      "  Result: Val 75.0%, Test 56.8%\n",
      "\n",
      "[mobilenetv3_large_100] Testing config: less_decay\n",
      "Training with config: less_decay\n",
      "  Label smoothing: 0.25, Dropout: 0.5\n",
      "  LR: 0.0003, Weight decay: 0.01, Max epochs: 80\n",
      "    Epoch 0: Train 74.7%, Val 59.2%\n",
      "    Epoch 20: Train 99.9%, Val 69.7%\n",
      "    Early stopping at epoch 27\n",
      "  Result: Val 76.3%, Test 52.6%\n",
      "\n",
      "[mobilenetv3_large_100] Testing config: more_decay\n",
      "Training with config: more_decay\n",
      "  Label smoothing: 0.25, Dropout: 0.5\n",
      "  LR: 0.0003, Weight decay: 0.03, Max epochs: 80\n",
      "    Epoch 0: Train 76.2%, Val 65.8%\n",
      "    Epoch 20: Train 100.0%, Val 72.4%\n",
      "    Epoch 40: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 42\n",
      "  Result: Val 77.6%, Test 60.0%\n",
      "\n",
      "[mobilenetv3_large_100] Testing config: conservative\n",
      "Training with config: conservative\n",
      "  Label smoothing: 0.1, Dropout: 0.3\n",
      "  LR: 0.0003, Weight decay: 0.02, Max epochs: 80\n",
      "    Epoch 0: Train 77.8%, Val 69.7%\n",
      "    Epoch 20: Train 99.5%, Val 68.4%\n",
      "    Epoch 40: Train 100.0%, Val 69.7%\n",
      "    Early stopping at epoch 42\n",
      "  Result: Val 75.0%, Test 50.5%\n",
      "\n",
      "[mobilenetv3_large_100] Testing config: aggressive\n",
      "Training with config: aggressive\n",
      "  Label smoothing: 0.35, Dropout: 0.6\n",
      "  LR: 0.0002, Weight decay: 0.025, Max epochs: 100\n",
      "    Epoch 0: Train 65.6%, Val 59.2%\n",
      "    Epoch 20: Train 99.8%, Val 67.1%\n",
      "    Epoch 40: Train 100.0%, Val 69.7%\n",
      "    Early stopping at epoch 45\n",
      "  Result: Val 75.0%, Test 54.7%\n",
      "\n",
      "================================================================================\n",
      "ROBUST TRAINING OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "Architecture         Config             Val Acc    Test Acc  \n",
      "--------------------------------------------------------------------------------\n",
      "üèÜ resnet34           more_decay           73.7%      64.2%\n",
      "   resnet34           extended_training    75.0%      63.2%\n",
      "   mobilenetv3_large_100 more_decay           77.6%      60.0%\n",
      "   resnet34           original             73.7%      58.9%\n",
      "   resnet34           lower_lr             71.1%      58.9%\n",
      "   resnet34           moderate_reg         72.4%      57.9%\n",
      "   resnet34           conservative         72.4%      57.9%\n",
      "   resnet34           less_smoothing       72.4%      56.8%\n",
      "   resnet34           higher_lr            68.4%      56.8%\n",
      "   mobilenetv3_large_100 extended_training    75.0%      56.8%\n",
      "   mobilenetv3_large_100 original             72.4%      55.8%\n",
      "   mobilenetv3_large_100 less_smoothing       75.0%      55.8%\n",
      "   resnet34           less_decay           75.0%      54.7%\n",
      "   resnet34           aggressive           68.4%      54.7%\n",
      "   mobilenetv3_large_100 aggressive           75.0%      54.7%\n",
      "   mobilenetv3_large_100 higher_lr            73.7%      53.7%\n",
      "   mobilenetv3_large_100 moderate_reg         72.4%      52.6%\n",
      "   mobilenetv3_large_100 less_decay           76.3%      52.6%\n",
      "   mobilenetv3_large_100 lower_lr             71.1%      51.6%\n",
      "   mobilenetv3_large_100 conservative         75.0%      50.5%\n",
      "\n",
      "============================================================\n",
      "BEST CONFIGURATION FOUND:\n",
      "Architecture: resnet34\n",
      "Config: more_decay\n",
      "Validation Accuracy: 73.7%\n",
      "Test Accuracy: 64.2%\n",
      "\n",
      "Best model saved: robust_training_optimization_20250902_155700\\BEST_robust_resnet34_more_decay_64.2pct.pth\n",
      "\n",
      "Total optimization time: 249.4 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hyperparameter combinations to test for robust_training optimization\n",
    "ROBUST_TRAINING_CONFIGS = [\n",
    "    # Original robust_training\n",
    "    {'label_smoothing': 0.25, 'dropout': 0.5, 'lr': 0.0003, 'weight_decay': 0.02, 'max_epochs': 60, 'name': 'original'},\n",
    "    \n",
    "    # Lower label smoothing variations\n",
    "    {'label_smoothing': 0.15, 'dropout': 0.5, 'lr': 0.0003, 'weight_decay': 0.02, 'max_epochs': 80, 'name': 'less_smoothing'},\n",
    "    {'label_smoothing': 0.20, 'dropout': 0.4, 'lr': 0.0003, 'weight_decay': 0.02, 'max_epochs': 80, 'name': 'moderate_reg'},\n",
    "    \n",
    "    # Learning rate variations\n",
    "    {'label_smoothing': 0.25, 'dropout': 0.5, 'lr': 0.0001, 'weight_decay': 0.02, 'max_epochs': 100, 'name': 'lower_lr'},\n",
    "    {'label_smoothing': 0.25, 'dropout': 0.5, 'lr': 0.0005, 'weight_decay': 0.01, 'max_epochs': 60, 'name': 'higher_lr'},\n",
    "    \n",
    "    # Training length variations\n",
    "    {'label_smoothing': 0.20, 'dropout': 0.4, 'lr': 0.0002, 'weight_decay': 0.015, 'max_epochs': 120, 'name': 'extended_training'},\n",
    "    \n",
    "    # Weight decay variations\n",
    "    {'label_smoothing': 0.25, 'dropout': 0.5, 'lr': 0.0003, 'weight_decay': 0.01, 'max_epochs': 80, 'name': 'less_decay'},\n",
    "    {'label_smoothing': 0.25, 'dropout': 0.5, 'lr': 0.0003, 'weight_decay': 0.03, 'max_epochs': 80, 'name': 'more_decay'},\n",
    "    \n",
    "    # Conservative approach (less aggressive)\n",
    "    {'label_smoothing': 0.10, 'dropout': 0.3, 'lr': 0.0003, 'weight_decay': 0.02, 'max_epochs': 80, 'name': 'conservative'},\n",
    "    \n",
    "    # Aggressive approach (more regularization)\n",
    "    {'label_smoothing': 0.35, 'dropout': 0.6, 'lr': 0.0002, 'weight_decay': 0.025, 'max_epochs': 100, 'name': 'aggressive'},\n",
    "]\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUGMENTATION_TARGET = 800\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    \"\"\"Detect if image is grayscale and convert to 3-channel RGB\"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_multi_source_data():\n",
    "    \"\"\"Load data from multiple sources\"\"\"\n",
    "    data_sources = [\n",
    "        (\"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*.png\", \"NDA_color\"),\n",
    "        (\"G:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*.png\", \"NDA_grayscale\"),\n",
    "    ]\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    for path_pattern, source_name in data_sources:\n",
    "        print(f\"Loading {source_name} images...\")\n",
    "        files = glob.glob(path_pattern)\n",
    "        source_count = 0\n",
    "        \n",
    "        for img_path in files:\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                \n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = detect_and_convert_image(img)\n",
    "                img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "                \n",
    "                filename = os.path.basename(img_path)\n",
    "                filename_no_ext = os.path.splitext(filename)[0]\n",
    "                parts = filename_no_ext.split('_')\n",
    "                \n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                \n",
    "                age_part = parts[3]\n",
    "                if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    age_value = float(age_part.replace('p', '.'))\n",
    "                    images.append(img_resized)\n",
    "                    ages.append(age_value)\n",
    "                    sources.append(source_name)\n",
    "                    source_count += 1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"  Loaded {source_count} images from {source_name}\")\n",
    "    \n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    # Group ages and filter\n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    print(f\"Source distribution: {dict(Counter(filtered_sources))}\")\n",
    "    \n",
    "    return np.array(filtered_images), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image_robust(image, strength_factor=1.0):\n",
    "    \"\"\"Enhanced augmentation for robust training with variable strength\"\"\"\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    # Base augmentations\n",
    "    if random.random() < 0.7:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Color/contrast augmentation\n",
    "    if random.random() < 0.8:\n",
    "        alpha = random.uniform(0.7, 1.3)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Strategic grayscale conversion\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.4:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Variable strength additional augmentations\n",
    "    gamma_prob = min(0.6, 0.3 * strength_factor)\n",
    "    noise_prob = min(0.5, 0.2 * strength_factor)\n",
    "    \n",
    "    if random.random() < gamma_prob:\n",
    "        gamma_range = (0.7, 1.4) if strength_factor > 1.0 else (0.8, 1.3)\n",
    "        gamma = random.uniform(*gamma_range)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    if random.random() < noise_prob:\n",
    "        noise_std = 12 if strength_factor > 1.0 else 8\n",
    "        noise = np.random.normal(0, noise_std, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_balanced_dataset_robust(X, y, sources, strength_factor=1.0):\n",
    "    \"\"\"Create balanced dataset with variable augmentation strength\"\"\"\n",
    "    class_counts = Counter(y)\n",
    "    max_count = max(class_counts.values())\n",
    "    target_count = max(AUGMENTATION_TARGET, max_count)\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    sources_balanced = []\n",
    "    \n",
    "    for class_idx in range(len(set(y))):\n",
    "        class_mask = np.array(y) == class_idx\n",
    "        class_images = X[class_mask]\n",
    "        class_sources = np.array(sources)[class_mask]\n",
    "        current_count = len(class_images)\n",
    "        \n",
    "        if current_count == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add originals\n",
    "        X_balanced.extend(class_images)\n",
    "        y_balanced.extend([class_idx] * current_count)\n",
    "        sources_balanced.extend(class_sources)\n",
    "        \n",
    "        # Add augmented\n",
    "        needed = target_count - current_count\n",
    "        for i in range(needed):\n",
    "            orig_idx = random.randint(0, current_count - 1)\n",
    "            aug_img = enhanced_augment_image_robust(class_images[orig_idx].copy(), strength_factor)\n",
    "            X_balanced.append(aug_img)\n",
    "            y_balanced.append(class_idx)\n",
    "            sources_balanced.append(class_sources[orig_idx])\n",
    "    \n",
    "    return np.array(X_balanced), np.array(y_balanced), sources_balanced\n",
    "\n",
    "class RobustDeerDataset(Dataset):\n",
    "    def __init__(self, X, y, sources=None, training=True):\n",
    "        self.X = torch.FloatTensor(X if isinstance(X, np.ndarray) else np.array(X))\n",
    "        self.y = torch.LongTensor(y if isinstance(y, np.ndarray) else np.array(y))\n",
    "        self.sources = sources\n",
    "        self.training = training\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Test time augmentation for validation/test\n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = torch.flip(image, [2])\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, label\n",
    "\n",
    "class RobustTrainer:\n",
    "    def __init__(self, architecture, num_classes, config, device):\n",
    "        self.architecture = architecture\n",
    "        self.num_classes = num_classes\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.config_name = config['name']\n",
    "        \n",
    "        # Create model with config-specific dropout\n",
    "        self.model = timm.create_model(architecture, pretrained=True, num_classes=num_classes)\n",
    "        \n",
    "        # Replace classifier with config-specific dropout\n",
    "        if hasattr(self.model, 'fc'):\n",
    "            in_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Sequential(\n",
    "                nn.Dropout(config['dropout']),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )\n",
    "        elif hasattr(self.model, 'classifier'):\n",
    "            if hasattr(self.model.classifier, 'in_features'):\n",
    "                in_features = self.model.classifier.in_features\n",
    "            else:\n",
    "                in_features = self.model.classifier[-1].in_features\n",
    "            self.model.classifier = nn.Sequential(\n",
    "                nn.Dropout(config['dropout']),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )\n",
    "        elif hasattr(self.model, 'head'):\n",
    "            if hasattr(self.model.head, 'in_features'):\n",
    "                in_features = self.model.head.in_features\n",
    "            else:\n",
    "                in_features = self.model.head[-1].in_features if hasattr(self.model.head, '__getitem__') else 512\n",
    "            self.model.head = nn.Sequential(\n",
    "                nn.Dropout(config['dropout']),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )\n",
    "        \n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        # Config-specific training components\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=config['max_epochs'], eta_min=1e-6)\n",
    "    \n",
    "    def train_model(self, train_loader, val_loader):\n",
    "        print(f\"Training with config: {self.config_name}\")\n",
    "        print(f\"  Label smoothing: {self.config['label_smoothing']}, Dropout: {self.config['dropout']}\")\n",
    "        print(f\"  LR: {self.config['lr']}, Weight decay: {self.config['weight_decay']}, Max epochs: {self.config['max_epochs']}\")\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        best_test_acc = 0.0  # Track test acc at best val\n",
    "        patience = 20\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(self.config['max_epochs']):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            val_acc = self.evaluate(val_loader)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = self.model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                train_acc = 100 * train_correct / train_total\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Load best weights\n",
    "        if best_state is not None:\n",
    "            self.model.load_state_dict(best_state)\n",
    "        \n",
    "        return best_val_acc\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Test time augmentation\n",
    "                outputs1 = self.model(images)\n",
    "                flipped = torch.flip(images, [3])\n",
    "                outputs2 = self.model(flipped)\n",
    "                outputs = (outputs1 + outputs2) / 2\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return 100 * correct / total\n",
    "\n",
    "class RobustTrainingOptimizer:\n",
    "    def __init__(self, save_dir=None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"robust_training_optimization_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    \n",
    "    def run_optimization(self, images, ages, sources):\n",
    "        unique_ages = sorted(list(set(ages)))\n",
    "        label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "        y_indices = np.array([label_mapping[age] for age in ages])\n",
    "        \n",
    "        print(f\"Classes: {len(unique_ages)}\")\n",
    "        print(f\"Label mapping: {label_mapping}\")\n",
    "        \n",
    "        # Use same train/test split for fair comparison\n",
    "        X_train, X_test, y_train, y_test, sources_train, sources_test = train_test_split(\n",
    "            images, y_indices, sources, test_size=0.2, random_state=42, stratify=y_indices\n",
    "        )\n",
    "        \n",
    "        X_train_final, X_val, y_train_final, y_val, sources_train_final, sources_val = train_test_split(\n",
    "            X_train, y_train, sources_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(f\"Data split - Train: {len(X_train_final)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "        \n",
    "        # Test best architectures from previous results: resnet34 and mobilenetv3_large_100\n",
    "        best_archs = ['resnet34', 'mobilenetv3_large_100']\n",
    "        \n",
    "        results = {}\n",
    "        best_overall = {'config': None, 'arch': None, 'test_acc': 0.0, 'val_acc': 0.0}\n",
    "        \n",
    "        for arch in best_archs:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"OPTIMIZING ROBUST TRAINING WITH {arch.upper()}\")\n",
    "            print('='*70)\n",
    "            \n",
    "            arch_results = {}\n",
    "            \n",
    "            for config in ROBUST_TRAINING_CONFIGS:\n",
    "                print(f\"\\n[{arch}] Testing config: {config['name']}\")\n",
    "                \n",
    "                try:\n",
    "                    # Create config-specific balanced dataset\n",
    "                    # Use different strength factors for different configs\n",
    "                    strength_factor = 1.2 if 'aggressive' in config['name'] else 1.0\n",
    "                    if 'conservative' in config['name']:\n",
    "                        strength_factor = 0.8\n",
    "                    \n",
    "                    X_balanced, y_balanced, sources_balanced = create_balanced_dataset_robust(\n",
    "                        X_train_final, y_train_final, sources_train_final, strength_factor\n",
    "                    )\n",
    "                    \n",
    "                    # Create datasets\n",
    "                    train_dataset = RobustDeerDataset(X_balanced, y_balanced, sources_balanced, training=True)\n",
    "                    val_dataset = RobustDeerDataset(X_val, y_val, sources_val, training=False)\n",
    "                    test_dataset = RobustDeerDataset(X_test, y_test, sources_test, training=False)\n",
    "                    \n",
    "                    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "                    \n",
    "                    # Train model\n",
    "                    trainer = RobustTrainer(arch, len(unique_ages), config, self.device)\n",
    "                    val_acc = trainer.train_model(train_loader, val_loader)\n",
    "                    test_acc = trainer.evaluate(test_loader)\n",
    "                    \n",
    "                    arch_results[config['name']] = {\n",
    "                        'val_acc': val_acc,\n",
    "                        'test_acc': test_acc,\n",
    "                        'config': config,\n",
    "                        'model_state': trainer.model.state_dict().copy()\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  Result: Val {val_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "                    \n",
    "                    # Track best overall\n",
    "                    if test_acc > best_overall['test_acc']:\n",
    "                        best_overall = {\n",
    "                            'config': config['name'],\n",
    "                            'arch': arch,\n",
    "                            'test_acc': test_acc,\n",
    "                            'val_acc': val_acc,\n",
    "                            'model_state': trainer.model.state_dict().copy()\n",
    "                        }\n",
    "                    \n",
    "                    # Save model\n",
    "                    save_path = os.path.join(self.save_dir, f\"robust_{arch}_{config['name']}_{test_acc:.1f}pct.pth\")\n",
    "                    torch.save({\n",
    "                        'model_state_dict': trainer.model.state_dict(),\n",
    "                        'architecture': arch,\n",
    "                        'config': config,\n",
    "                        'num_classes': len(unique_ages),\n",
    "                        'label_mapping': label_mapping,\n",
    "                        'test_accuracy': test_acc,\n",
    "                        'val_accuracy': val_acc\n",
    "                    }, save_path)\n",
    "                    \n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  FAILED: {str(e)[:60]}...\")\n",
    "                    continue\n",
    "            \n",
    "            results[arch] = arch_results\n",
    "        \n",
    "        # Print optimization results\n",
    "        self.print_optimization_results(results, best_overall)\n",
    "        \n",
    "        # Save best model separately\n",
    "        if best_overall['model_state'] is not None:\n",
    "            best_save_path = os.path.join(self.save_dir, f\"BEST_robust_{best_overall['arch']}_{best_overall['config']}_{best_overall['test_acc']:.1f}pct.pth\")\n",
    "            torch.save({\n",
    "                'model_state_dict': best_overall['model_state'],\n",
    "                'architecture': best_overall['arch'],\n",
    "                'config_name': best_overall['config'],\n",
    "                'num_classes': len(unique_ages),\n",
    "                'label_mapping': label_mapping,\n",
    "                'test_accuracy': best_overall['test_acc'],\n",
    "                'val_accuracy': best_overall['val_acc']\n",
    "            }, best_save_path)\n",
    "            print(f\"\\nBest model saved: {best_save_path}\")\n",
    "        \n",
    "        return results, best_overall\n",
    "    \n",
    "    def print_optimization_results(self, results, best_overall):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ROBUST TRAINING OPTIMIZATION RESULTS\")\n",
    "        print('='*80)\n",
    "        print(f\"{'Architecture':<20} {'Config':<18} {'Val Acc':<10} {'Test Acc':<10}\")\n",
    "        print('-'*80)\n",
    "        \n",
    "        all_results = []\n",
    "        for arch, configs in results.items():\n",
    "            for config_name, metrics in configs.items():\n",
    "                all_results.append((arch, config_name, metrics['val_acc'], metrics['test_acc']))\n",
    "        \n",
    "        # Sort by test accuracy\n",
    "        all_results.sort(key=lambda x: x[3], reverse=True)\n",
    "        \n",
    "        for i, (arch, config, val_acc, test_acc) in enumerate(all_results):\n",
    "            marker = \"üèÜ\" if i == 0 else \"  \"\n",
    "            print(f\"{marker} {arch:<18} {config:<18} {val_acc:6.1f}%    {test_acc:6.1f}%\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"BEST CONFIGURATION FOUND:\")\n",
    "        print(f\"Architecture: {best_overall['arch']}\")\n",
    "        print(f\"Config: {best_overall['config']}\")\n",
    "        print(f\"Validation Accuracy: {best_overall['val_acc']:.1f}%\")\n",
    "        print(f\"Test Accuracy: {best_overall['test_acc']:.1f}%\")\n",
    "\n",
    "def main():\n",
    "    print(\"Robust Training Optimization for Deer Age Prediction\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    images, ages, sources = load_multi_source_data()\n",
    "    \n",
    "    # Run optimization\n",
    "    optimizer = RobustTrainingOptimizer()\n",
    "    results, best_config = optimizer.run_optimization(images, ages, sources)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nTotal optimization time: {elapsed:.1f} minutes\")\n",
    "    \n",
    "    return results, best_config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, best = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f2728-eb8a-478d-83ee-c2a78c958d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
