{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e5eff5-c905-4da2-b77d-3ed7a7ca0e9d",
   "metadata": {},
   "source": [
    "### Check RTX5090 running CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6ab5f0-9231-4ae0-8fc4-d27217483190",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20250910+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 5090\n",
      "GPU memory: 31.8 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 works!\n",
      "EfficientNet works!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Check if CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not detected by PyTorch\")\n",
    "\n",
    "# Test ResNet50 specifically\n",
    "model = models.resnet50(pretrained=True).cuda()\n",
    "test_batch = torch.randn(2, 3, 224, 224).cuda()\n",
    "try:\n",
    "    output = model(test_batch)\n",
    "    print(\"ResNet50 works!\")\n",
    "except Exception as e:\n",
    "    print(f\"ResNet50 failed: {e}\")\n",
    "\n",
    "# Test EfficientNet\n",
    "try:\n",
    "    model_eff = models.efficientnet_b0(pretrained=True).cuda()\n",
    "    output_eff = model_eff(test_batch)\n",
    "    print(\"EfficientNet works!\")\n",
    "except Exception as e:\n",
    "    print(f\"EfficientNet failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71bc11f-e647-462f-a1f3-466206538d7d",
   "metadata": {},
   "source": [
    "### Process deer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43154fba-18c6-4690-a3cd-6d4e8c863549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTX 5090 Comprehensive Model Evaluation\n",
      "512x512 Images - Multiple Architectures\n",
      "============================================================\n",
      "Loading color images...\n",
      "Loaded 201 color images\n",
      "Loading grayscale images...\n",
      "Loaded 40 grayscale images\n",
      "Total images: 241\n",
      "Final dataset: 241 images\n",
      "Age distribution: {2.5: 41, 3.5: 50, 4.5: 57, 5.5: 60, 1.5: 33}\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n",
      "GPU Memory: 34.2 GB\n",
      "COMPREHENSIVE MODEL EVALUATION - 33 models, 10 folds\n",
      "Image size: 600x600\n",
      "Total experiments: 330\n",
      "Classes: 5\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B0\n",
      "Batch size: 128\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 64.4%, Val 66.4%\n",
      "    Epoch 20: Train 100.0%, Val 74.2%\n",
      "    Early stopping at epoch 35\n",
      "    efficientnet_b0 Fold 1: Train 100.0%, Val 77.2%, Test 61.0%, Mult 4709.2\n",
      "\n",
      "[Fold 2/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 66.2%, Val 44.4%\n",
      "    Epoch 20: Train 100.0%, Val 53.6%\n",
      "    Epoch 40: Train 100.0%, Val 54.3%\n",
      "    Epoch 60: Train 100.0%, Val 59.0%\n",
      "    Early stopping at epoch 67\n",
      "    efficientnet_b0 Fold 2: Train 100.0%, Val 63.5%, Test 59.0%, Mult 3746.5\n",
      "\n",
      "[Fold 3/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 61.7%, Val 56.5%\n",
      "    Epoch 20: Train 100.0%, Val 60.6%\n",
      "    Epoch 40: Train 100.0%, Val 64.1%\n",
      "    Early stopping at epoch 48\n",
      "    efficientnet_b0 Fold 3: Train 100.0%, Val 68.2%, Test 68.9%, Mult 4699.0\n",
      "\n",
      "[Fold 4/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 63.6%, Val 69.7%\n",
      "    Epoch 20: Train 100.0%, Val 70.1%\n",
      "    Early stopping at epoch 35\n",
      "    efficientnet_b0 Fold 4: Train 100.0%, Val 77.6%, Test 64.1%, Mult 4974.2\n",
      "\n",
      "[Fold 5/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 64.3%, Val 74.6%\n",
      "    Epoch 20: Train 100.0%, Val 71.7%\n",
      "    Early stopping at epoch 27\n",
      "    efficientnet_b0 Fold 5: Train 99.7%, Val 77.5%, Test 76.4%, Mult 5921.0\n",
      "\n",
      "[Fold 6/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 62.2%, Val 78.9%\n",
      "    Epoch 20: Train 100.0%, Val 72.9%\n",
      "    Early stopping at epoch 25\n",
      "    efficientnet_b0 Fold 6: Train 62.2%, Val 78.9%, Test 42.5%, Mult 3353.2\n",
      "\n",
      "[Fold 7/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 69.5%, Val 64.0%\n",
      "    Epoch 20: Train 100.0%, Val 64.9%\n",
      "    Epoch 40: Train 100.0%, Val 61.2%\n",
      "    Early stopping at epoch 49\n",
      "    efficientnet_b0 Fold 7: Train 99.9%, Val 68.9%, Test 71.2%, Mult 4905.7\n",
      "\n",
      "[Fold 8/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 70.4%, Val 54.5%\n",
      "    Epoch 20: Train 100.0%, Val 60.9%\n",
      "    Early stopping at epoch 26\n",
      "    efficientnet_b0 Fold 8: Train 98.4%, Val 63.3%, Test 53.5%, Mult 3386.6\n",
      "\n",
      "[Fold 9/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 62.8%, Val 58.1%\n",
      "    Epoch 20: Train 100.0%, Val 63.6%\n",
      "    Early stopping at epoch 38\n",
      "    efficientnet_b0 Fold 9: Train 99.9%, Val 67.5%, Test 70.0%, Mult 4725.0\n",
      "\n",
      "[Fold 10/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 64.1%, Val 73.8%\n",
      "    Epoch 20: Train 100.0%, Val 71.8%\n",
      "    Early stopping at epoch 26\n",
      "    efficientnet_b0 Fold 10: Train 97.8%, Val 81.5%, Test 61.4%, Mult 5004.1\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B1\n",
      "Batch size: 96\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b1...\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\efficientnet_b1-c27df63c.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.1M/30.1M [00:00<00:00, 60.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 59.3%, Val 58.4%\n",
      "    Epoch 20: Train 100.0%, Val 65.1%\n",
      "    Epoch 40: Train 100.0%, Val 68.7%\n",
      "    Epoch 60: Train 100.0%, Val 69.6%\n",
      "    Early stopping at epoch 61\n",
      "    efficientnet_b1 Fold 1: Train 100.0%, Val 72.4%, Test 62.6%, Mult 4532.2\n",
      "\n",
      "[Fold 2/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 63.9%, Val 55.3%\n",
      "    Epoch 20: Train 99.8%, Val 59.8%\n",
      "    Epoch 40: Train 100.0%, Val 59.7%\n",
      "    Early stopping at epoch 47\n",
      "    efficientnet_b1 Fold 2: Train 100.0%, Val 63.1%, Test 58.4%, Mult 3685.0\n",
      "\n",
      "[Fold 3/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 62.4%, Val 58.0%\n",
      "    Epoch 20: Train 100.0%, Val 65.3%\n",
      "    Epoch 40: Train 100.0%, Val 63.9%\n",
      "    Early stopping at epoch 47\n",
      "    efficientnet_b1 Fold 3: Train 100.0%, Val 66.9%, Test 73.6%, Mult 4923.8\n",
      "\n",
      "[Fold 4/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 63.5%, Val 72.5%\n",
      "    Epoch 20: Train 100.0%, Val 75.5%\n",
      "    Early stopping at epoch 35\n",
      "    efficientnet_b1 Fold 4: Train 100.0%, Val 77.6%, Test 63.8%, Mult 4950.9\n",
      "\n",
      "[Fold 5/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 61.4%, Val 69.2%\n",
      "    Epoch 20: Train 99.9%, Val 67.6%\n",
      "    Early stopping at epoch 26\n",
      "    efficientnet_b1 Fold 5: Train 97.7%, Val 74.5%, Test 76.3%, Mult 5684.4\n",
      "\n",
      "[Fold 6/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 62.5%, Val 65.6%\n",
      "    Epoch 20: Train 100.0%, Val 80.3%\n",
      "    Early stopping at epoch 35\n",
      "    efficientnet_b1 Fold 6: Train 99.8%, Val 82.1%, Test 56.6%, Mult 4646.9\n",
      "\n",
      "[Fold 7/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 63.6%, Val 65.0%\n",
      "    Epoch 20: Train 100.0%, Val 59.2%\n",
      "    Early stopping at epoch 26\n",
      "    efficientnet_b1 Fold 7: Train 97.3%, Val 66.9%, Test 60.2%, Mult 4027.4\n",
      "\n",
      "[Fold 8/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 64.1%, Val 48.8%\n",
      "    Epoch 20: Train 100.0%, Val 54.2%\n",
      "    Epoch 40: Train 100.0%, Val 59.7%\n",
      "    Early stopping at epoch 56\n",
      "    efficientnet_b1 Fold 8: Train 100.0%, Val 60.9%, Test 51.8%, Mult 3154.6\n",
      "\n",
      "[Fold 9/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 58.1%, Val 59.7%\n",
      "    Epoch 20: Train 100.0%, Val 73.4%\n",
      "    Epoch 40: Train 100.0%, Val 67.7%\n",
      "    Early stopping at epoch 42\n",
      "    efficientnet_b1 Fold 9: Train 99.9%, Val 77.5%, Test 66.7%, Mult 5169.3\n",
      "\n",
      "[Fold 10/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 63.5%, Val 60.3%\n",
      "    Epoch 20: Train 100.0%, Val 67.9%\n",
      "    Early stopping at epoch 38\n",
      "    efficientnet_b1 Fold 10: Train 99.9%, Val 77.6%, Test 66.4%, Mult 5152.6\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B2\n",
      "Batch size: 80\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 78.5%, Val 72.3%\n",
      "    Epoch 20: Train 100.0%, Val 71.0%\n",
      "    Epoch 40: Train 100.0%, Val 73.9%\n",
      "    Epoch 60: Train 100.0%, Val 77.0%\n",
      "    Early stopping at epoch 69\n",
      "    efficientnet_b2 Fold 1: Train 100.0%, Val 78.3%, Test 56.0%, Mult 4384.8\n",
      "\n",
      "[Fold 2/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 76.2%, Val 49.3%\n",
      "    Epoch 20: Train 100.0%, Val 51.8%\n",
      "    Early stopping at epoch 26\n",
      "    efficientnet_b2 Fold 2: Train 99.8%, Val 54.3%, Test 55.8%, Mult 3029.9\n",
      "\n",
      "[Fold 3/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 76.2%, Val 61.6%\n",
      "    Epoch 20: Train 100.0%, Val 65.2%\n",
      "    Epoch 40: Train 100.0%, Val 64.0%\n",
      "    Early stopping at epoch 54\n",
      "    efficientnet_b2 Fold 3: Train 100.0%, Val 75.7%, Test 71.2%, Mult 5389.8\n",
      "\n",
      "[Fold 4/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 76.5%, Val 73.0%\n",
      "    Epoch 20: Train 100.0%, Val 77.7%\n",
      "    Epoch 40: Train 100.0%, Val 76.8%\n",
      "    Epoch 60: Train 100.0%, Val 76.6%\n",
      "    Early stopping at epoch 64\n",
      "    efficientnet_b2 Fold 4: Train 100.0%, Val 82.5%, Test 70.0%, Mult 5775.0\n",
      "\n",
      "[Fold 5/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 76.0%, Val 72.0%\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Epoch 40: Train 100.0%, Val 68.7%\n",
      "    Epoch 60: Train 100.0%, Val 69.3%\n",
      "    Early stopping at epoch 67\n",
      "    efficientnet_b2 Fold 5: Train 100.0%, Val 76.1%, Test 73.3%, Mult 5578.1\n",
      "\n",
      "[Fold 6/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 72.9%, Val 69.1%\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Epoch 40: Train 100.0%, Val 77.0%\n",
      "    Early stopping at epoch 55\n",
      "    efficientnet_b2 Fold 6: Train 100.0%, Val 78.3%, Test 58.6%, Mult 4588.4\n",
      "\n",
      "[Fold 7/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 79.4%, Val 62.5%\n",
      "    Epoch 20: Train 100.0%, Val 59.1%\n",
      "    Epoch 40: Train 100.0%, Val 63.6%\n",
      "    Early stopping at epoch 43\n",
      "    efficientnet_b2 Fold 7: Train 100.0%, Val 67.1%, Test 73.7%, Mult 4945.3\n",
      "\n",
      "[Fold 8/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 76.5%, Val 56.4%\n",
      "    Epoch 20: Train 100.0%, Val 61.6%\n",
      "    Epoch 40: Train 100.0%, Val 59.2%\n",
      "    Early stopping at epoch 54\n",
      "    efficientnet_b2 Fold 8: Train 100.0%, Val 63.5%, Test 56.2%, Mult 3568.7\n",
      "\n",
      "[Fold 9/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 77.1%, Val 61.1%\n",
      "    Epoch 20: Train 100.0%, Val 66.4%\n",
      "    Epoch 40: Train 100.0%, Val 64.5%\n",
      "    Epoch 60: Train 100.0%, Val 65.2%\n",
      "    Early stopping at epoch 64\n",
      "    efficientnet_b2 Fold 9: Train 100.0%, Val 67.1%, Test 65.8%, Mult 4415.2\n",
      "\n",
      "[Fold 10/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 76.1%, Val 71.4%\n",
      "    Epoch 20: Train 100.0%, Val 77.8%\n",
      "    Early stopping at epoch 36\n",
      "    efficientnet_b2 Fold 10: Train 100.0%, Val 78.7%, Test 69.8%, Mult 5493.3\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B3\n",
      "Batch size: 64\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 78.9%, Val 69.1%\n",
      "    Epoch 20: Train 100.0%, Val 73.9%\n",
      "    Epoch 40: Train 100.0%, Val 66.2%\n",
      "    Early stopping at epoch 48\n",
      "    efficientnet_b3 Fold 1: Train 100.0%, Val 77.9%, Test 63.1%, Mult 4915.5\n",
      "\n",
      "[Fold 2/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 79.6%, Val 47.3%\n",
      "    Epoch 20: Train 100.0%, Val 48.7%\n",
      "    Epoch 40: Train 100.0%, Val 50.2%\n",
      "    Early stopping at epoch 50\n",
      "    efficientnet_b3 Fold 2: Train 100.0%, Val 53.5%, Test 64.9%, Mult 3472.2\n",
      "\n",
      "[Fold 3/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 79.0%, Val 62.9%\n",
      "    Epoch 20: Train 100.0%, Val 58.4%\n",
      "    Epoch 40: Train 100.0%, Val 73.1%\n",
      "    Epoch 60: Train 100.0%, Val 67.9%\n",
      "    Early stopping at epoch 64\n",
      "    efficientnet_b3 Fold 3: Train 100.0%, Val 73.1%, Test 66.5%, Mult 4861.2\n",
      "\n",
      "[Fold 4/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 79.7%, Val 64.5%\n",
      "    Epoch 20: Train 100.0%, Val 70.5%\n",
      "    Early stopping at epoch 34\n",
      "    efficientnet_b3 Fold 4: Train 100.0%, Val 77.5%, Test 67.0%, Mult 5192.5\n",
      "\n",
      "[Fold 5/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 75.2%, Val 67.5%\n",
      "    Epoch 20: Train 100.0%, Val 64.6%\n",
      "    Epoch 40: Train 100.0%, Val 73.4%\n",
      "    Epoch 60: Train 100.0%, Val 74.0%\n",
      "    efficientnet_b3 Fold 5: Train 100.0%, Val 74.0%, Test 74.5%, Mult 5513.0\n",
      "\n",
      "[Fold 6/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 78.2%, Val 71.0%\n",
      "    Epoch 20: Train 100.0%, Val 73.7%\n",
      "    Epoch 40: Train 100.0%, Val 75.0%\n",
      "    Early stopping at epoch 49\n",
      "    efficientnet_b3 Fold 6: Train 100.0%, Val 79.6%, Test 65.3%, Mult 5197.9\n",
      "\n",
      "[Fold 7/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 80.3%, Val 66.8%\n",
      "    Epoch 20: Train 100.0%, Val 67.2%\n",
      "    Epoch 40: Train 100.0%, Val 60.0%\n",
      "    Early stopping at epoch 42\n",
      "    efficientnet_b3 Fold 7: Train 100.0%, Val 73.1%, Test 69.5%, Mult 5080.4\n",
      "\n",
      "[Fold 8/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 80.4%, Val 60.9%\n",
      "    Epoch 20: Train 100.0%, Val 72.1%\n",
      "    Early stopping at epoch 39\n",
      "    efficientnet_b3 Fold 8: Train 99.9%, Val 74.2%, Test 58.3%, Mult 4325.9\n",
      "\n",
      "[Fold 9/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 77.6%, Val 71.2%\n",
      "    Epoch 20: Train 100.0%, Val 77.6%\n",
      "    Epoch 40: Train 100.0%, Val 80.8%\n",
      "    Epoch 60: Train 100.0%, Val 79.6%\n",
      "    Early stopping at epoch 66\n",
      "    efficientnet_b3 Fold 9: Train 100.0%, Val 81.3%, Test 63.8%, Mult 5186.9\n",
      "\n",
      "[Fold 10/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 77.8%, Val 73.7%\n",
      "    Epoch 20: Train 100.0%, Val 74.9%\n",
      "    Early stopping at epoch 32\n",
      "    efficientnet_b3 Fold 10: Train 100.0%, Val 80.4%, Test 60.6%, Mult 4872.2\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B4\n",
      "Batch size: 48\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 68.6%, Val 61.2%\n",
      "    Epoch 20: Train 100.0%, Val 70.2%\n",
      "    Epoch 40: Train 100.0%, Val 69.3%\n",
      "    Early stopping at epoch 55\n",
      "    efficientnet_b4 Fold 1: Train 100.0%, Val 74.0%, Test 57.4%, Mult 4247.6\n",
      "\n",
      "[Fold 2/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 71.1%, Val 52.0%\n",
      "    Epoch 20: Train 100.0%, Val 49.5%\n",
      "    Epoch 40: Train 100.0%, Val 54.3%\n",
      "    Early stopping at epoch 52\n",
      "    efficientnet_b4 Fold 2: Train 100.0%, Val 59.2%, Test 61.2%, Mult 3623.0\n",
      "\n",
      "[Fold 3/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 68.7%, Val 49.3%\n",
      "    Epoch 20: Train 100.0%, Val 54.2%\n",
      "    Epoch 40: Train 100.0%, Val 61.6%\n",
      "    Epoch 60: Train 100.0%, Val 62.9%\n",
      "    Early stopping at epoch 71\n",
      "    efficientnet_b4 Fold 3: Train 100.0%, Val 65.8%, Test 64.5%, Mult 4244.1\n",
      "\n",
      "[Fold 4/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 70.5%, Val 71.7%\n",
      "    Epoch 20: Train 99.9%, Val 73.0%\n",
      "    Early stopping at epoch 29\n",
      "    efficientnet_b4 Fold 4: Train 99.9%, Val 76.8%, Test 61.6%, Mult 4730.9\n",
      "\n",
      "[Fold 5/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 67.5%, Val 62.6%\n",
      "    Epoch 20: Train 100.0%, Val 66.0%\n",
      "    Epoch 40: Train 100.0%, Val 72.6%\n",
      "    Early stopping at epoch 43\n",
      "    efficientnet_b4 Fold 5: Train 99.9%, Val 73.6%, Test 69.4%, Mult 5107.8\n",
      "\n",
      "[Fold 6/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 67.9%, Val 69.4%\n",
      "    Epoch 20: Train 100.0%, Val 73.1%\n",
      "    Epoch 40: Train 100.0%, Val 74.4%\n",
      "    Early stopping at epoch 49\n",
      "    efficientnet_b4 Fold 6: Train 100.0%, Val 76.4%, Test 60.6%, Mult 4629.8\n",
      "\n",
      "[Fold 7/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 70.5%, Val 60.4%\n",
      "    Epoch 20: Train 100.0%, Val 60.8%\n",
      "    Early stopping at epoch 28\n",
      "    efficientnet_b4 Fold 7: Train 100.0%, Val 68.2%, Test 59.4%, Mult 4051.1\n",
      "\n",
      "[Fold 8/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 74.1%, Val 54.9%\n",
      "    Epoch 20: Train 100.0%, Val 59.2%\n",
      "    Epoch 40: Train 100.0%, Val 56.3%\n",
      "    Early stopping at epoch 50\n",
      "    efficientnet_b4 Fold 8: Train 100.0%, Val 60.8%, Test 55.5%, Mult 3374.4\n",
      "\n",
      "[Fold 9/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 67.5%, Val 61.9%\n",
      "    Epoch 20: Train 100.0%, Val 68.9%\n",
      "    Epoch 40: Train 100.0%, Val 68.3%\n",
      "    Epoch 60: Train 100.0%, Val 67.5%\n",
      "    Early stopping at epoch 72\n",
      "    efficientnet_b4 Fold 9: Train 100.0%, Val 77.8%, Test 66.6%, Mult 5181.5\n",
      "\n",
      "[Fold 10/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 65.8%, Val 60.7%\n",
      "    Epoch 20: Train 100.0%, Val 67.2%\n",
      "    Epoch 40: Train 100.0%, Val 70.6%\n",
      "    Epoch 60: Train 100.0%, Val 76.6%\n",
      "    efficientnet_b4 Fold 10: Train 100.0%, Val 79.0%, Test 58.6%, Mult 4629.4\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B5\n",
      "Batch size: 32\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 88.9%, Val 65.0%\n",
      "    Epoch 20: Train 100.0%, Val 78.7%\n",
      "    Epoch 40: Train 100.0%, Val 78.8%\n",
      "    Epoch 60: Train 100.0%, Val 79.1%\n",
      "    Early stopping at epoch 60\n",
      "    efficientnet_b5 Fold 1: Train 100.0%, Val 83.2%, Test 57.7%, Mult 4800.6\n",
      "\n",
      "[Fold 2/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 88.2%, Val 57.9%\n",
      "    Epoch 20: Train 100.0%, Val 56.6%\n",
      "    Early stopping at epoch 39\n",
      "    efficientnet_b5 Fold 2: Train 99.9%, Val 62.9%, Test 56.4%, Mult 3547.6\n",
      "\n",
      "[Fold 3/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 87.3%, Val 58.0%\n",
      "    Epoch 20: Train 100.0%, Val 62.7%\n",
      "    Epoch 40: Train 100.0%, Val 63.7%\n",
      "    Epoch 60: Train 100.0%, Val 65.1%\n",
      "    efficientnet_b5 Fold 3: Train 100.0%, Val 68.8%, Test 66.7%, Mult 4589.0\n",
      "\n",
      "[Fold 4/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 87.1%, Val 72.7%\n",
      "    Epoch 20: Train 100.0%, Val 74.7%\n",
      "    Early stopping at epoch 27\n",
      "    efficientnet_b5 Fold 4: Train 100.0%, Val 79.0%, Test 65.0%, Mult 5135.0\n",
      "\n",
      "[Fold 5/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 82.0%, Val 67.2%\n",
      "    Epoch 20: Train 100.0%, Val 69.4%\n",
      "    Epoch 40: Train 100.0%, Val 72.1%\n",
      "    Early stopping at epoch 59\n",
      "    efficientnet_b5 Fold 5: Train 100.0%, Val 80.4%, Test 78.4%, Mult 6303.4\n",
      "\n",
      "[Fold 6/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 89.6%, Val 68.8%\n",
      "    Epoch 20: Train 100.0%, Val 76.1%\n",
      "    Early stopping at epoch 30\n",
      "    efficientnet_b5 Fold 6: Train 100.0%, Val 78.9%, Test 57.6%, Mult 4544.6\n",
      "\n",
      "[Fold 7/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 87.6%, Val 61.9%\n",
      "    Epoch 20: Train 100.0%, Val 59.8%\n",
      "    Early stopping at epoch 27\n",
      "    efficientnet_b5 Fold 7: Train 100.0%, Val 70.3%, Test 68.6%, Mult 4822.6\n",
      "\n",
      "[Fold 8/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 88.3%, Val 65.4%\n",
      "    Epoch 20: Train 100.0%, Val 69.7%\n",
      "    Early stopping at epoch 38\n",
      "    efficientnet_b5 Fold 8: Train 100.0%, Val 73.6%, Test 56.1%, Mult 4129.0\n",
      "\n",
      "[Fold 9/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 88.6%, Val 72.1%\n",
      "    Epoch 20: Train 100.0%, Val 70.0%\n",
      "    Epoch 40: Train 100.0%, Val 75.8%\n",
      "    Epoch 60: Train 100.0%, Val 71.9%\n",
      "    Early stopping at epoch 79\n",
      "    efficientnet_b5 Fold 9: Train 100.0%, Val 78.1%, Test 69.5%, Mult 5427.9\n",
      "\n",
      "[Fold 10/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 86.7%, Val 69.5%\n",
      "    Epoch 20: Train 99.9%, Val 72.6%\n",
      "    Epoch 40: Train 100.0%, Val 81.7%\n",
      "    Epoch 60: Train 100.0%, Val 81.0%\n",
      "    Early stopping at epoch 76\n",
      "    efficientnet_b5 Fold 10: Train 100.0%, Val 84.3%, Test 67.6%, Mult 5698.7\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B6\n",
      "Batch size: 24\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b6...\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b6_lukemelas-24a108a5.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\efficientnet_b6_lukemelas-24a108a5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 165M/165M [00:02<00:00, 75.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 89.4%, Val 71.7%\n",
      "    Epoch 20: Train 100.0%, Val 69.4%\n",
      "    Early stopping at epoch 28\n",
      "    efficientnet_b6 Fold 1: Train 100.0%, Val 79.8%, Test 56.6%, Mult 4516.7\n",
      "\n",
      "[Fold 2/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 91.1%, Val 51.7%\n",
      "    Epoch 20: Train 100.0%, Val 55.6%\n",
      "    Early stopping at epoch 35\n",
      "    efficientnet_b6 Fold 2: Train 99.2%, Val 60.7%, Test 60.6%, Mult 3678.4\n",
      "\n",
      "[Fold 3/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 89.5%, Val 62.1%\n",
      "    Epoch 20: Train 100.0%, Val 68.8%\n",
      "    Early stopping at epoch 37\n",
      "    efficientnet_b6 Fold 3: Train 100.0%, Val 75.3%, Test 70.3%, Mult 5293.6\n",
      "\n",
      "[Fold 4/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 89.0%, Val 76.3%\n",
      "    Epoch 20: Train 99.9%, Val 72.3%\n",
      "    Early stopping at epoch 31\n",
      "    efficientnet_b6 Fold 4: Train 100.0%, Val 80.4%, Test 70.4%, Mult 5660.2\n",
      "\n",
      "[Fold 5/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 89.7%, Val 71.1%\n",
      "    Epoch 20: Train 100.0%, Val 60.5%\n",
      "    Early stopping at epoch 35\n",
      "    efficientnet_b6 Fold 5: Train 100.0%, Val 71.7%, Test 78.7%, Mult 5642.8\n",
      "\n",
      "[Fold 6/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 90.0%, Val 74.6%\n",
      "    Epoch 20: Train 100.0%, Val 68.6%\n",
      "    Early stopping at epoch 28\n",
      "    efficientnet_b6 Fold 6: Train 100.0%, Val 78.9%, Test 55.5%, Mult 4379.0\n",
      "\n",
      "[Fold 7/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 89.5%, Val 61.8%\n",
      "    Epoch 20: Train 100.0%, Val 65.7%\n",
      "    Early stopping at epoch 29\n",
      "    efficientnet_b6 Fold 7: Train 100.0%, Val 68.6%, Test 70.7%, Mult 4850.0\n",
      "\n",
      "[Fold 8/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 88.2%, Val 60.8%\n",
      "    Epoch 20: Train 100.0%, Val 69.7%\n",
      "    Early stopping at epoch 34\n",
      "    efficientnet_b6 Fold 8: Train 100.0%, Val 72.1%, Test 55.5%, Mult 4001.6\n",
      "\n",
      "[Fold 9/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 88.6%, Val 67.7%\n",
      "    Epoch 20: Train 100.0%, Val 75.5%\n",
      "    Epoch 40: Train 100.0%, Val 66.7%\n",
      "    Early stopping at epoch 41\n",
      "    efficientnet_b6 Fold 9: Train 99.9%, Val 77.3%, Test 68.4%, Mult 5287.3\n",
      "\n",
      "[Fold 10/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 90.1%, Val 72.0%\n",
      "    Epoch 20: Train 100.0%, Val 78.0%\n",
      "    Epoch 40: Train 100.0%, Val 73.1%\n",
      "    Epoch 60: Train 100.0%, Val 75.2%\n",
      "    Early stopping at epoch 62\n",
      "    efficientnet_b6 Fold 10: Train 100.0%, Val 83.0%, Test 66.3%, Mult 5502.9\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B7\n",
      "Batch size: 16\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b7...\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\efficientnet_b7_lukemelas-c5b4e57e.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 255M/255M [00:03<00:00, 69.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 90.1%, Val 64.6%\n",
      "    Epoch 20: Train 100.0%, Val 69.8%\n",
      "    Early stopping at epoch 28\n",
      "    efficientnet_b7 Fold 1: Train 99.9%, Val 75.3%, Test 54.4%, Mult 4096.3\n",
      "\n",
      "[Fold 2/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 91.5%, Val 56.7%\n",
      "    Epoch 20: Train 99.9%, Val 55.3%\n",
      "    Early stopping at epoch 29\n",
      "    efficientnet_b7 Fold 2: Train 100.0%, Val 58.1%, Test 69.9%, Mult 4061.2\n",
      "\n",
      "[Fold 3/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 92.0%, Val 55.8%\n",
      "    Epoch 20: Train 99.9%, Val 67.4%\n",
      "    Epoch 40: Train 100.0%, Val 63.1%\n",
      "    Early stopping at epoch 55\n",
      "    efficientnet_b7 Fold 3: Train 100.0%, Val 72.6%, Test 57.5%, Mult 4174.5\n",
      "\n",
      "[Fold 4/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 90.2%, Val 71.0%\n",
      "    Epoch 20: Train 100.0%, Val 76.0%\n",
      "    Early stopping at epoch 35\n",
      "    efficientnet_b7 Fold 4: Train 100.0%, Val 82.5%, Test 72.8%, Mult 6006.0\n",
      "\n",
      "[Fold 5/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 91.0%, Val 66.2%\n",
      "    Epoch 20: Train 98.8%, Val 52.8%\n",
      "    Epoch 40: Train 100.0%, Val 65.2%\n",
      "    Epoch 60: Train 100.0%, Val 68.5%\n",
      "    Early stopping at epoch 67\n",
      "    efficientnet_b7 Fold 5: Train 100.0%, Val 76.0%, Test 81.9%, Mult 6224.4\n",
      "\n",
      "[Fold 6/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 90.8%, Val 68.1%\n",
      "    Epoch 20: Train 100.0%, Val 69.1%\n",
      "    Early stopping at epoch 35\n",
      "    efficientnet_b7 Fold 6: Train 99.9%, Val 77.6%, Test 61.0%, Mult 4733.6\n",
      "\n",
      "[Fold 7/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 91.1%, Val 66.3%\n",
      "    Epoch 20: Train 100.0%, Val 66.1%\n",
      "    Epoch 40: Train 100.0%, Val 59.9%\n",
      "    Early stopping at epoch 44\n",
      "    efficientnet_b7 Fold 7: Train 100.0%, Val 69.3%, Test 71.1%, Mult 4927.2\n",
      "\n",
      "[Fold 8/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 91.6%, Val 64.7%\n",
      "    Epoch 20: Train 100.0%, Val 65.0%\n",
      "    Early stopping at epoch 32\n",
      "    efficientnet_b7 Fold 8: Train 99.8%, Val 72.1%, Test 59.2%, Mult 4268.3\n",
      "\n",
      "[Fold 9/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 92.4%, Val 71.4%\n",
      "    Epoch 20: Train 100.0%, Val 63.5%\n",
      "    Early stopping at epoch 27\n",
      "    efficientnet_b7 Fold 9: Train 100.0%, Val 75.9%, Test 58.8%, Mult 4462.9\n",
      "\n",
      "[Fold 10/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 89.5%, Val 77.2%\n",
      "    Epoch 20: Train 99.5%, Val 70.4%\n",
      "    Early stopping at epoch 31\n",
      "    efficientnet_b7 Fold 10: Train 100.0%, Val 85.5%, Test 61.5%, Mult 5258.2\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: RESNET18\n",
      "Batch size: 256\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] resnet18\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 47.4%, Val 48.0%\n",
      "    Epoch 20: Train 100.0%, Val 62.8%\n",
      "    Early stopping at epoch 37\n",
      "    resnet18 Fold 1: Train 100.0%, Val 65.1%, Test 53.2%, Mult 3463.3\n",
      "\n",
      "[Fold 2/10] resnet18\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 50.8%, Val 39.1%\n",
      "    Epoch 20: Train 100.0%, Val 56.0%\n",
      "    Early stopping at epoch 37\n",
      "    resnet18 Fold 2: Train 100.0%, Val 56.1%, Test 60.8%, Mult 3410.9\n",
      "\n",
      "[Fold 3/10] resnet18\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 51.2%, Val 58.7%\n",
      "    Epoch 20: Train 100.0%, Val 68.6%\n",
      "    Early stopping at epoch 32\n",
      "    resnet18 Fold 3: Train 100.0%, Val 70.1%, Test 67.4%, Mult 4724.7\n",
      "\n",
      "[Fold 4/10] resnet18\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 46.9%, Val 59.3%\n",
      "    Epoch 20: Train 100.0%, Val 72.1%\n",
      "    Epoch 40: Train 100.0%, Val 76.0%\n",
      "    Early stopping at epoch 40\n",
      "    resnet18 Fold 4: Train 100.0%, Val 77.0%, Test 60.4%, Mult 4650.8\n",
      "\n",
      "[Fold 5/10] resnet18\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 48.2%, Val 57.5%\n",
      "    Epoch 20: Train 100.0%, Val 70.0%\n",
      "    Epoch 40: Train 100.0%, Val 75.2%\n",
      "    Epoch 60: Train 100.0%, Val 79.1%\n",
      "    Early stopping at epoch 62\n",
      "    resnet18 Fold 5: Train 100.0%, Val 80.1%, Test 77.2%, Mult 6183.7\n",
      "\n",
      "[Fold 6/10] resnet18\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 46.5%, Val 68.8%\n",
      "    Epoch 20: Train 100.0%, Val 77.9%\n",
      "    Early stopping at epoch 32\n",
      "    resnet18 Fold 6: Train 100.0%, Val 83.3%, Test 59.9%, Mult 4989.7\n",
      "\n",
      "[Fold 7/10] resnet18\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 48.4%, Val 51.4%\n",
      "    Epoch 20: Train 100.0%, Val 63.7%\n",
      "    Early stopping at epoch 34\n",
      "    resnet18 Fold 7: Train 100.0%, Val 67.4%, Test 65.9%, Mult 4441.7\n",
      "\n",
      "[Fold 8/10] resnet18\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 46.9%, Val 44.4%\n",
      "    Epoch 20: Train 100.0%, Val 60.1%\n",
      "    Epoch 40: Train 100.0%, Val 58.5%\n",
      "    Early stopping at epoch 45\n",
      "    resnet18 Fold 8: Train 100.0%, Val 60.1%, Test 52.2%, Mult 3137.2\n",
      "\n",
      "[Fold 9/10] resnet18\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 49.9%, Val 52.5%\n",
      "    Epoch 20: Train 100.0%, Val 69.3%\n",
      "    Early stopping at epoch 29\n",
      "    resnet18 Fold 9: Train 100.0%, Val 74.4%, Test 68.6%, Mult 5103.8\n",
      "\n",
      "[Fold 10/10] resnet18\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 51.4%, Val 57.7%\n",
      "    Epoch 20: Train 100.0%, Val 74.6%\n",
      "    Early stopping at epoch 28\n",
      "    resnet18 Fold 10: Train 99.9%, Val 78.3%, Test 62.4%, Mult 4885.9\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: RESNET34\n",
      "Batch size: 192\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] resnet34\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet34...\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\resnet34-b627a593.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83.3M/83.3M [00:01<00:00, 71.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 62.7%, Val 56.1%\n",
      "    Epoch 20: Train 99.8%, Val 63.2%\n",
      "    Early stopping at epoch 32\n",
      "    resnet34 Fold 1: Train 100.0%, Val 72.6%, Test 52.9%, Mult 3840.5\n",
      "\n",
      "[Fold 2/10] resnet34\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 65.9%, Val 43.7%\n",
      "    Epoch 20: Train 100.0%, Val 56.9%\n",
      "    Epoch 40: Train 99.6%, Val 57.5%\n",
      "    Early stopping at epoch 40\n",
      "    resnet34 Fold 2: Train 100.0%, Val 61.3%, Test 58.8%, Mult 3604.4\n",
      "\n",
      "[Fold 3/10] resnet34\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 65.2%, Val 51.7%\n",
      "    Epoch 20: Train 100.0%, Val 66.8%\n",
      "    Epoch 40: Train 100.0%, Val 69.8%\n",
      "    Epoch 60: Train 100.0%, Val 68.1%\n",
      "    Early stopping at epoch 73\n",
      "    resnet34 Fold 3: Train 99.9%, Val 75.9%, Test 68.1%, Mult 5168.8\n",
      "\n",
      "[Fold 4/10] resnet34\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 63.7%, Val 69.3%\n",
      "    Epoch 20: Train 99.9%, Val 72.4%\n",
      "    Epoch 40: Train 99.8%, Val 67.9%\n",
      "    Early stopping at epoch 40\n",
      "    resnet34 Fold 4: Train 99.9%, Val 78.9%, Test 63.0%, Mult 4970.7\n",
      "\n",
      "[Fold 5/10] resnet34\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 61.4%, Val 60.0%\n",
      "    Epoch 20: Train 99.5%, Val 67.8%\n",
      "    Epoch 40: Train 100.0%, Val 76.5%\n",
      "    Epoch 60: Train 100.0%, Val 70.2%\n",
      "    Early stopping at epoch 62\n",
      "    resnet34 Fold 5: Train 99.9%, Val 81.8%, Test 66.6%, Mult 5447.9\n",
      "\n",
      "[Fold 6/10] resnet34\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 60.2%, Val 66.8%\n",
      "    Epoch 20: Train 100.0%, Val 77.6%\n",
      "    Epoch 40: Train 99.9%, Val 75.5%\n",
      "    Early stopping at epoch 46\n",
      "    resnet34 Fold 6: Train 99.9%, Val 79.2%, Test 51.7%, Mult 4094.6\n",
      "\n",
      "[Fold 7/10] resnet34\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 67.4%, Val 57.0%\n",
      "    Epoch 20: Train 99.8%, Val 56.0%\n",
      "    Epoch 40: Train 100.0%, Val 63.3%\n",
      "    Epoch 60: Train 100.0%, Val 59.1%\n",
      "    Early stopping at epoch 77\n",
      "    resnet34 Fold 7: Train 100.0%, Val 67.4%, Test 68.7%, Mult 4630.4\n",
      "\n",
      "[Fold 8/10] resnet34\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 66.7%, Val 54.8%\n",
      "    Epoch 20: Train 100.0%, Val 65.3%\n",
      "    Epoch 40: Train 100.0%, Val 59.8%\n",
      "    Early stopping at epoch 46\n",
      "    resnet34 Fold 8: Train 99.8%, Val 66.5%, Test 51.8%, Mult 3444.7\n",
      "\n",
      "[Fold 9/10] resnet34\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 66.0%, Val 62.7%\n",
      "    Epoch 20: Train 100.0%, Val 70.2%\n",
      "    Early stopping at epoch 26\n",
      "    resnet34 Fold 9: Train 98.7%, Val 73.9%, Test 69.6%, Mult 5143.4\n",
      "\n",
      "[Fold 10/10] resnet34\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 63.7%, Val 67.3%\n",
      "    Epoch 20: Train 99.9%, Val 71.8%\n",
      "    Epoch 40: Train 100.0%, Val 63.3%\n",
      "    Early stopping at epoch 43\n",
      "    resnet34 Fold 10: Train 100.0%, Val 81.5%, Test 66.9%, Mult 5452.4\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: RESNET50\n",
      "Batch size: 128\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] resnet50\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet50...\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:01<00:00, 61.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 23,244,805 trainable, 1,444,928 frozen\n",
      "    Epoch 0: Train 59.5%, Val 56.5%\n",
      "    Epoch 20: Train 100.0%, Val 62.3%\n",
      "    Early stopping at epoch 32\n",
      "    resnet50 Fold 1: Train 100.0%, Val 73.8%, Test 53.2%, Mult 3926.2\n",
      "\n",
      "[Fold 2/10] resnet50\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet50...\n",
      "Parameters: 23,244,805 trainable, 1,444,928 frozen\n",
      "    Epoch 0: Train 59.0%, Val 38.2%\n",
      "    Epoch 20: Train 99.9%, Val 58.3%\n",
      "    Epoch 40: Train 100.0%, Val 57.9%\n",
      "    Early stopping at epoch 45\n",
      "    resnet50 Fold 2: Train 99.9%, Val 58.3%, Test 59.6%, Mult 3474.7\n",
      "\n",
      "[Fold 3/10] resnet50\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet50...\n",
      "Parameters: 23,244,805 trainable, 1,444,928 frozen\n",
      "    Epoch 0: Train 59.0%, Val 54.9%\n",
      "    Epoch 20: Train 100.0%, Val 54.2%\n",
      "    Early stopping at epoch 27\n",
      "    resnet50 Fold 3: Train 100.0%, Val 63.4%, Test 67.1%, Mult 4254.1\n",
      "\n",
      "[Fold 4/10] resnet50\n",
      "Dataset: 5000 samples from 153 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading resnet50...\n",
      "Parameters: 23,244,805 trainable, 1,444,928 frozen\n",
      "    Epoch 0: Train 60.3%, Val 63.1%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RTX 5090 Optimized Configuration\n",
    "IMAGE_SIZE = (600, 600)  # Scaled up from 224x224\n",
    "AUGMENTATION_TARGET = 1000\n",
    "NUM_FOLDS = 10\n",
    "NUM_WORKERS = 0\n",
    "MIXED_PRECISION = True\n",
    "COMPILE_MODEL = False\n",
    "\n",
    "# Model configurations with appropriate batch sizes for 512x512 on RTX 5090\n",
    "MODEL_CONFIGS = {\n",
    "    # EfficientNet family\n",
    "    'efficientnet_b0': {'model_fn': models.efficientnet_b0, 'batch_size': 128, 'freeze_layers': 3},\n",
    "    'efficientnet_b1': {'model_fn': models.efficientnet_b1, 'batch_size': 96, 'freeze_layers': 3},\n",
    "    'efficientnet_b2': {'model_fn': models.efficientnet_b2, 'batch_size': 80, 'freeze_layers': 3},\n",
    "    'efficientnet_b3': {'model_fn': models.efficientnet_b3, 'batch_size': 64, 'freeze_layers': 3},\n",
    "    'efficientnet_b4': {'model_fn': models.efficientnet_b4, 'batch_size': 48, 'freeze_layers': 3},\n",
    "    'efficientnet_b5': {'model_fn': models.efficientnet_b5, 'batch_size': 32, 'freeze_layers': 3},\n",
    "    'efficientnet_b6': {'model_fn': models.efficientnet_b6, 'batch_size': 24, 'freeze_layers': 3},\n",
    "    'efficientnet_b7': {'model_fn': models.efficientnet_b7, 'batch_size': 16, 'freeze_layers': 3},\n",
    "    \n",
    "    # ResNet family\n",
    "    'resnet18': {'model_fn': models.resnet18, 'batch_size': 256, 'freeze_layers': 2},\n",
    "    'resnet34': {'model_fn': models.resnet34, 'batch_size': 192, 'freeze_layers': 2},\n",
    "    'resnet50': {'model_fn': models.resnet50, 'batch_size': 128, 'freeze_layers': 2},\n",
    "    'resnet101': {'model_fn': models.resnet101, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'resnet152': {'model_fn': models.resnet152, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # DenseNet family\n",
    "    'densenet121': {'model_fn': models.densenet121, 'batch_size': 96, 'freeze_layers': 2},\n",
    "    'densenet169': {'model_fn': models.densenet169, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'densenet201': {'model_fn': models.densenet201, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # MobileNet family\n",
    "    'mobilenet_v2': {'model_fn': models.mobilenet_v2, 'batch_size': 192, 'freeze_layers': 3},\n",
    "    'mobilenet_v3_small': {'model_fn': models.mobilenet_v3_small, 'batch_size': 256, 'freeze_layers': 3},\n",
    "    'mobilenet_v3_large': {'model_fn': models.mobilenet_v3_large, 'batch_size': 192, 'freeze_layers': 3},\n",
    "    \n",
    "    # RegNet family\n",
    "    'regnet_y_400mf': {'model_fn': models.regnet_y_400mf, 'batch_size': 128, 'freeze_layers': 2},\n",
    "    'regnet_y_800mf': {'model_fn': models.regnet_y_800mf, 'batch_size': 96, 'freeze_layers': 2},\n",
    "    'regnet_y_1_6gf': {'model_fn': models.regnet_y_1_6gf, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'regnet_y_3_2gf': {'model_fn': models.regnet_y_3_2gf, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # ConvNeXt family\n",
    "    'convnext_tiny': {'model_fn': models.convnext_tiny, 'batch_size': 96, 'freeze_layers': 2},\n",
    "    'convnext_small': {'model_fn': models.convnext_small, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'convnext_base': {'model_fn': models.convnext_base, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # MaxViT family\n",
    "    'maxvit_t': {'model_fn': models.maxvit_t, 'batch_size': 48, 'freeze_layers': 2},\n",
    "    \n",
    "    # Swin Transformer family\n",
    "    'swin_t': {'model_fn': models.swin_t, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    'swin_s': {'model_fn': models.swin_s, 'batch_size': 48, 'freeze_layers': 2},\n",
    "    'swin_b': {'model_fn': models.swin_b, 'batch_size': 32, 'freeze_layers': 2},\n",
    "    \n",
    "    # Vision Transformer family\n",
    "    'vit_b_16': {'model_fn': models.vit_b_16, 'batch_size': 64, 'freeze_layers': 6},\n",
    "    'vit_b_32': {'model_fn': models.vit_b_32, 'batch_size': 96, 'freeze_layers': 6},\n",
    "    'vit_l_16': {'model_fn': models.vit_l_16, 'batch_size': 32, 'freeze_layers': 8},\n",
    "}\n",
    "\n",
    "# Training hyperparameters\n",
    "TRAINING_CONFIG = {\n",
    "    'backbone_lr': 0.0001,\n",
    "    'classifier_lr': 0.0005,\n",
    "    'optimizer': 'adamw',\n",
    "    'weight_decay': 0.05,\n",
    "    'scheduler': 'cosine',\n",
    "    'label_smoothing': 0.1,\n",
    "    'dropout': 0.3,\n",
    "    'max_epochs': 80,\n",
    "    'patience': 25,\n",
    "    'augmentation_strength': 'medium'\n",
    "}\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    color_path = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "    gray_path = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*_NDA.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    \n",
    "    return np.array(filtered_images, dtype=np.uint8), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image(image, strength='medium'):\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    if strength == 'light':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.5, 0.3, 0.6, 0.2, 0.1\n",
    "        rot_range, bright_range = 8, (0.85, 1.15)\n",
    "    elif strength == 'medium':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.7, 0.5, 0.8, 0.4, 0.3\n",
    "        rot_range, bright_range = 12, (0.75, 1.25)\n",
    "    else:  # heavy\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.8, 0.6, 0.9, 0.5, 0.4\n",
    "        rot_range, bright_range = 18, (0.65, 1.35)\n",
    "    \n",
    "    if random.random() < rot_prob:\n",
    "        angle = random.uniform(-rot_range, rot_range)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < flip_prob:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    if random.random() < bright_prob:\n",
    "        alpha = random.uniform(*bright_range)\n",
    "        beta = random.randint(-20, 20)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    if random.random() < gamma_prob:\n",
    "        gamma = random.uniform(0.85, 1.15)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    if random.random() < noise_prob:\n",
    "        noise = np.random.normal(0, 5, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class OptimizedDataset(Dataset):\n",
    "    def __init__(self, base_images, labels, aug_strength='medium', target_per_class=1000, training=True):\n",
    "        self.base_images = base_images\n",
    "        self.labels = np.array(labels)\n",
    "        self.aug_strength = aug_strength\n",
    "        self.training = training\n",
    "        self.target_per_class = target_per_class\n",
    "        \n",
    "        unique_classes = np.unique(labels)\n",
    "        self.class_to_indices = {}\n",
    "        for cls in unique_classes:\n",
    "            self.class_to_indices[cls] = np.where(self.labels == cls)[0]\n",
    "        \n",
    "        self.num_classes = len(unique_classes)\n",
    "        self.class_list = sorted(unique_classes)\n",
    "        self.length = self.num_classes * self.target_per_class\n",
    "        \n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(3, 1, 1)\n",
    "        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(3, 1, 1)\n",
    "        \n",
    "        print(f\"Dataset: {self.length} samples from {len(base_images)} base images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        class_idx = idx // self.target_per_class\n",
    "        within_class_idx = idx % self.target_per_class\n",
    "        \n",
    "        target_class = self.class_list[class_idx]\n",
    "        available_indices = self.class_to_indices[target_class]\n",
    "        \n",
    "        base_idx = available_indices[within_class_idx % len(available_indices)]\n",
    "        image = self.base_images[base_idx].copy()\n",
    "        \n",
    "        if self.training and within_class_idx >= len(available_indices):\n",
    "            image = enhanced_augment_image(image, self.aug_strength)\n",
    "        \n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.transpose(2, 0, 1)\n",
    "        \n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = np.flip(image, axis=2).copy()\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return torch.from_numpy(image.astype(np.float32)), target_class\n",
    "\n",
    "class MultiModelTrainer:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"multimodel_512_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.results = []\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "            \n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            \n",
    "            if MIXED_PRECISION:\n",
    "                self.scaler = torch.amp.GradScaler('cuda')\n",
    "            else:\n",
    "                self.scaler = None\n",
    "    \n",
    "    def create_model(self, model_name, model_config):\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        model = model_config['model_fn'](weights='DEFAULT')\n",
    "        \n",
    "        # Freeze layers based on model type\n",
    "        freeze_layers = model_config.get('freeze_layers', 2)\n",
    "        \n",
    "        if hasattr(model, 'features'):  # EfficientNet, DenseNet, etc.\n",
    "            layers_to_freeze = list(model.features.children())[:freeze_layers]\n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            if hasattr(model, 'classifier'):\n",
    "                original_features = model.classifier[-1].in_features if isinstance(model.classifier, nn.Sequential) else model.classifier.in_features\n",
    "                model.classifier = nn.Sequential(\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                    nn.Linear(original_features, 512),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                    nn.Linear(256, self.num_classes)\n",
    "                )\n",
    "        \n",
    "        elif hasattr(model, 'fc'):  # ResNet, RegNet\n",
    "            if hasattr(model, 'layer1'):  # ResNet\n",
    "                layers_to_freeze = [model.conv1, model.bn1]\n",
    "                if freeze_layers >= 1:\n",
    "                    layers_to_freeze.append(model.layer1)\n",
    "                if freeze_layers >= 2:\n",
    "                    layers_to_freeze.append(model.layer2)\n",
    "            \n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            original_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        elif hasattr(model, 'head'):  # Vision Transformers, Swin, ConvNeXt\n",
    "            # Freeze some transformer blocks\n",
    "            if hasattr(model, 'encoder') and hasattr(model.encoder, 'layers'):  # ViT\n",
    "                layers_to_freeze = list(model.encoder.layers.children())[:freeze_layers]\n",
    "                for layer in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            elif hasattr(model, 'features'):  # ConvNeXt\n",
    "                layers_to_freeze = list(model.features.children())[:freeze_layers]\n",
    "                for layer in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            \n",
    "            original_features = model.head.in_features\n",
    "            model.head = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Parameters: {trainable_params:,} trainable, {frozen_params:,} frozen\")\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def get_optimizer(self, model):\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if any(keyword in name for keyword in ['classifier', 'fc', 'head']):\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        param_groups = [\n",
    "            {'params': backbone_params, 'lr': TRAINING_CONFIG['backbone_lr']},\n",
    "            {'params': classifier_params, 'lr': TRAINING_CONFIG['classifier_lr']}\n",
    "        ]\n",
    "        \n",
    "        return optim.AdamW(param_groups, weight_decay=TRAINING_CONFIG['weight_decay'], fused=True)\n",
    "    \n",
    "    def get_scheduler(self, optimizer):\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=TRAINING_CONFIG['max_epochs'], eta_min=1e-6\n",
    "        )\n",
    "    \n",
    "    def train_model(self, model, model_name, train_loader, val_loader, test_loader, fold_num):\n",
    "        optimizer = self.get_optimizer(model)\n",
    "        scheduler = self.get_scheduler(optimizer)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=TRAINING_CONFIG['label_smoothing'])\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        best_train_acc = 0.0\n",
    "        best_test_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(TRAINING_CONFIG['max_epochs']):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if MIXED_PRECISION and self.scaler:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.step(optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    if MIXED_PRECISION:\n",
    "                        with torch.amp.autocast('cuda'):\n",
    "                            outputs = model(images)\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_train_acc = train_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "                \n",
    "                # Test accuracy when validation improves\n",
    "                model.eval()\n",
    "                test_correct = 0\n",
    "                test_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for images, labels in test_loader:\n",
    "                        images, labels = images.to(self.device), labels.to(self.device)\n",
    "                        \n",
    "                        if MIXED_PRECISION:\n",
    "                            with torch.amp.autocast('cuda'):\n",
    "                                outputs1 = model(images)\n",
    "                                outputs2 = model(torch.flip(images, [3]))\n",
    "                                outputs = (outputs1 + outputs2) / 2\n",
    "                        else:\n",
    "                            outputs1 = model(images)\n",
    "                            outputs2 = model(torch.flip(images, [3]))\n",
    "                            outputs = (outputs1 + outputs2) / 2\n",
    "                        \n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        test_total += labels.size(0)\n",
    "                        test_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                best_test_acc = 100 * test_correct / test_total\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= TRAINING_CONFIG['patience']:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Calculate multiplicative metric\n",
    "        multiplicative_score = (best_val_acc / 100) * (best_test_acc / 100) * 10000\n",
    "        \n",
    "        # Save model\n",
    "        save_path = os.path.join(self.save_dir, f\"{model_name}_fold{fold_num}_val{best_val_acc:.1f}_test{best_test_acc:.1f}_mult{multiplicative_score:.1f}.pth\")\n",
    "        torch.save({\n",
    "            'model_state_dict': best_state,\n",
    "            'model_name': model_name,\n",
    "            'fold_number': fold_num,\n",
    "            'train_accuracy': best_train_acc,\n",
    "            'validation_accuracy': best_val_acc,\n",
    "            'test_accuracy': best_test_acc,\n",
    "            'multiplicative_score': multiplicative_score,\n",
    "            'image_size': IMAGE_SIZE,\n",
    "            'training_config': TRAINING_CONFIG\n",
    "        }, save_path)\n",
    "        \n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'fold': fold_num,\n",
    "            'train_acc': best_train_acc,\n",
    "            'val_acc': best_val_acc,\n",
    "            'test_acc': best_test_acc,\n",
    "            'multiplicative_score': multiplicative_score,\n",
    "            'save_path': save_path\n",
    "        }\n",
    "        \n",
    "        print(f\"    {model_name} Fold {fold_num}: Train {best_train_acc:.1f}%, Val {best_val_acc:.1f}%, Test {best_test_acc:.1f}%, Mult {multiplicative_score:.1f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_comprehensive_evaluation(self, images, ages, sources):\n",
    "        print(f\"COMPREHENSIVE MODEL EVALUATION - {len(MODEL_CONFIGS)} models, {NUM_FOLDS} folds\")\n",
    "        print(f\"Image size: {IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}\")\n",
    "        print(f\"Total experiments: {len(MODEL_CONFIGS) * NUM_FOLDS}\")\n",
    "        \n",
    "        unique_ages = sorted(list(set(ages)))\n",
    "        label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "        y_indices = np.array([label_mapping[age] for age in ages])\n",
    "        \n",
    "        print(f\"Classes: {len(unique_ages)}\")\n",
    "        \n",
    "        for model_name, model_config in MODEL_CONFIGS.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"TESTING MODEL: {model_name.upper()}\")\n",
    "            print(f\"Batch size: {model_config['batch_size']}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            for fold in range(1, NUM_FOLDS + 1):\n",
    "                print(f\"\\n[Fold {fold}/{NUM_FOLDS}] {model_name}\")\n",
    "                \n",
    "                try:\n",
    "                    # Data splitting\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        images, y_indices, test_size=0.2, random_state=fold * 42, stratify=y_indices\n",
    "                    )\n",
    "                    \n",
    "                    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "                        X_train, y_train, test_size=0.2, random_state=fold * 42 + 1, stratify=y_train\n",
    "                    )\n",
    "                    \n",
    "                    # Create datasets\n",
    "                    train_dataset = OptimizedDataset(X_train_final, y_train_final, \n",
    "                                                   TRAINING_CONFIG['augmentation_strength'], AUGMENTATION_TARGET, True)\n",
    "                    val_dataset = OptimizedDataset(X_val, y_val, 'light', 200, False)\n",
    "                    test_dataset = OptimizedDataset(X_test, y_test, 'light', 200, False)\n",
    "                    \n",
    "                    # Create data loaders\n",
    "                    batch_size = model_config['batch_size']\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "                    \n",
    "                    # Create and train model\n",
    "                    model = self.create_model(model_name, model_config)\n",
    "                    result = self.train_model(model, model_name, train_loader, val_loader, test_loader, fold)\n",
    "                    self.results.append(result)\n",
    "                    \n",
    "                    # Cleanup\n",
    "                    del model, train_dataset, val_dataset, test_dataset, train_loader, val_loader, test_loader\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    FAILED: {str(e)}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        self.save_results()\n",
    "        self.print_summary()\n",
    "    \n",
    "    def save_results(self):\n",
    "        results_path = os.path.join(self.save_dir, \"comprehensive_results.json\")\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"\\nResults saved to: {results_path}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Group by model\n",
    "        model_results = {}\n",
    "        for result in self.results:\n",
    "            model_name = result['model_name']\n",
    "            if model_name not in model_results:\n",
    "                model_results[model_name] = []\n",
    "            model_results[model_name].append(result)\n",
    "        \n",
    "        # Print summary for each model\n",
    "        for model_name, results in model_results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "            \n",
    "            avg_train = np.mean([r['train_acc'] for r in results])\n",
    "            avg_val = np.mean([r['val_acc'] for r in results])\n",
    "            avg_test = np.mean([r['test_acc'] for r in results])\n",
    "            avg_mult = np.mean([r['multiplicative_score'] for r in results])\n",
    "            \n",
    "            print(f\"{model_name:20} | Train: {avg_train:5.1f}% | Val: {avg_val:5.1f}% | Test: {avg_test:5.1f}% | Mult: {avg_mult:6.1f}\")\n",
    "        \n",
    "        # Best overall model\n",
    "        if self.results:\n",
    "            best_result = max(self.results, key=lambda x: x['multiplicative_score'])\n",
    "            print(f\"\\nBest model: {best_result['model_name']} (Fold {best_result['fold']})\")\n",
    "            print(f\"Multiplicative score: {best_result['multiplicative_score']:.1f}\")\n",
    "            print(f\"Val: {best_result['val_acc']:.1f}%, Test: {best_result['test_acc']:.1f}%\")\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"RTX 5090 Comprehensive Model Evaluation\")\n",
    "    print(\"512x512 Images - Multiple Architectures\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    images, ages, sources = load_combined_data()\n",
    "    \n",
    "    trainer = MultiModelTrainer(num_classes=len(set(ages)))\n",
    "    trainer.run_comprehensive_evaluation(images, ages, sources)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nComplete evaluation finished in: {elapsed:.1f} minutes\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561f6ee-b954-43c4-b2f7-fcf0cb6639ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
