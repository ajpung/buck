{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a5b6e2-ade6-40ef-9121-92a94d2a855e",
   "metadata": {},
   "source": [
    "## What changed?\n",
    "\n",
    "This notebook takes the output result of `250813_nda_all` and attempts to optimize a single model instead of an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62247f7c-827d-4f33-99dd-bc10db9c0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"❌ CUDA not detected by PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e236b-95da-4e2a-b91a-0f540984910c",
   "metadata": {},
   "source": [
    "### Test GPU implementation\n",
    "ResNet50 and EfficientNet are widely-used models in this field. Given the newness of the RTX 5090, it's worth seeing if these work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a6149-b921-426f-9593-f0e3756609bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Test ResNet50 specifically\n",
    "model = models.resnet50(pretrained=True).cuda()\n",
    "test_batch = torch.randn(2, 3, 224, 224).cuda()\n",
    "try:\n",
    "    output = model(test_batch)\n",
    "    print(\"ResNet50 works!\")\n",
    "except Exception as e:\n",
    "    print(f\"ResNet50 failed: {e}\")\n",
    "\n",
    "# Test EfficientNet\n",
    "try:\n",
    "    model_eff = models.efficientnet_b0(pretrained=True).cuda()\n",
    "    output_eff = model_eff(test_batch)\n",
    "    print(\"EfficientNet works!\")\n",
    "except Exception as e:\n",
    "    print(f\"EfficientNet failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1083fe-4efc-4cf3-874a-b0056cb1eafa",
   "metadata": {},
   "source": [
    "### Feeding different data folds to model\n",
    "\n",
    "- Trying to recapture  ✓ Val: 84.2%, Test: 81.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a32385-d38a-49e6-95e2-e5de010edd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Check current GPU memory\n",
    "allocated = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"GPU memory after cleanup: {allocated:.1f} GB\")\n",
    "\n",
    "# RTX 5090 Calculated Configuration - Targeting ~27GB VRAM usage\n",
    "FIXED_HYPERPARAMS = {\n",
    "    'backbone_lr': 0.0001,\n",
    "    'classifier_lr': 0.0005,\n",
    "    'batch_size': 384,  # Calculated to use ~27GB total VRAM\n",
    "    'optimizer': 'adamw',\n",
    "    'dropout': 0.4,\n",
    "    'freeze_layers': 3\n",
    "}\n",
    "\n",
    "# Missing hyperparameters to test\n",
    "MISSING_HYPERPARAMS = {\n",
    "    'weight_decay': [0.05, 0.08],\n",
    "    'scheduler': ['cosine', 'plateau'],\n",
    "    'label_smoothing': [0.15, 0.2],\n",
    "    'augmentation_strength': ['medium', 'heavy']\n",
    "}\n",
    "\n",
    "# Calculated RTX 5090 settings\n",
    "IMAGE_SIZE = (576, 576)  # Calculated to fit with batch_size=384 in ~27GB total\n",
    "AUGMENTATION_TARGET = 2000\n",
    "NUM_FOLDS = 25\n",
    "NUM_WORKERS = 0\n",
    "MIXED_PRECISION = True\n",
    "COMPILE_MODEL = False\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    color_path = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "    gray_path = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*_NDA.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    \n",
    "    return np.array(filtered_images, dtype=np.uint8), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image(image, strength='medium'):\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    if strength == 'light':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.5, 0.3, 0.6, 0.2, 0.1\n",
    "        rot_range, bright_range = 10, (0.8, 1.2)\n",
    "    elif strength == 'medium':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.7, 0.5, 0.8, 0.4, 0.3\n",
    "        rot_range, bright_range = 15, (0.7, 1.3)\n",
    "    else:  # heavy\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.8, 0.6, 0.9, 0.5, 0.4\n",
    "        rot_range, bright_range = 20, (0.6, 1.4)\n",
    "    \n",
    "    if random.random() < rot_prob:\n",
    "        angle = random.uniform(-rot_range, rot_range)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < flip_prob:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.4:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    if random.random() < bright_prob:\n",
    "        alpha = random.uniform(*bright_range)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    if random.random() < gamma_prob:\n",
    "        gamma = random.uniform(0.8, 1.2)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    if random.random() < noise_prob:\n",
    "        noise = np.random.normal(0, 7, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class MemoryEfficientDataset(Dataset):\n",
    "    def __init__(self, base_images, labels, aug_strength='medium', target_per_class=2000, training=True):\n",
    "        self.base_images = base_images\n",
    "        self.labels = np.array(labels)\n",
    "        self.aug_strength = aug_strength\n",
    "        self.training = training\n",
    "        self.target_per_class = target_per_class\n",
    "        \n",
    "        unique_classes = np.unique(labels)\n",
    "        self.class_to_indices = {}\n",
    "        for cls in unique_classes:\n",
    "            self.class_to_indices[cls] = np.where(self.labels == cls)[0]\n",
    "        \n",
    "        self.num_classes = len(unique_classes)\n",
    "        self.class_list = sorted(unique_classes)\n",
    "        self.length = self.num_classes * self.target_per_class\n",
    "        \n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(3, 1, 1)\n",
    "        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(3, 1, 1)\n",
    "        \n",
    "        print(f\"Memory-efficient dataset: {self.length} samples from {len(base_images)} base images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        class_idx = idx // self.target_per_class\n",
    "        within_class_idx = idx % self.target_per_class\n",
    "        \n",
    "        target_class = self.class_list[class_idx]\n",
    "        available_indices = self.class_to_indices[target_class]\n",
    "        \n",
    "        base_idx = available_indices[within_class_idx % len(available_indices)]\n",
    "        image = self.base_images[base_idx].copy()\n",
    "        \n",
    "        if self.training and within_class_idx >= len(available_indices):\n",
    "            image = enhanced_augment_image(image, self.aug_strength)\n",
    "        \n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.transpose(2, 0, 1)\n",
    "        \n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = np.flip(image, axis=2).copy()\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return torch.from_numpy(image.astype(np.float32)), target_class\n",
    "\n",
    "class RTX5090OptimizedModel:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"rtx5090_optimized_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.best_multiplicative_score = 0.0\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "            \n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            \n",
    "            if MIXED_PRECISION:\n",
    "                print(\"Mixed precision enabled\")\n",
    "                self.scaler = torch.amp.GradScaler('cuda')\n",
    "            else:\n",
    "                self.scaler = None\n",
    "    \n",
    "    def create_model(self, dropout=0.3, freeze_layers=3):\n",
    "        print(\"Loading EfficientNet-B4 (optimized for RTX 5090)\")\n",
    "        model = models.efficientnet_b4(weights='DEFAULT')\n",
    "        \n",
    "        layers_to_freeze = list(model.features.children())[:freeze_layers]\n",
    "        for layer in layers_to_freeze:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Model parameters: {trainable_params:,} trainable, {frozen_params:,} frozen\")\n",
    "        \n",
    "        original_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(original_features, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout * 0.25),\n",
    "            nn.Linear(512, self.num_classes)\n",
    "        )\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def get_optimizer(self, model, opt_type, backbone_lr, classifier_lr, weight_decay):\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'classifier' in name:\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        param_groups = [\n",
    "            {'params': backbone_params, 'lr': backbone_lr},\n",
    "            {'params': classifier_params, 'lr': classifier_lr}\n",
    "        ]\n",
    "        \n",
    "        print(f\"Optimizer groups: {len(backbone_params)} backbone, {len(classifier_params)} classifier params\")\n",
    "        \n",
    "        if opt_type == 'adamw':\n",
    "            return optim.AdamW(param_groups, weight_decay=weight_decay, fused=True)\n",
    "        elif opt_type == 'sgd':\n",
    "            return optim.SGD(param_groups, weight_decay=weight_decay, momentum=0.9, fused=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {opt_type}\")\n",
    "    \n",
    "    def get_scheduler(self, optimizer, scheduler_type, max_epochs):\n",
    "        if scheduler_type == 'cosine':\n",
    "            return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-6)\n",
    "        elif scheduler_type == 'plateau':\n",
    "            return optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=12, factor=0.5, verbose=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scheduler: {scheduler_type}\")\n",
    "    \n",
    "    def train_with_hyperparams(self, train_loader, val_loader, test_loader, hyperparams, fold_num):\n",
    "        print(f\"Creating model for fold {fold_num}...\")\n",
    "        model = self.create_model(\n",
    "            dropout=hyperparams['dropout'], \n",
    "            freeze_layers=hyperparams['freeze_layers']\n",
    "        )\n",
    "        \n",
    "        optimizer = self.get_optimizer(\n",
    "            model, hyperparams['optimizer'], \n",
    "            hyperparams['backbone_lr'], hyperparams['classifier_lr'], \n",
    "            hyperparams['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = self.get_scheduler(optimizer, hyperparams['scheduler'], 100)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=hyperparams['label_smoothing'])\n",
    "        \n",
    "        start_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"Starting VRAM usage: {start_memory:.1f} GB\")\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        patience = 35\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(100):\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if MIXED_PRECISION and self.scaler:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.step(optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                if batch_idx == 0:\n",
    "                    current_memory = torch.cuda.memory_allocated() / 1e9\n",
    "                    print(f\"    Training VRAM: {current_memory:.1f} GB\", end=\"\")\n",
    "            \n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    if MIXED_PRECISION:\n",
    "                        with torch.amp.autocast('cuda'):\n",
    "                            outputs = model(images)\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if hyperparams['scheduler'] == 'plateau':\n",
    "                scheduler.step(val_acc)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            if epoch % 25 == 0 and epoch > 0:\n",
    "                print(f\" - Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}% ({epoch_time:.1f}s)\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                if MIXED_PRECISION:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs1 = model(images)\n",
    "                        outputs2 = model(torch.flip(images, [3]))\n",
    "                        outputs = (outputs1 + outputs2) / 2\n",
    "                else:\n",
    "                    outputs1 = model(images)\n",
    "                    outputs2 = model(torch.flip(images, [3]))\n",
    "                    outputs = (outputs1 + outputs2) / 2\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        multiplicative_score = (best_val_acc / 100) * (test_acc / 100) * 10000\n",
    "        \n",
    "        if multiplicative_score > self.best_multiplicative_score:\n",
    "            self.best_multiplicative_score = multiplicative_score\n",
    "            \n",
    "            save_path = os.path.join(self.save_dir, f\"best_rtx5090_mult_{multiplicative_score:.1f}_val_{best_val_acc:.1f}_test_{test_acc:.1f}_fold_{fold_num}.pth\")\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'hyperparams': hyperparams,\n",
    "                'val_accuracy': best_val_acc,\n",
    "                'test_accuracy': test_acc,\n",
    "                'train_accuracy': train_acc,\n",
    "                'multiplicative_score': multiplicative_score,\n",
    "                'fold_number': fold_num,\n",
    "                'model_info': 'EfficientNet-B4_576x576_batch384'\n",
    "            }, save_path)\n",
    "            print(f\"    NEW GLOBAL BEST! Mult: {multiplicative_score:.1f}, Val: {best_val_acc:.1f}%, Test: {test_acc:.1f}% (Fold {fold_num})\")\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def run_fold_search(self, images, ages, sources):\n",
    "        print(f\"RTX 5090 Optimized Training - {NUM_FOLDS} folds\")\n",
    "        print(f\"Configuration: EfficientNet-B4, {IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}, batch_size={FIXED_HYPERPARAMS['batch_size']}\")\n",
    "        print(f\"Target VRAM usage: ~27GB\")\n",
    "        \n",
    "        unique_ages = sorted(list(set(ages)))\n",
    "        label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "        y_indices = np.array([label_mapping[age] for age in ages])\n",
    "        \n",
    "        print(f\"Classes: {len(unique_ages)}\")\n",
    "        print(f\"Label mapping: {label_mapping}\")\n",
    "        \n",
    "        missing_keys = list(MISSING_HYPERPARAMS.keys())\n",
    "        missing_values = list(MISSING_HYPERPARAMS.values())\n",
    "        missing_combos = list(itertools.product(*missing_values))\n",
    "        \n",
    "        best_found = False\n",
    "        \n",
    "        for combo_idx, missing_combo in enumerate(missing_combos, 1):\n",
    "            hyperparams = FIXED_HYPERPARAMS.copy()\n",
    "            for key, value in zip(missing_keys, missing_combo):\n",
    "                hyperparams[key] = value\n",
    "            \n",
    "            print(f\"\\n[Config {combo_idx}/{len(missing_combos)}] Testing:\")\n",
    "            print(f\"  EfficientNet-B4, batch={hyperparams['batch_size']}, lr={hyperparams['backbone_lr']}/{hyperparams['classifier_lr']}\")\n",
    "            print(f\"  wd={hyperparams['weight_decay']}, sched={hyperparams['scheduler']}, smooth={hyperparams['label_smoothing']}, aug={hyperparams['augmentation_strength']}\")\n",
    "            \n",
    "            for fold in range(1, NUM_FOLDS + 1):\n",
    "                random.seed(fold * 42)\n",
    "                np.random.seed(fold * 42)\n",
    "                torch.manual_seed(fold * 42)\n",
    "                \n",
    "                print(f\"\\n  [Fold {fold:2d}/{NUM_FOLDS}]\", end=\" \")\n",
    "                \n",
    "                try:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        images, y_indices, test_size=0.2, random_state=fold * 42, stratify=y_indices\n",
    "                    )\n",
    "                    \n",
    "                    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "                        X_train, y_train, test_size=0.2, random_state=fold * 42 + 1, stratify=y_train\n",
    "                    )\n",
    "                    \n",
    "                    train_dataset = MemoryEfficientDataset(X_train_final, y_train_final, \n",
    "                                                         hyperparams['augmentation_strength'], AUGMENTATION_TARGET, True)\n",
    "                    val_dataset = MemoryEfficientDataset(X_val, y_val, 'medium', 100, False)\n",
    "                    test_dataset = MemoryEfficientDataset(X_test, y_test, 'medium', 100, False)\n",
    "                    \n",
    "                    train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], \n",
    "                                            shuffle=True, num_workers=0)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'], \n",
    "                                          shuffle=False, num_workers=0)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'], \n",
    "                                           shuffle=False, num_workers=0)\n",
    "                    \n",
    "                    was_best = self.train_with_hyperparams(\n",
    "                        train_loader, val_loader, test_loader, hyperparams, fold\n",
    "                    )\n",
    "                    \n",
    "                    if was_best:\n",
    "                        best_found = True\n",
    "                    \n",
    "                    # Aggressive cleanup\n",
    "                    del train_dataset, val_dataset, test_dataset, train_loader, val_loader, test_loader, model\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.synchronize()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                    if allocated > 5.0:\n",
    "                        print(f\"    Warning: {allocated:.1f}GB still allocated\")\n",
    "                        torch.cuda.empty_cache()\n",
    "                        torch.cuda.synchronize()\n",
    "                        gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"FAILED: {str(e)}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.synchronize()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"RTX 5090 OPTIMIZED TRAINING COMPLETE\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Best Multiplicative Score: {self.best_multiplicative_score:.1f}\")\n",
    "        print(f\"Configuration: EfficientNet-B4, {IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}, batch_size={FIXED_HYPERPARAMS['batch_size']}\")\n",
    "        print(f\"Results saved to: {self.save_dir}\")\n",
    "        \n",
    "        return best_found\n",
    "\n",
    "def main():\n",
    "    # Add this at the very start of main()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"RTX 5090 Optimized Deer Age Training\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    import psutil\n",
    "    process = psutil.Process()\n",
    "    print(f\"Initial RAM usage: {process.memory_info().rss / 1e9:.1f} GB\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    images, ages, sources = load_combined_data()\n",
    "    print(f\"After data loading: {process.memory_info().rss / 1e9:.1f} GB\")\n",
    "    \n",
    "    trainer = RTX5090OptimizedModel(num_classes=len(set(ages)))\n",
    "    \n",
    "    success = trainer.run_fold_search(images, ages, sources)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nTotal Training Time: {elapsed:.1f} minutes\")\n",
    "    print(f\"Final RAM usage: {process.memory_info().rss / 1e9:.1f} GB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4dea7-77c0-4a10-8ebb-4af5e0f13bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
