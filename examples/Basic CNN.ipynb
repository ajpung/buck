{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9bd270a-95d8-4330-92ef-478f908a8604",
   "metadata": {},
   "source": [
    "## Ingest images\n",
    "Images of each deer are roughly square, and stored in a local folder. The images are ingested via `glob` and files that do not contain year information is removed. Once ingested, resized, and stacked, the labels are extracted from each images based on their filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a96314-0633-40b4-b727-9ceef74d57af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 images found\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from generic.analysis.basics import extract_labels\n",
    "from generic.analysis.basics import ingest_resize_stack\n",
    "\n",
    "# Find/ingest files in folder; force square & b/w\n",
    "files = glob(\"..\\\\images\\\\squared\\\\*.png\")\n",
    "files = [s for s in files if \"xpx\" not in s]\n",
    "print(len(files), \"images found\")\n",
    "\n",
    "# Ingest images\n",
    "images = ingest_resize_stack(files)\n",
    "_,_,_,ages,_ = extract_labels(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecdea1f-2cb5-4ede-bce3-b6a2b349a836",
   "metadata": {},
   "source": [
    "## Split datasets\n",
    "The image stack is then split into training and test data, with a split of 80/20 -- 80% of the data resides in the training set, while the remaining 20% defines the test data. The training and test data are then normalized, and the labels for each dataset are cast to categorical values instead of their age values.\n",
    "\n",
    "The validation data is then extracted from the training dataset, again in an 80/20 split -- 80% of the previous training data remains within the training dataset, and 20% is redefined as the validation set. Each dataset (training, validation, and test) are reshaped based on their format of grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb9da76-9dd0-4571-bda9-5161e636d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {np.float64(1.5): 0, np.float64(2.5): 1, np.float64(3.5): 2, np.float64(4.5): 3, np.float64(5.5): 4, np.float64(6.5): 5, np.float64(12.5): 6}\n",
      "Converted labels: [1 1 2 1 2 3 4 6 4 4 2 4 0 2 4 1 1 3 0 4 1 4 1 2 0 3 1 3 1 2 1 2 0 1 2 4 5\n",
      " 0 1 2]\n",
      "\n",
      "Training set label distribution:\n",
      "Label 0 (1.5): 3 samples (9.38%)\n",
      "Label 1 (2.5): 9 samples (28.12%)\n",
      "Label 2 (3.5): 8 samples (25.00%)\n",
      "Label 3 (4.5): 3 samples (9.38%)\n",
      "Label 4 (5.5): 7 samples (21.88%)\n",
      "Label 5 (6.5): 1 samples (3.12%)\n",
      "Label 6 (12.5): 1 samples (3.12%)\n",
      "\n",
      "Test set label distribution:\n",
      "Label 0 (1.5): 2 samples (25.00%)\n",
      "Label 1 (2.5): 3 samples (37.50%)\n",
      "Label 2 (3.5): 1 samples (12.50%)\n",
      "Label 3 (4.5): 1 samples (12.50%)\n",
      "Label 4 (5.5): 1 samples (12.50%)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert labels to integers from 0 to 5 for proper one-hot encoding\n",
    "# Create a mapping from your floating-point labels to integers\n",
    "label_mapping = {label: i for i, label in enumerate(np.unique(ages))}\n",
    "print(\"Label mapping:\", label_mapping)\n",
    "\n",
    "# Apply the mapping to convert labels to integers\n",
    "integer_labels = np.array([label_mapping[l] for l in ages])\n",
    "print(\"Converted labels:\", integer_labels)\n",
    "\n",
    "# Use a regular train_test_split without stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, integer_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Check the distribution after splitting\n",
    "print(\"\\nTraining set label distribution:\")\n",
    "unique_train_labels = np.unique(y_train)\n",
    "for label in unique_train_labels:\n",
    "    count = np.sum(y_train == label)\n",
    "    print(f\"Label {label} ({list(label_mapping.keys())[list(label_mapping.values()).index(label)]}): {count} samples ({count/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nTest set label distribution:\")\n",
    "unique_test_labels = np.unique(y_test)\n",
    "for label in unique_test_labels:\n",
    "    count = np.sum(y_test == label)\n",
    "    print(f\"Label {label} ({list(label_mapping.keys())[list(label_mapping.values()).index(label)]}): {count} samples ({count/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "# Normalize the images\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b8ea21-15ef-48f8-bb95-d46e12734dea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input to `.fit()` should have rank 4. Got array with shape: (32, 288, 288)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[32m      7\u001b[39m datagen = ImageDataGenerator(\n\u001b[32m      8\u001b[39m     rotation_range=\u001b[32m15\u001b[39m,\n\u001b[32m      9\u001b[39m     width_shift_range=\u001b[32m0.1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     vertical_flip=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     13\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mdatagen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create a validation set (without stratification)\u001b[39;00m\n\u001b[32m     17\u001b[39m X_train, X_valid, y_train, y_valid = train_test_split(\n\u001b[32m     18\u001b[39m     X_train, y_train, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1452\u001b[39m, in \u001b[36mImageDataGenerator.fit\u001b[39m\u001b[34m(self, x, augment, rounds, seed)\u001b[39m\n\u001b[32m   1450\u001b[39m x = np.asarray(x, dtype=\u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim != \u001b[32m4\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1452\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1453\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInput to `.fit()` should have rank 4. Got array with shape: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1454\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(x.shape)\n\u001b[32m   1455\u001b[39m     )\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape[\u001b[38;5;28mself\u001b[39m.channel_axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m}:\n\u001b[32m   1457\u001b[39m     warnings.warn(\n\u001b[32m   1458\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected input to be images (as Numpy array) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1459\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mfollowing the data format convention \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1469\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33m channels).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1470\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Input to `.fit()` should have rank 4. Got array with shape: (32, 288, 288)"
     ]
    }
   ],
   "source": [
    "# One-hot encode labels with the correct number of classes\n",
    "num_classes = len(label_mapping)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Create a validation set (without stratification)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Reshape data to add channel dimension\n",
    "X_train = X_train.reshape(X_train.shape[0], 288, 288, 1)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], 288, 288, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 288, 288, 1)\n",
    "\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "print(X_valid.shape[0], \"validation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd20bb-a4f0-4593-98f0-c191d3f28d1e",
   "metadata": {},
   "source": [
    "## Build network\n",
    "With the data formatted and separated, we can now build the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b9a6d-8c7c-4d47-b8e1-625e828ae551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# First, let's check how many unique classes you actually have\n",
    "num_classes = len(np.unique(labels))\n",
    "print(f\"Number of unique classes: {num_classes}\")\n",
    "\n",
    "# Make sure your final Dense layer matches this number\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=(288, 288, 1)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "# Change this line to match your actual number of classes (6)\n",
    "model.add(Dense(6, activation='softmax'))  # Change from 10 to 6\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f63cf2-052e-4eea-b951-c690d6219243",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a6daa-ee89-4170-83f4-d4379ef0978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5.keras', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=5, epochs=100, validation_data=(X_valid,y_valid), callbacks=[checkpointer], verbose=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac9b50-bb7a-4aee-a69f-16a16fd54457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reverse mapping to get original labels\n",
    "reverse_mapping = {i: label for label, i in label_mapping.items()}\n",
    "\n",
    "# Load the best weights\n",
    "model.load_weights('model.weights.best.hdf5.keras')\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Create a comprehensive evaluation table that includes all classes\n",
    "all_classes = list(range(num_classes))\n",
    "all_class_names = [reverse_mapping[i] for i in all_classes]\n",
    "\n",
    "# Create a comprehensive DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Class Index': all_classes,\n",
    "    'Original Label': all_class_names,\n",
    "    'In Test Set': [i in y_true for i in all_classes],\n",
    "    'In Predictions': [i in y_pred for i in all_classes]\n",
    "})\n",
    "\n",
    "# Add metrics where applicable\n",
    "precision_values = []\n",
    "recall_values = []\n",
    "f1_values = []\n",
    "\n",
    "for cls in all_classes:\n",
    "    if cls in y_true and cls in y_pred:\n",
    "        # We can calculate metrics for this class\n",
    "        true_binary = (y_true == cls).astype(int)\n",
    "        pred_binary = (y_pred == cls).astype(int)\n",
    "        precision_values.append(precision_score(true_binary, pred_binary, zero_division=0))\n",
    "        recall_values.append(recall_score(true_binary, pred_binary, zero_division=0))\n",
    "        f1_values.append(f1_score(true_binary, pred_binary, zero_division=0))\n",
    "    else:\n",
    "        # Class not present in test set or predictions\n",
    "        precision_values.append(float('nan'))\n",
    "        recall_values.append(float('nan'))\n",
    "        f1_values.append(float('nan'))\n",
    "\n",
    "results_df['Precision'] = precision_values\n",
    "results_df['Recall'] = recall_values\n",
    "results_df['F1 Score'] = f1_values\n",
    "\n",
    "print(\"Comprehensive Class Evaluation:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Create a confusion matrix (will only show classes present in test set)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "present_classes = sorted(set(np.concatenate([y_true, y_pred])))\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=[f\"{reverse_mapping[i]}\" for i in present_classes],\n",
    "           yticklabels=[f\"{reverse_mapping[i]}\" for i in present_classes])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Only Classes Present in Test Set)')\n",
    "plt.show()\n",
    "\n",
    "# Show overall accuracy\n",
    "accuracy = np.mean(y_pred == y_true)\n",
    "print(f\"\\nOverall Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d1a4d-6096-4720-a0c6-7ccfaf03d613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
