{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a9ce06-de99-4d13-89f5-02a509fd73c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 2060\n",
      "GPU memory: 6.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA recognized\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"âŒ CUDA not detected by PyTorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6bf38a0-b457-4061-9fac-34fdcdb68261",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "GPU Memory: 6.0 GB\n",
      "Mixed Precision: Enabled\n",
      "Models will be saved to: deer_age_models_20250731_072915\n",
      "Loading images...\n",
      "Images loaded with shape: (466, 224, 224, 3)\n",
      "Classes: [0 1 2 3 4] (0=1.5yr, 1=2.5yr, 2=3.5yr, 3=4.5yr, 4=5.5yr)\n",
      "Class distribution: Counter({np.int64(4): 118, np.int64(2): 111, np.int64(3): 89, np.int64(1): 82, np.int64(0): 66})\n",
      "Loaded 466 images, age range: 0.0-4.0\n",
      "Train: 372, Test: 94\n",
      "\n",
      "Testing 6 model families...\n",
      "\n",
      "Testing ResNet50...\n",
      "Training ResNet50 on cuda\n",
      "  Epoch  0: Train 22.6%, Test 25.5%\n",
      "  Epoch 10: Train 100.0%, Test 47.9%\n",
      "  Epoch 19: Train 100.0%, Test 50.0%\n",
      "  Early stopping at epoch 19\n",
      "ResNet50: Accuracy=51.064, Saved: ResNet50_51p064.pth\n",
      "\n",
      "Testing EfficientNetB0...\n",
      "Training EfficientNetB0 on cuda\n",
      "  Epoch  0: Train 29.6%, Test 35.1%\n",
      "  Epoch 10: Train 100.0%, Test 50.0%\n",
      "  Epoch 18: Train 100.0%, Test 55.3%\n",
      "  Early stopping at epoch 18\n",
      "EfficientNetB0: Accuracy=55.319, Saved: EfficientNetB0_55p319.pth\n",
      "\n",
      "Testing VGG16...\n",
      "Training VGG16 on cuda\n",
      "  Epoch  0: Train 21.0%, Test 24.5%\n",
      "  Epoch 10: Train 24.2%, Test 21.3%\n",
      "  Epoch 20: Train 24.2%, Test 25.5%\n",
      "  Epoch 21: Train 24.7%, Test 19.1%\n",
      "  Early stopping at epoch 21\n",
      "VGG16: Accuracy=29.787, Saved: VGG16_29p787.pth\n",
      "\n",
      "Testing MobileNetV2...\n",
      "Training MobileNetV2 on cuda\n",
      "  Epoch  0: Train 29.3%, Test 29.8%\n",
      "  Epoch 10: Train 99.7%, Test 45.7%\n",
      "  Epoch 20: Train 100.0%, Test 47.9%\n",
      "  Epoch 26: Train 99.7%, Test 47.9%\n",
      "  Early stopping at epoch 26\n",
      "MobileNetV2: Accuracy=54.255, Saved: MobileNetV2_54p255.pth\n",
      "\n",
      "Testing InceptionV3...\n",
      "Training InceptionV3 on cuda\n",
      "  Epoch  0: Train 23.7%, Test 21.3%\n",
      "  Epoch 10: Train 92.5%, Test 39.4%\n",
      "  Epoch 20: Train 100.0%, Test 44.7%\n",
      "  Epoch 28: Train 99.7%, Test 43.6%\n",
      "  Early stopping at epoch 28\n",
      "InceptionV3: Accuracy=47.872, Saved: InceptionV3_47p872.pth\n",
      "\n",
      "Testing DenseNet121...\n",
      "Training DenseNet121 on cuda\n",
      "  Epoch  0: Train 27.4%, Test 24.5%\n",
      "  Epoch 10: Train 100.0%, Test 54.3%\n",
      "  Epoch 19: Train 100.0%, Test 55.3%\n",
      "  Early stopping at epoch 19\n",
      "DenseNet121: Accuracy=56.383, Saved: DenseNet121_56p383.pth\n",
      "\n",
      "Best family: DenseNet121 (accuracy: 56.383)\n",
      "\n",
      "Testing variations of DenseNet121...\n",
      "\n",
      "==================================================\n",
      "FINAL RESULTS\n",
      "==================================================\n",
      " 1. DenseNet121     - Accuracy: 56.383\n",
      " 2. EfficientNetB0  - Accuracy: 55.319\n",
      " 3. MobileNetV2     - Accuracy: 54.255\n",
      " 4. ResNet50        - Accuracy: 51.064\n",
      " 5. InceptionV3     - Accuracy: 47.872\n",
      " 6. VGG16           - Accuracy: 29.787\n",
      "\n",
      "Best model: DenseNet121 with 56.383 accuracy\n",
      "Saved as: DenseNet121_56p383.pth\n",
      "\n",
      "Total models tested: 6\n",
      "All models saved in folder: deer_age_models_20250731_072915\n",
      "All models saved with accuracy in filename.\n"
     ]
    }
   ],
   "source": [
    "# Broad model family search\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mixed precision imports (from your reference code)\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    MIXED_PRECISION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXED_PRECISION_AVAILABLE = False\n",
    "    class autocast:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "# GPU Configuration (matching your reference code)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    if MIXED_PRECISION_AVAILABLE:\n",
    "        scaler = GradScaler()\n",
    "        use_amp = True\n",
    "        print(\"Mixed Precision: Enabled\")\n",
    "    else:\n",
    "        use_amp = False\n",
    "        print(\"Mixed Precision: Disabled\")\n",
    "else:\n",
    "    use_amp = False\n",
    "    print(\"WARNING: GPU not available\")\n",
    "\n",
    "# Data paths\n",
    "color_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\"\n",
    "grayscale_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\grayscale\"\n",
    "\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        age_str = parts[3]\n",
    "        try:\n",
    "            age = float(age_str.replace('p', '.'))\n",
    "            # Cap ages over 5.5 to 5.5\n",
    "            if age > 5.5:\n",
    "                age = 5.5\n",
    "            return age\n",
    "        except ValueError:\n",
    "            # Skip files with non-numeric age (e.g., \"xpx\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def age_to_class(age):\n",
    "    age_mapping = {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
    "    return age_mapping.get(age, None)\n",
    "\n",
    "def load_images(color_path, grayscale_path, img_size=(224, 224)):\n",
    "    images = []\n",
    "    ages = []\n",
    "    \n",
    "    # Process color images (convert to grayscale)\n",
    "    if os.path.exists(color_path):\n",
    "        for filename in os.listdir(color_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(color_path, filename)\n",
    "                        img = cv2.imread(img_path)\n",
    "                        if img is not None:\n",
    "                            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                            img_resized = cv2.resize(img_gray, img_size)\n",
    "                            assert img_resized.shape == img_size, f\"Image {filename} not resized correctly: {img_resized.shape}\"\n",
    "                            # Convert to 3-channel for pretrained models\n",
    "                            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                            images.append(img_rgb)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    # Process grayscale images\n",
    "    if os.path.exists(grayscale_path):\n",
    "        for filename in os.listdir(grayscale_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(grayscale_path, filename)\n",
    "                        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                        if img is not None:\n",
    "                            img_resized = cv2.resize(img, img_size)\n",
    "                            assert img_resized.shape == img_size, f\"Image {filename} not resized correctly: {img_resized.shape}\"\n",
    "                            # Convert to 3-channel for pretrained models\n",
    "                            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                            images.append(img_rgb)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    ages = np.array(ages)\n",
    "    \n",
    "    # Verify final dimensions\n",
    "    assert images.shape[1:3] == img_size, f\"Final image dimensions incorrect: {images.shape}\"\n",
    "    print(f\"Images loaded with shape: {images.shape}\")\n",
    "    print(f\"Classes: {np.unique(ages)} (0=1.5yr, 1=2.5yr, 2=3.5yr, 3=4.5yr, 4=5.5yr)\")\n",
    "    print(f\"Class distribution: {Counter(ages)}\")\n",
    "    \n",
    "    return images, ages\n",
    "\n",
    "class DeerDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].clone()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Normalize to [0,1]\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Convert to CHW format\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Normalize with ImageNet stats\n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def create_model(model_name, num_classes=5):\n",
    "    \"\"\"Create model using timm (matching your reference code)\"\"\"\n",
    "    if model_name == 'ResNet50':\n",
    "        model = timm.create_model('resnet50', pretrained=True, num_classes=num_classes)\n",
    "    elif model_name == 'EfficientNetB0':\n",
    "        model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=num_classes)\n",
    "    elif model_name == 'VGG16':\n",
    "        model = timm.create_model('vgg16', pretrained=True, num_classes=num_classes)\n",
    "    elif model_name == 'MobileNetV2':\n",
    "        model = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=num_classes)\n",
    "    elif model_name == 'InceptionV3':\n",
    "        model = timm.create_model('inception_v3', pretrained=True, num_classes=num_classes)\n",
    "    elif model_name == 'DenseNet121':\n",
    "        model = timm.create_model('densenet121', pretrained=True, num_classes=num_classes)\n",
    "    elif model_name == 'ResNet101':\n",
    "        model = timm.create_model('resnet101', pretrained=True, num_classes=num_classes)\n",
    "    elif model_name == 'ResNet152':\n",
    "        model = timm.create_model('resnet152', pretrained=True, num_classes=num_classes)\n",
    "    elif model_name == 'EfficientNetB1':\n",
    "        model = timm.create_model('efficientnet_b1', pretrained=True, num_classes=num_classes)\n",
    "    elif model_name == 'EfficientNetB2':\n",
    "        model = timm.create_model('efficientnet_b2', pretrained=True, num_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def train_model(model, train_loader, test_loader, model_name, epochs=50):\n",
    "    \"\"\"Train model with your proven GPU configuration\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, min_lr=1e-6)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    print(f\"Training {model_name} on {device}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_loss_total = 0.0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_loss_total += loss.item()\n",
    "            \n",
    "            # Memory management (from your reference code)\n",
    "            if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        scheduler.step(test_acc)\n",
    "        \n",
    "        # Early stopping\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            patience_counter = 0\n",
    "            best_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 10 == 0 or patience_counter >= patience:\n",
    "            print(f\"  Epoch {epoch:2d}: Train {train_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"  Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, best_acc\n",
    "\n",
    "# Create timestamped output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"deer_age_models_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Models will be saved to: {output_dir}\")\n",
    "\n",
    "# Load data\n",
    "print(\"Loading images...\")\n",
    "X, y = load_images(color_path, grayscale_path)\n",
    "print(f\"Loaded {len(X)} images, age range: {y.min():.1f}-{y.max():.1f}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DeerDataset(X_train, y_train)\n",
    "test_dataset = DeerDataset(X_test, y_test)\n",
    "\n",
    "# Create dataloaders (using your batch size from reference code)\n",
    "batch_size = 16  # From your reference code for RTX 2060\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Model families to test\n",
    "model_families = [\n",
    "    'ResNet50', 'EfficientNetB0', 'VGG16', \n",
    "    'MobileNetV2', 'InceptionV3', 'DenseNet121'\n",
    "]\n",
    "\n",
    "results = []\n",
    "best_accuracy = 0\n",
    "best_family = None\n",
    "\n",
    "print(f\"\\nTesting {len(model_families)} model families...\")\n",
    "\n",
    "for model_name in model_families:\n",
    "    print(f\"\\nTesting {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        model = create_model(model_name)\n",
    "        trained_model, accuracy = train_model(\n",
    "            model, train_loader, test_loader, model_name\n",
    "        )\n",
    "        \n",
    "        # Save model with accuracy in filename\n",
    "        acc_str = f\"{accuracy:.3f}\".replace('.', 'p')\n",
    "        model_filename = f\"{model_name}_{acc_str}.pth\"\n",
    "        model_path = os.path.join(output_dir, model_filename)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': trained_model.state_dict(),\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'num_classes': 5\n",
    "        }, model_path)\n",
    "        \n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'filename': model_filename,\n",
    "            'full_path': model_path\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name}: Accuracy={accuracy:.3f}, Saved: {model_filename}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_family = model_name\n",
    "        \n",
    "        # Cleanup (from your reference code)\n",
    "        del model, trained_model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Find best performing family\n",
    "print(f\"\\nBest family: {best_family} (accuracy: {best_accuracy:.3f})\")\n",
    "\n",
    "# Test variations within best family\n",
    "if best_family:\n",
    "    print(f\"\\nTesting variations of {best_family}...\")\n",
    "    \n",
    "    if best_family == 'ResNet50':\n",
    "        variations = ['ResNet101', 'ResNet152']\n",
    "    elif best_family == 'EfficientNetB0':\n",
    "        variations = ['EfficientNetB1', 'EfficientNetB2']\n",
    "    else:\n",
    "        variations = []\n",
    "    \n",
    "    for var_name in variations:\n",
    "        print(f\"\\nTesting {var_name}...\")\n",
    "        \n",
    "        try:\n",
    "            model = create_model(var_name)\n",
    "            trained_model, accuracy = train_model(\n",
    "                model, train_loader, test_loader, var_name\n",
    "            )\n",
    "            \n",
    "            # Save model\n",
    "            acc_str = f\"{accuracy:.3f}\".replace('.', 'p')\n",
    "            model_filename = f\"{var_name}_{acc_str}.pth\"\n",
    "            model_path = os.path.join(output_dir, model_filename)\n",
    "            \n",
    "            torch.save({\n",
    "                'model_state_dict': trained_model.state_dict(),\n",
    "                'model_name': var_name,\n",
    "                'accuracy': accuracy,\n",
    "                'num_classes': 5\n",
    "            }, model_path)\n",
    "            \n",
    "            results.append({\n",
    "                'model': var_name,\n",
    "                'accuracy': accuracy,\n",
    "                'filename': model_filename,\n",
    "                'full_path': model_path\n",
    "            })\n",
    "            \n",
    "            print(f\"{var_name}: Accuracy={accuracy:.3f}, Saved: {model_filename}\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del model, trained_model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {var_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Final results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"FINAL RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i:2d}. {result['model']:15s} - Accuracy: {result['accuracy']:.3f}\")\n",
    "\n",
    "if results:\n",
    "    best_result = results[0]\n",
    "    print(f\"\\nBest model: {best_result['model']} with {best_result['accuracy']:.3f} accuracy\")\n",
    "    print(f\"Saved as: {best_result['filename']}\")\n",
    "\n",
    "print(f\"\\nTotal models tested: {len(results)}\")\n",
    "print(f\"All models saved in folder: {output_dir}\")\n",
    "print(\"All models saved with accuracy in filename.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd175bbe-545a-4fb8-8e68-fbd05ab1f5d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "GPU Memory: 6.0 GB\n",
      "Mixed Precision: Enabled\n",
      "Models will be saved to: deer_age_deep_survey_20250731_075454\n",
      "Loading images...\n",
      "Images loaded with shape: (466, 224, 224, 3)\n",
      "Class distribution: Counter({np.int64(4): 118, np.int64(2): 111, np.int64(3): 89, np.int64(1): 82, np.int64(0): 66})\n",
      "Loaded 466 images\n",
      "Train: 372, Test: 94\n",
      "\n",
      "Deep survey: Testing 12 models with improved regularization...\n",
      "\n",
      "Testing DenseNet121...\n",
      "Training DenseNet121 with improved regularization\n",
      "  Epoch  0: Train 25.5%, Test 30.9%\n",
      "  Epoch 10: Train 85.8%, Test 56.4%\n",
      "  Epoch 20: Train 91.1%, Test 46.8%\n",
      "  Epoch 25: Train 95.7%, Test 47.9%\n",
      "  Early stopping at epoch 25\n",
      "DenseNet121: 56.383% - Saved: DenseNet121_56p383.pth\n",
      "\n",
      "Testing DenseNet169...\n",
      "Training DenseNet169 with improved regularization\n",
      "  Epoch  0: Train 27.7%, Test 34.0%\n",
      "  Epoch 10: Train 90.3%, Test 52.1%\n",
      "  Epoch 20: Train 95.7%, Test 60.6%\n",
      "  Epoch 30: Train 98.9%, Test 60.6%\n",
      "  Epoch 40: Train 99.7%, Test 63.8%\n",
      "  Epoch 46: Train 99.5%, Test 61.7%\n",
      "  Early stopping at epoch 46\n",
      "DenseNet169: 63.830% - Saved: DenseNet169_63p830.pth\n",
      "\n",
      "Testing DenseNet201...\n",
      "Training DenseNet201 with improved regularization\n",
      "  Epoch  0: Train 24.7%, Test 34.0%\n",
      "  Epoch 10: Train 93.3%, Test 50.0%\n",
      "  Epoch 20: Train 98.1%, Test 50.0%\n",
      "  Epoch 27: Train 98.7%, Test 56.4%\n",
      "  Early stopping at epoch 27\n",
      "DenseNet201: 59.574% - Saved: DenseNet201_59p574.pth\n",
      "\n",
      "Testing EfficientNetB0...\n",
      "Training EfficientNetB0 with improved regularization\n",
      "  Epoch  0: Train 22.8%, Test 16.0%\n",
      "  Epoch 10: Train 63.2%, Test 42.6%\n",
      "  Epoch 20: Train 67.2%, Test 46.8%\n",
      "  Epoch 30: Train 77.4%, Test 46.8%\n",
      "  Epoch 40: Train 79.6%, Test 48.9%\n",
      "  Epoch 50: Train 81.2%, Test 46.8%\n",
      "  Early stopping at epoch 50\n",
      "EfficientNetB0: 48.936% - Saved: EfficientNetB0_48p936.pth\n",
      "\n",
      "Testing EfficientNetB1...\n",
      "Training EfficientNetB1 with improved regularization\n",
      "  Epoch  0: Train 24.2%, Test 24.5%\n",
      "  Epoch 10: Train 67.7%, Test 34.0%\n",
      "  Epoch 20: Train 84.1%, Test 45.7%\n",
      "  Early stopping at epoch 20\n",
      "EfficientNetB1: 48.936% - Saved: EfficientNetB1_48p936.pth\n",
      "\n",
      "Testing EfficientNetB2...\n",
      "Training EfficientNetB2 with improved regularization\n",
      "  Epoch  0: Train 22.8%, Test 29.8%\n",
      "  Epoch 10: Train 67.7%, Test 42.6%\n",
      "  Epoch 20: Train 80.6%, Test 43.6%\n",
      "  Epoch 30: Train 83.3%, Test 46.8%\n",
      "  Epoch 36: Train 85.5%, Test 48.9%\n",
      "  Early stopping at epoch 36\n",
      "EfficientNetB2: 52.128% - Saved: EfficientNetB2_52p128.pth\n",
      "\n",
      "Testing EfficientNetB3...\n",
      "Training EfficientNetB3 with improved regularization\n",
      "  Epoch  0: Train 19.9%, Test 28.7%\n",
      "  Epoch 10: Train 59.7%, Test 42.6%\n",
      "  Epoch 20: Train 77.2%, Test 45.7%\n",
      "  Epoch 28: Train 83.9%, Test 50.0%\n",
      "  Early stopping at epoch 28\n",
      "EfficientNetB3: 50.000% - Saved: EfficientNetB3_50p000.pth\n",
      "\n",
      "Testing MobileNetV2...\n",
      "Training MobileNetV2 with improved regularization\n",
      "  Epoch  0: Train 22.6%, Test 19.1%\n",
      "  Epoch 10: Train 22.3%, Test 22.3%\n",
      "  Epoch 20: Train 22.3%, Test 21.3%\n",
      "  Epoch 30: Train 20.7%, Test 24.5%\n",
      "  Epoch 40: Train 25.5%, Test 26.6%\n",
      "  Epoch 41: Train 23.7%, Test 24.5%\n",
      "  Early stopping at epoch 41\n",
      "MobileNetV2: 32.979% - Saved: MobileNetV2_32p979.pth\n",
      "\n",
      "Testing MobileNetV3Small...\n",
      "Training MobileNetV3Small with improved regularization\n",
      "  Epoch  0: Train 20.4%, Test 19.1%\n",
      "  Epoch 10: Train 26.1%, Test 18.1%\n",
      "  Epoch 20: Train 26.6%, Test 25.5%\n",
      "  Epoch 30: Train 24.7%, Test 25.5%\n",
      "  Epoch 40: Train 26.9%, Test 25.5%\n",
      "  Epoch 50: Train 26.3%, Test 26.6%\n",
      "  Epoch 54: Train 24.5%, Test 24.5%\n",
      "  Early stopping at epoch 54\n",
      "MobileNetV3Small: 28.723% - Saved: MobileNetV3Small_28p723.pth\n",
      "\n",
      "Testing MobileNetV3Large...\n",
      "Training MobileNetV3Large with improved regularization\n",
      "  Epoch  0: Train 20.2%, Test 16.0%\n",
      "  Epoch 10: Train 24.7%, Test 21.3%\n",
      "  Epoch 20: Train 23.9%, Test 20.2%\n",
      "  Epoch 27: Train 23.4%, Test 23.4%\n",
      "  Early stopping at epoch 27\n",
      "MobileNetV3Large: 26.596% - Saved: MobileNetV3Large_26p596.pth\n",
      "\n",
      "Testing ResNet50_Regularized...\n",
      "Training ResNet50_Regularized with improved regularization\n",
      "  Epoch  0: Train 21.8%, Test 22.3%\n",
      "  Epoch 10: Train 64.8%, Test 40.4%\n",
      "  Epoch 20: Train 86.6%, Test 47.9%\n",
      "  Epoch 30: Train 92.5%, Test 50.0%\n",
      "  Epoch 40: Train 92.5%, Test 50.0%\n",
      "  Epoch 50: Train 94.4%, Test 53.2%\n",
      "ResNet50_Regularized: 55.319% - Saved: ResNet50_Regularized_55p319.pth\n",
      "\n",
      "Testing ResNeXt50...\n",
      "Training ResNeXt50 with improved regularization\n",
      "  Epoch  0: Train 20.4%, Test 24.5%\n",
      "  Epoch 10: Train 83.6%, Test 54.3%\n",
      "  Epoch 20: Train 98.9%, Test 57.4%\n",
      "  Epoch 30: Train 99.7%, Test 60.6%\n",
      "  Epoch 40: Train 100.0%, Test 60.6%\n",
      "  Epoch 50: Train 99.7%, Test 60.6%\n",
      "  Early stopping at epoch 50\n",
      "ResNeXt50: 63.830% - Saved: ResNeXt50_63p830.pth\n",
      "\n",
      "============================================================\n",
      "DEEP SURVEY RESULTS - TOP 3 FAMILIES + EXTRAS\n",
      "============================================================\n",
      "DENSENET FAMILY:\n",
      "  DenseNet169          - 63.830%\n",
      "  DenseNet201          - 59.574%\n",
      "  DenseNet121          - 56.383%\n",
      "\n",
      "EFFICIENTNET FAMILY:\n",
      "  EfficientNetB2       - 52.128%\n",
      "  EfficientNetB3       - 50.000%\n",
      "  EfficientNetB0       - 48.936%\n",
      "  EfficientNetB1       - 48.936%\n",
      "\n",
      "MOBILENET FAMILY:\n",
      "  MobileNetV2          - 32.979%\n",
      "  MobileNetV3Small     - 28.723%\n",
      "  MobileNetV3Large     - 26.596%\n",
      "\n",
      "OTHER MODELS:\n",
      "  ResNeXt50            - 63.830%\n",
      "  ResNet50_Regularized - 55.319%\n",
      "\n",
      "============================================================\n",
      "OVERALL RANKING:\n",
      " 1. DenseNet169          - 63.830%\n",
      " 2. ResNeXt50            - 63.830%\n",
      " 3. DenseNet201          - 59.574%\n",
      " 4. DenseNet121          - 56.383%\n",
      " 5. ResNet50_Regularized - 55.319%\n",
      " 6. EfficientNetB2       - 52.128%\n",
      " 7. EfficientNetB3       - 50.000%\n",
      " 8. EfficientNetB0       - 48.936%\n",
      " 9. EfficientNetB1       - 48.936%\n",
      "10. MobileNetV2          - 32.979%\n",
      "11. MobileNetV3Small     - 28.723%\n",
      "12. MobileNetV3Large     - 26.596%\n",
      "\n",
      "BEST MODEL: DenseNet169 - 63.830%\n",
      "Saved as: DenseNet169_63p830.pth\n",
      "\n",
      "Total models tested: 12\n",
      "All models saved in: deer_age_deep_survey_20250731_075454\n",
      "Note: Improved regularization should reduce train/test accuracy gap\n"
     ]
    }
   ],
   "source": [
    "# Deeper family search.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    MIXED_PRECISION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXED_PRECISION_AVAILABLE = False\n",
    "    class autocast:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "# GPU Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    if MIXED_PRECISION_AVAILABLE:\n",
    "        scaler = GradScaler()\n",
    "        use_amp = True\n",
    "        print(\"Mixed Precision: Enabled\")\n",
    "    else:\n",
    "        use_amp = False\n",
    "else:\n",
    "    use_amp = False\n",
    "\n",
    "# Data paths\n",
    "color_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\"\n",
    "grayscale_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\grayscale\"\n",
    "\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        age_str = parts[3]\n",
    "        try:\n",
    "            age = float(age_str.replace('p', '.'))\n",
    "            if age > 5.5:\n",
    "                age = 5.5\n",
    "            return age\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def age_to_class(age):\n",
    "    age_mapping = {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
    "    return age_mapping.get(age, None)\n",
    "\n",
    "def augment_image(image):\n",
    "    \"\"\"Enhanced augmentation to reduce overfitting\"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    if random.random() < 0.7:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < 0.8:\n",
    "        alpha = random.uniform(0.7, 1.3)\n",
    "        beta = random.randint(-25, 25)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    if random.random() < 0.3:\n",
    "        noise = np.random.normal(0, 8, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def load_images(color_path, grayscale_path, img_size=(224, 224)):\n",
    "    images = []\n",
    "    ages = []\n",
    "    \n",
    "    if os.path.exists(color_path):\n",
    "        for filename in os.listdir(color_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(color_path, filename)\n",
    "                        img = cv2.imread(img_path)\n",
    "                        if img is not None:\n",
    "                            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                            img_resized = cv2.resize(img_gray, img_size)\n",
    "                            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                            images.append(img_rgb)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    if os.path.exists(grayscale_path):\n",
    "        for filename in os.listdir(grayscale_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(grayscale_path, filename)\n",
    "                        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                        if img is not None:\n",
    "                            img_resized = cv2.resize(img, img_size)\n",
    "                            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                            images.append(img_rgb)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    ages = np.array(ages)\n",
    "    \n",
    "    print(f\"Images loaded with shape: {images.shape}\")\n",
    "    print(f\"Class distribution: {Counter(ages)}\")\n",
    "    \n",
    "    return images, ages\n",
    "\n",
    "class AugmentedDeerDataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False):\n",
    "        self.X = X\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.augment = augment\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].copy()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Apply augmentation during training\n",
    "        if self.augment:\n",
    "            image = augment_image(image)\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        image = torch.FloatTensor(image)\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def create_model_with_regularization(model_name, num_classes=5, dropout_rate=0.5):\n",
    "    \"\"\"Create model with better regularization\"\"\"\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes, drop_rate=dropout_rate)\n",
    "    \n",
    "    # Freeze more layers to reduce overfitting\n",
    "    if 'resnet' in model_name:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('layer4' in name or 'fc' in name):\n",
    "                param.requires_grad = False\n",
    "    elif 'efficientnet' in model_name:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('blocks.6' in name or 'blocks.7' in name or 'classifier' in name):\n",
    "                param.requires_grad = False\n",
    "    elif 'densenet' in model_name:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('denseblock4' in name or 'classifier' in name):\n",
    "                param.requires_grad = False\n",
    "    elif 'mobilenet' in model_name:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('features.18' in name or 'features.19' in name or 'classifier' in name):\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def train_model_improved(model, train_loader, test_loader, model_name, epochs=60):\n",
    "    \"\"\"Improved training with better regularization\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    \n",
    "    # Lower learning rate and higher weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    print(f\"Training {model_name} with improved regularization\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        # Early stopping\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            patience_counter = 0\n",
    "            best_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 10 == 0 or patience_counter >= patience:\n",
    "            print(f\"  Epoch {epoch:2d}: Train {train_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"  Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, best_acc\n",
    "\n",
    "# Create timestamped output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"deer_age_deep_survey_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Models will be saved to: {output_dir}\")\n",
    "\n",
    "# Load data\n",
    "print(\"Loading images...\")\n",
    "X, y = load_images(color_path, grayscale_path)\n",
    "print(f\"Loaded {len(X)} images\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "# Create datasets with augmentation\n",
    "train_dataset = AugmentedDeerDataset(X_train, y_train, augment=True)\n",
    "test_dataset = AugmentedDeerDataset(X_test, y_test, augment=False)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Deep exploration of top 3 families\n",
    "model_configs = [\n",
    "    # DenseNet family (won previous round)\n",
    "    ('densenet121', 'DenseNet121'),\n",
    "    ('densenet169', 'DenseNet169'),\n",
    "    ('densenet201', 'DenseNet201'),\n",
    "    \n",
    "    # EfficientNet family (2nd place)\n",
    "    ('efficientnet_b0', 'EfficientNetB0'),\n",
    "    ('efficientnet_b1', 'EfficientNetB1'),\n",
    "    ('efficientnet_b2', 'EfficientNetB2'),\n",
    "    ('efficientnet_b3', 'EfficientNetB3'),\n",
    "    \n",
    "    # MobileNet family (3rd place)\n",
    "    ('mobilenetv2_100', 'MobileNetV2'),\n",
    "    ('mobilenetv3_small_100', 'MobileNetV3Small'),\n",
    "    ('mobilenetv3_large_100', 'MobileNetV3Large'),\n",
    "    \n",
    "    # Additional high-performers to test\n",
    "    ('resnet50', 'ResNet50_Regularized'),\n",
    "    ('resnext50_32x4d', 'ResNeXt50'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(f\"\\nDeep survey: Testing {len(model_configs)} models with improved regularization...\")\n",
    "\n",
    "for model_timm_name, display_name in model_configs:\n",
    "    print(f\"\\nTesting {display_name}...\")\n",
    "    \n",
    "    try:\n",
    "        model = create_model_with_regularization(model_timm_name, dropout_rate=0.5)\n",
    "        trained_model, accuracy = train_model_improved(\n",
    "            model, train_loader, test_loader, display_name\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        acc_str = f\"{accuracy:.3f}\".replace('.', 'p')\n",
    "        model_filename = f\"{display_name}_{acc_str}.pth\"\n",
    "        model_path = os.path.join(output_dir, model_filename)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': trained_model.state_dict(),\n",
    "            'model_name': display_name,\n",
    "            'timm_name': model_timm_name,\n",
    "            'accuracy': accuracy,\n",
    "            'num_classes': 5\n",
    "        }, model_path)\n",
    "        \n",
    "        results.append({\n",
    "            'model': display_name,\n",
    "            'timm_name': model_timm_name,\n",
    "            'accuracy': accuracy,\n",
    "            'filename': model_filename,\n",
    "            'full_path': model_path\n",
    "        })\n",
    "        \n",
    "        print(f\"{display_name}: {accuracy:.3f}% - Saved: {model_filename}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, trained_model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {display_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Final results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DEEP SURVEY RESULTS - TOP 3 FAMILIES + EXTRAS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "print(\"DENSENET FAMILY:\")\n",
    "for result in results:\n",
    "    if 'DenseNet' in result['model']:\n",
    "        print(f\"  {result['model']:20s} - {result['accuracy']:.3f}%\")\n",
    "\n",
    "print(\"\\nEFFICIENTNET FAMILY:\")\n",
    "for result in results:\n",
    "    if 'EfficientNet' in result['model']:\n",
    "        print(f\"  {result['model']:20s} - {result['accuracy']:.3f}%\")\n",
    "\n",
    "print(\"\\nMOBILENET FAMILY:\")\n",
    "for result in results:\n",
    "    if 'MobileNet' in result['model']:\n",
    "        print(f\"  {result['model']:20s} - {result['accuracy']:.3f}%\")\n",
    "\n",
    "print(\"\\nOTHER MODELS:\")\n",
    "for result in results:\n",
    "    if not any(family in result['model'] for family in ['DenseNet', 'EfficientNet', 'MobileNet']):\n",
    "        print(f\"  {result['model']:20s} - {result['accuracy']:.3f}%\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OVERALL RANKING:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i:2d}. {result['model']:20s} - {result['accuracy']:.3f}%\")\n",
    "\n",
    "if results:\n",
    "    best_result = results[0]\n",
    "    print(f\"\\nBEST MODEL: {best_result['model']} - {best_result['accuracy']:.3f}%\")\n",
    "    print(f\"Saved as: {best_result['filename']}\")\n",
    "\n",
    "print(f\"\\nTotal models tested: {len(results)}\")\n",
    "print(f\"All models saved in: {output_dir}\")\n",
    "print(\"Note: Improved regularization should reduce train/test accuracy gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59e44bc-dc57-4f40-b884-3bcc192bbfd1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Mixed Precision: Enabled\n",
      "Ensemble models saved to: deer_age_ensemble_20250731_220115\n",
      "Loading images...\n",
      "Total images: 466\n",
      "Class distribution: Counter({np.int64(4): 118, np.int64(2): 111, np.int64(3): 89, np.int64(1): 82, np.int64(0): 66})\n",
      "Train: 372, Test: 94\n",
      "\n",
      "============================================================\n",
      "SMALL DATA STRATEGY: MIXUP + MULTI-SCALE + ENSEMBLE\n",
      "============================================================\n",
      "Approach: Conservative training + Mixup synthetic data\n",
      "\n",
      "========================================\n",
      "Training DenseNet169\n",
      "========================================\n",
      "Training DenseNet169 with Mixup + Multi-scale\n",
      "  Epoch   0: Train 29.8%, Test+TTA 37.2%\n",
      "  Epoch  20: Train 64.2%, Test+TTA 54.3%\n",
      "  Epoch  40: Train 80.1%, Test+TTA 56.4%\n",
      "  Epoch  60: Train 78.5%, Test+TTA 62.8%\n",
      "  Epoch  64: Train 81.5%, Test+TTA 57.4%\n",
      "  Early stopping at epoch 64\n",
      "DenseNet169: 64.9% - Saved\n",
      "\n",
      "========================================\n",
      "Training ResNeXt50\n",
      "========================================\n",
      "Training ResNeXt50 with Mixup + Multi-scale\n",
      "  Epoch   0: Train 23.1%, Test+TTA 24.5%\n",
      "  Epoch  20: Train 84.9%, Test+TTA 53.2%\n",
      "  Epoch  40: Train 84.7%, Test+TTA 52.1%\n",
      "  Epoch  58: Train 84.4%, Test+TTA 57.4%\n",
      "  Early stopping at epoch 58\n",
      "ResNeXt50: 57.4% - Saved\n",
      "\n",
      "========================================\n",
      "Training DenseNet201\n",
      "========================================\n",
      "Training DenseNet201 with Mixup + Multi-scale\n",
      "  Epoch   0: Train 28.2%, Test+TTA 36.2%\n",
      "  Epoch  20: Train 81.2%, Test+TTA 52.1%\n",
      "  Epoch  40: Train 82.8%, Test+TTA 54.3%\n",
      "  Epoch  56: Train 82.5%, Test+TTA 56.4%\n",
      "  Early stopping at epoch 56\n",
      "DenseNet201: 60.6% - Saved\n",
      "\n",
      "========================================\n",
      "ENSEMBLE EVALUATION\n",
      "========================================\n",
      "INDIVIDUAL MODEL RESULTS:\n",
      "  DenseNet169: 64.9%\n",
      "  ResNeXt50: 57.4%\n",
      "  DenseNet201: 60.6%\n",
      "\n",
      "ENSEMBLE RESULT: 57.4%\n",
      "Gap to 75%: 17.6%\n",
      "\n",
      "All models saved in: deer_age_ensemble_20250731_220115\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Second attempt at model families \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    MIXED_PRECISION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXED_PRECISION_AVAILABLE = False\n",
    "    class autocast:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    if MIXED_PRECISION_AVAILABLE:\n",
    "        scaler = GradScaler()\n",
    "        use_amp = True\n",
    "        print(\"Mixed Precision: Enabled\")\n",
    "    else:\n",
    "        use_amp = False\n",
    "else:\n",
    "    use_amp = False\n",
    "\n",
    "color_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\"\n",
    "grayscale_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\grayscale\"\n",
    "\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        age_str = parts[3]\n",
    "        try:\n",
    "            age = float(age_str.replace('p', '.'))\n",
    "            if age > 5.5:\n",
    "                age = 5.5\n",
    "            return age\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def age_to_class(age):\n",
    "    age_mapping = {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
    "    return age_mapping.get(age, None)\n",
    "\n",
    "def mixup_data(x, y, alpha=0.4):\n",
    "    \"\"\"Mixup augmentation to create synthetic training examples\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup loss function\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def load_images(color_path, grayscale_path, img_size=(224, 224)):\n",
    "    images = []\n",
    "    ages = []\n",
    "    \n",
    "    if os.path.exists(color_path):\n",
    "        for filename in os.listdir(color_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(color_path, filename)\n",
    "                        img = cv2.imread(img_path)\n",
    "                        if img is not None:\n",
    "                            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                            img_resized = cv2.resize(img_gray, img_size)\n",
    "                            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                            images.append(img_rgb)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    if os.path.exists(grayscale_path):\n",
    "        for filename in os.listdir(grayscale_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(grayscale_path, filename)\n",
    "                        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                        if img is not None:\n",
    "                            img_resized = cv2.resize(img, img_size)\n",
    "                            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                            images.append(img_rgb)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    ages = np.array(ages)\n",
    "    \n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    print(f\"Class distribution: {Counter(ages)}\")\n",
    "    \n",
    "    return images, ages\n",
    "\n",
    "def conservative_augment(image):\n",
    "    \"\"\"Very light augmentation to preserve deer features\"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    if random.random() < 0.3:\n",
    "        angle = random.uniform(-8, 8)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < 0.4:\n",
    "        alpha = random.uniform(0.9, 1.1)\n",
    "        beta = random.randint(-10, 10)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class MultiScaleDataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False, scale_size=224):\n",
    "        self.X = X\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.augment = augment\n",
    "        self.scale_size = scale_size\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].copy()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Multi-scale training\n",
    "        if self.augment:\n",
    "            scale_factor = random.choice([0.8, 0.9, 1.0, 1.1, 1.2])\n",
    "            new_size = int(self.scale_size * scale_factor)\n",
    "            image = cv2.resize(image, (new_size, new_size))\n",
    "            image = cv2.resize(image, (self.scale_size, self.scale_size))\n",
    "            \n",
    "            image = conservative_augment(image)\n",
    "        \n",
    "        image = torch.FloatTensor(image)\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def create_conservative_model(model_name, num_classes=5):\n",
    "    \"\"\"Back to simpler model creation that worked\"\"\"\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes, drop_rate=0.4)\n",
    "    \n",
    "    # Conservative freezing (like the 63.8% model)\n",
    "    if 'densenet' in model_name:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('denseblock4' in name or 'classifier' in name):\n",
    "                param.requires_grad = False\n",
    "    elif 'resnext' in model_name or 'resnet' in model_name:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('layer4' in name or 'fc' in name):\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def train_with_mixup_and_multiscale(model, train_loader, test_loader, model_name, epochs=120):\n",
    "    \"\"\"Training with mixup + multi-scale + very conservative approach\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # Conservative optimizer (back to what worked)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    patience = 30\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    print(f\"Training {model_name} with Mixup + Multi-scale\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training with mixup\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Apply mixup\n",
    "            if random.random() < 0.5:  # 50% chance of mixup\n",
    "                mixed_images, y_a, y_b, lam = mixup_data(images, labels, alpha=0.4)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(mixed_images)\n",
    "                        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(mixed_images)\n",
    "                    loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # For accuracy calculation, use original labels\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == y_a).sum().item()\n",
    "            else:\n",
    "                # Normal training\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Simple TTA evaluation (not too heavy)\n",
    "        test_acc = evaluate_with_simple_tta(model, test_loader)\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            patience_counter = 0\n",
    "            best_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 20 == 0 or patience_counter >= patience:\n",
    "            print(f\"  Epoch {epoch:3d}: Train {train_acc:.1f}%, Test+TTA {test_acc:.1f}%\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"  Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, best_acc\n",
    "\n",
    "def evaluate_with_simple_tta(model, test_loader):\n",
    "    \"\"\"Simple TTA - just 3 versions\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Original prediction\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs1 = model(images)\n",
    "            else:\n",
    "                outputs1 = model(images)\n",
    "            \n",
    "            # Horizontal flip\n",
    "            flipped = torch.flip(images, [3])\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs2 = model(flipped)\n",
    "            else:\n",
    "                outputs2 = model(flipped)\n",
    "            \n",
    "            # Slight zoom\n",
    "            zoomed = F.interpolate(images, scale_factor=0.95, mode='bilinear', align_corners=False)\n",
    "            zoomed = F.interpolate(zoomed, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs3 = model(zoomed)\n",
    "            else:\n",
    "                outputs3 = model(zoomed)\n",
    "            \n",
    "            # Average predictions\n",
    "            avg_outputs = (F.softmax(outputs1, dim=1) + F.softmax(outputs2, dim=1) + F.softmax(outputs3, dim=1)) / 3\n",
    "            _, predicted = torch.max(avg_outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "def ensemble_predict(models, test_loader):\n",
    "    \"\"\"Simple ensemble of multiple models\"\"\"\n",
    "    all_models_eval = [model.eval() for model in models]\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            ensemble_output = torch.zeros(images.size(0), 5).to(device)\n",
    "            \n",
    "            for model in models:\n",
    "                # Simple TTA for each model\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs1 = model(images)\n",
    "                        outputs2 = model(torch.flip(images, [3]))\n",
    "                else:\n",
    "                    outputs1 = model(images)\n",
    "                    outputs2 = model(torch.flip(images, [3]))\n",
    "                \n",
    "                avg_model_output = (F.softmax(outputs1, dim=1) + F.softmax(outputs2, dim=1)) / 2\n",
    "                ensemble_output += avg_model_output\n",
    "            \n",
    "            # Final ensemble prediction\n",
    "            _, predicted = torch.max(ensemble_output, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "# Main execution\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"deer_age_ensemble_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Ensemble models saved to: {output_dir}\")\n",
    "\n",
    "print(\"Loading images...\")\n",
    "X, y = load_images(color_path, grayscale_path)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultiScaleDataset(X_train, y_train, augment=True)\n",
    "test_dataset = MultiScaleDataset(X_test, y_test, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SMALL DATA STRATEGY: MIXUP + MULTI-SCALE + ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "print(\"Approach: Conservative training + Mixup synthetic data\")\n",
    "\n",
    "# Train multiple models for ensemble\n",
    "model_configs = [\n",
    "    ('densenet169', 'DenseNet169'),\n",
    "    ('resnext50_32x4d', 'ResNeXt50'),\n",
    "    ('densenet201', 'DenseNet201'),\n",
    "]\n",
    "\n",
    "trained_models = []\n",
    "individual_scores = []\n",
    "\n",
    "for model_timm_name, display_name in model_configs:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training {display_name}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    try:\n",
    "        model = create_conservative_model(model_timm_name)\n",
    "        trained_model, accuracy = train_with_mixup_and_multiscale(\n",
    "            model, train_loader, test_loader, display_name\n",
    "        )\n",
    "        \n",
    "        # Save individual model\n",
    "        acc_str = f\"{accuracy:.1f}\".replace('.', 'p')\n",
    "        model_filename = f\"{display_name}_{acc_str}pct.pth\"\n",
    "        model_path = os.path.join(output_dir, model_filename)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': trained_model.state_dict(),\n",
    "            'model_name': display_name,\n",
    "            'timm_name': model_timm_name,\n",
    "            'accuracy': accuracy,\n",
    "            'num_classes': 5\n",
    "        }, model_path)\n",
    "        \n",
    "        trained_models.append(trained_model)\n",
    "        individual_scores.append(accuracy)\n",
    "        \n",
    "        print(f\"{display_name}: {accuracy:.1f}% - Saved\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {display_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Ensemble evaluation\n",
    "if len(trained_models) > 1:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"ENSEMBLE EVALUATION\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    ensemble_accuracy = ensemble_predict(trained_models, test_loader)\n",
    "    \n",
    "    print(\"INDIVIDUAL MODEL RESULTS:\")\n",
    "    for i, (score, config) in enumerate(zip(individual_scores, model_configs)):\n",
    "        print(f\"  {config[1]}: {score:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nENSEMBLE RESULT: {ensemble_accuracy:.1f}%\")\n",
    "    \n",
    "    if ensemble_accuracy >= 75.0:\n",
    "        print(\"SUCCESS: 75% target achieved!\")\n",
    "    else:\n",
    "        gap = 75.0 - ensemble_accuracy\n",
    "        print(f\"Gap to 75%: {gap:.1f}%\")\n",
    "        \n",
    "        if ensemble_accuracy > max(individual_scores):\n",
    "            improvement = ensemble_accuracy - max(individual_scores)\n",
    "            print(f\"Ensemble improvement: +{improvement:.1f}%\")\n",
    "\n",
    "print(f\"\\nAll models saved in: {output_dir}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d49ddb-fc22-461c-87fc-db647028a0b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Mixed Precision: Enabled\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 287\n",
      "Comprehensive results saved to: deer_age_comprehensive_20250801_075228\n",
      "Loading images...\n",
      "Total images: 466\n",
      "Class distribution: Counter({np.int64(4): 118, np.int64(2): 111, np.int64(3): 89, np.int64(1): 82, np.int64(0): 66})\n",
      "Train: 372, Test: 94\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: ADVANCED ARCHITECTURE SURVEY\n",
      "================================================================================\n",
      "Added Vision Transformer\n",
      "Added ConvNeXt\n",
      "Added EfficientNetV2\n",
      "\n",
      "--- Testing vit_base ---\n",
      "Self-supervised pretraining...\n",
      "  Pretraining epoch 0: 23.3%\n",
      "  Pretraining epoch 10: 23.1%\n",
      "  Pretraining epoch 20: 24.1%\n",
      "Pretraining complete\n",
      "Training vit_base with ALL techniques\n",
      "  Epoch   0: Train 23.7%, Test+TTA 19.1%\n",
      "  Epoch  25: Train 25.0%, Test+TTA 27.7%\n",
      "  Epoch  50: Train 23.7%, Test+TTA 25.5%\n",
      "  Epoch  61: Train 24.7%, Test+TTA 24.5%\n",
      "  Early stopping at epoch 61\n",
      "vit_base with pretraining: 27.7%\n",
      "\n",
      "--- Testing convnext_tiny ---\n",
      "Self-supervised pretraining...\n",
      "  Pretraining epoch 0: 25.6%\n",
      "  Pretraining epoch 10: 23.2%\n",
      "  Pretraining epoch 20: 24.0%\n",
      "Pretraining complete\n",
      "Training convnext_tiny with ALL techniques\n",
      "  Epoch   0: Train 19.9%, Test+TTA 18.1%\n",
      "  Epoch  25: Train 25.3%, Test+TTA 25.5%\n",
      "  Epoch  42: Train 25.5%, Test+TTA 23.4%\n",
      "  Early stopping at epoch 42\n",
      "convnext_tiny with pretraining: 25.5%\n",
      "\n",
      "--- Testing efficientnet_v2_s ---\n",
      "Self-supervised pretraining...\n",
      "  Pretraining epoch 0: 70.1%\n",
      "  Pretraining epoch 10: 76.6%\n",
      "  Pretraining epoch 20: 76.9%\n",
      "Pretraining complete\n",
      "Training efficientnet_v2_s with ALL techniques\n",
      "  Epoch   0: Train 29.3%, Test+TTA 28.7%\n",
      "  Epoch  25: Train 89.2%, Test+TTA 53.2%\n",
      "  Epoch  50: Train 88.7%, Test+TTA 56.4%\n",
      "  Epoch  58: Train 88.7%, Test+TTA 58.5%\n",
      "  Early stopping at epoch 58\n",
      "efficientnet_v2_s with pretraining: 60.6%\n",
      "\n",
      "--- Testing densenet169 ---\n",
      "Self-supervised pretraining...\n",
      "  Pretraining epoch 0: 67.2%\n",
      "  Pretraining epoch 10: 73.7%\n",
      "  Pretraining epoch 20: 78.0%\n",
      "Pretraining complete\n",
      "Training densenet169 with ALL techniques\n",
      "  Epoch   0: Train 23.9%, Test+TTA 33.0%\n",
      "  Epoch  25: Train 79.6%, Test+TTA 54.3%\n",
      "  Epoch  50: Train 92.5%, Test+TTA 56.4%\n",
      "  Epoch  75: Train 97.3%, Test+TTA 56.4%\n",
      "  Epoch  77: Train 90.3%, Test+TTA 56.4%\n",
      "  Early stopping at epoch 77\n",
      "densenet169 with pretraining: 59.6%\n",
      "\n",
      "--- Testing resnext50 ---\n",
      "Self-supervised pretraining...\n",
      "  Pretraining epoch 0: 67.1%\n",
      "  Pretraining epoch 10: 78.0%\n",
      "  Pretraining epoch 20: 81.2%\n",
      "Pretraining complete\n",
      "Training resnext50 with ALL techniques\n",
      "  Epoch   0: Train 26.1%, Test+TTA 28.7%\n",
      "  Epoch  25: Train 87.4%, Test+TTA 61.7%\n",
      "  Epoch  50: Train 84.7%, Test+TTA 57.4%\n",
      "  Epoch  62: Train 99.7%, Test+TTA 61.7%\n",
      "  Early stopping at epoch 62\n",
      "resnext50 with pretraining: 62.8%\n",
      "\n",
      "--- Testing densenet201 ---\n",
      "Self-supervised pretraining...\n",
      "  Pretraining epoch 0: 65.5%\n",
      "  Pretraining epoch 10: 78.4%\n",
      "  Pretraining epoch 20: 78.2%\n",
      "Pretraining complete\n",
      "Training densenet201 with ALL techniques\n",
      "  Epoch   0: Train 21.2%, Test+TTA 28.7%\n",
      "  Epoch  25: Train 85.2%, Test+TTA 52.1%\n",
      "  Epoch  50: Train 91.4%, Test+TTA 48.9%\n",
      "  Epoch  75: Train 97.0%, Test+TTA 52.1%\n",
      "  Epoch 100: Train 80.9%, Test+TTA 56.4%\n",
      "  Epoch 116: Train 89.5%, Test+TTA 58.5%\n",
      "  Early stopping at epoch 116\n",
      "densenet201 with pretraining: 59.6%\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: PROGRESSIVE TRAINING\n",
      "================================================================================\n",
      "\n",
      "--- Progressive training densenet169 ---\n",
      "Progressive training for densenet169\n",
      "  Progressive epoch 0: 24.5%\n",
      "  Progressive epoch 10: 30.9%\n",
      "  Progressive epoch 20: 33.0%\n",
      "  Progressive epoch 30: 33.0%\n",
      "  Progressive epoch 40: 38.3%\n",
      "densenet169 progressive: 40.4%\n",
      "\n",
      "--- Progressive training resnext50_32x4d ---\n",
      "Progressive training for resnext50_32x4d\n",
      "  Progressive epoch 0: 40.4%\n",
      "  Progressive epoch 10: 58.5%\n",
      "  Progressive epoch 20: 60.6%\n",
      "  Progressive epoch 30: 57.4%\n",
      "  Progressive epoch 40: 59.6%\n",
      "resnext50_32x4d progressive: 66.0%\n",
      "\n",
      "================================================================================\n",
      "PHASE 3: HYBRID CNN + TRADITIONAL ML\n",
      "================================================================================\n",
      "Using resnext50 for feature extraction...\n",
      "Training RandomForest on CNN features...\n",
      "CNN + RandomForest: 23.4%\n",
      "Training XGBoost on CNN features...\n",
      "CNN + XGBoost: 21.3%\n",
      "Training SVM on CNN features...\n",
      "CNN + SVM: 25.5%\n",
      "\n",
      "================================================================================\n",
      "PHASE 4: ORDINAL REGRESSION\n",
      "================================================================================\n",
      "Training ordinal regression model...\n",
      "Error with ordinal regression: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (372,) + inhomogeneous part.\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE RESULTS SUMMARY\n",
      "================================================================================\n",
      "PHASE 1 - ADVANCED ARCHITECTURES + PRETRAINING:\n",
      "  vit_base                  - 27.7%\n",
      "  convnext_tiny             - 25.5%\n",
      "  efficientnet_v2_s         - 60.6%\n",
      "  densenet169               - 59.6%\n",
      "  resnext50                 - 62.8%\n",
      "  densenet201               - 59.6%\n",
      "\n",
      "PHASE 2 - PROGRESSIVE TRAINING:\n",
      "  densenet169_progressive   - 40.4%\n",
      "  resnext50_32x4d_progressive - 66.0%\n",
      "\n",
      "PHASE 3 - HYBRID CNN + TRADITIONAL ML:\n",
      "  CNN+RandomForest          - 23.4%\n",
      "  CNN+XGBoost               - 21.3%\n",
      "  CNN+SVM                   - 25.5%\n",
      "\n",
      "PHASE 4 - ORDINAL REGRESSION:\n",
      "\n",
      "================================================================================\n",
      "OVERALL BEST RESULTS:\n",
      " 1. resnext50_32x4d_progressive - 66.0% - 9.0% to go\n",
      " 2. resnext50                 - 62.8% - 12.2% to go\n",
      " 3. efficientnet_v2_s         - 60.6% - 14.4% to go\n",
      " 4. densenet169               - 59.6% - 15.4% to go\n",
      " 5. densenet201               - 59.6% - 15.4% to go\n",
      " 6. densenet169_progressive   - 40.4% - 34.6% to go\n",
      " 7. vit_base                  - 27.7% - 47.3% to go\n",
      " 8. convnext_tiny             - 25.5% - 49.5% to go\n",
      " 9. CNN+SVM                   - 25.5% - 49.5% to go\n",
      "10. CNN+RandomForest          - 23.4% - 51.6% to go\n",
      "\n",
      "BEST OVERALL: resnext50_32x4d_progressive - 66.0%\n",
      "Best effort with 466 images: 9.0% short of 75% target\n",
      "Consider multi-fold ensemble if this is insufficient\n",
      "\n",
      "All models and results saved in: deer_age_comprehensive_20250801_075228\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Third attempt at model families\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    MIXED_PRECISION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXED_PRECISION_AVAILABLE = False\n",
    "    class autocast:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    if MIXED_PRECISION_AVAILABLE:\n",
    "        scaler = GradScaler()\n",
    "        use_amp = True\n",
    "        print(\"Mixed Precision: Enabled\")\n",
    "    else:\n",
    "        use_amp = False\n",
    "else:\n",
    "    use_amp = False\n",
    "\n",
    "color_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\"\n",
    "grayscale_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\grayscale\"\n",
    "\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        age_str = parts[3]\n",
    "        try:\n",
    "            age = float(age_str.replace('p', '.'))\n",
    "            if age > 5.5:\n",
    "                age = 5.5\n",
    "            return age\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def age_to_class(age):\n",
    "    age_mapping = {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
    "    return age_mapping.get(age, None)\n",
    "\n",
    "def age_to_ordinal(age):\n",
    "    \"\"\"Convert age to ordinal targets for ordinal regression\"\"\"\n",
    "    class_idx = age_to_class(age)\n",
    "    if class_idx is None:\n",
    "        return None\n",
    "    # Create ordinal targets: [1,1,1,0,0] for class 2, [1,1,1,1,0] for class 3, etc.\n",
    "    ordinal = [1 if i <= class_idx else 0 for i in range(5)]\n",
    "    return ordinal\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"CutMix augmentation\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    \n",
    "    W, H = x.size(2), x.size(3)\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "    \n",
    "    return x, y, y[index], lam\n",
    "\n",
    "def load_images(color_path, grayscale_path, img_size=(224, 224)):\n",
    "    images = []\n",
    "    ages = []\n",
    "    \n",
    "    if os.path.exists(color_path):\n",
    "        for filename in os.listdir(color_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(color_path, filename)\n",
    "                        img = cv2.imread(img_path)\n",
    "                        if img is not None:\n",
    "                            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                            img_resized = cv2.resize(img_gray, img_size)\n",
    "                            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                            images.append(img_rgb)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    if os.path.exists(grayscale_path):\n",
    "        for filename in os.listdir(grayscale_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(grayscale_path, filename)\n",
    "                        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                        if img is not None:\n",
    "                            img_resized = cv2.resize(img, img_size)\n",
    "                            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "                            images.append(img_rgb)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    ages = np.array(ages)\n",
    "    \n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    print(f\"Class distribution: {Counter(ages)}\")\n",
    "    \n",
    "    return images, ages\n",
    "\n",
    "def advanced_augment(image):\n",
    "    \"\"\"Comprehensive augmentation suite\"\"\"\n",
    "    # Random horizontal flip\n",
    "    if random.random() < 0.6:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Random rotation with scaling\n",
    "    if random.random() < 0.8:\n",
    "        angle = random.uniform(-20, 20)\n",
    "        scale = random.uniform(0.9, 1.1)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, scale)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Color jittering\n",
    "    if random.random() < 0.9:\n",
    "        alpha = random.uniform(0.8, 1.2)\n",
    "        beta = random.randint(-20, 20)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Gaussian noise\n",
    "    if random.random() < 0.4:\n",
    "        noise = np.random.normal(0, random.uniform(3, 8), image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    # Random erasing\n",
    "    if random.random() < 0.3:\n",
    "        h, w, c = image.shape\n",
    "        area = h * w\n",
    "        target_area = random.uniform(0.02, 0.1) * area\n",
    "        aspect_ratio = random.uniform(0.3, 3.3)\n",
    "        \n",
    "        h_erase = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "        w_erase = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "        \n",
    "        if h_erase < h and w_erase < w:\n",
    "            x1 = random.randint(0, h - h_erase)\n",
    "            y1 = random.randint(0, w - w_erase)\n",
    "            image[x1:x1+h_erase, y1:y1+w_erase, :] = random.randint(0, 255)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class AdvancedDataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False, progressive_size=None, ordinal=False):\n",
    "        self.X = X\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.augment = augment\n",
    "        self.progressive_size = progressive_size or 224\n",
    "        self.ordinal = ordinal\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].copy()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Progressive resizing\n",
    "        if self.progressive_size != 224:\n",
    "            image = cv2.resize(image, (self.progressive_size, self.progressive_size))\n",
    "            image = cv2.resize(image, (224, 224))\n",
    "        \n",
    "        if self.augment:\n",
    "            image = advanced_augment(image)\n",
    "        \n",
    "        image = torch.FloatTensor(image)\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        if self.ordinal:\n",
    "            # Convert to ordinal targets\n",
    "            ordinal_targets = torch.FloatTensor([1 if i <= label.item() else 0 for i in range(4)])\n",
    "            return image, ordinal_targets\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class OrdinalLoss(nn.Module):\n",
    "    \"\"\"Ordinal regression loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super(OrdinalLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        return F.binary_cross_entropy_with_logits(predictions, targets)\n",
    "\n",
    "class SelfSupervisedPretrainer:\n",
    "    \"\"\"Self-supervised pretraining on the deer images\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        # Replace final layer for rotation prediction (4 classes: 0, 90, 180, 270)\n",
    "        if hasattr(model, 'classifier'):\n",
    "            model.classifier = nn.Linear(model.classifier.in_features, 4)\n",
    "        elif hasattr(model, 'fc'):\n",
    "            model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "    \n",
    "    def create_rotation_dataset(self, images):\n",
    "        \"\"\"Create rotation prediction dataset\"\"\"\n",
    "        rotation_images = []\n",
    "        rotation_labels = []\n",
    "        \n",
    "        for img in images:\n",
    "            for rotation in range(4):\n",
    "                rotated = np.rot90(img, k=rotation, axes=(0, 1))\n",
    "                rotation_images.append(rotated)\n",
    "                rotation_labels.append(rotation)\n",
    "        \n",
    "        return np.array(rotation_images), np.array(rotation_labels)\n",
    "    \n",
    "    def pretrain(self, images, epochs=50):\n",
    "        \"\"\"Self-supervised pretraining\"\"\"\n",
    "        print(\"Self-supervised pretraining...\")\n",
    "        \n",
    "        rot_images, rot_labels = self.create_rotation_dataset(images)\n",
    "        dataset = AdvancedDataset(rot_images, rot_labels, augment=True)\n",
    "        loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        \n",
    "        # Ensure model is on correct device\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images_batch, labels_batch in loader:\n",
    "                # Ensure tensors are on correct device\n",
    "                images_batch = images_batch.to(device)\n",
    "                labels_batch = labels_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = self.model(images_batch)\n",
    "                        loss = criterion(outputs, labels_batch)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = self.model(images_batch)\n",
    "                    loss = criterion(outputs, labels_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels_batch.size(0)\n",
    "                correct += (predicted == labels_batch).sum().item()\n",
    "                \n",
    "                # Memory cleanup\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                acc = 100 * correct / total\n",
    "                print(f\"  Pretraining epoch {epoch}: {acc:.1f}%\")\n",
    "        \n",
    "        print(\"Pretraining complete\")\n",
    "        return self.model\n",
    "\n",
    "def create_diverse_models():\n",
    "    \"\"\"Create diverse model architectures\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Vision Transformers\n",
    "    try:\n",
    "        models['vit_base'] = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=5)\n",
    "        print(\"Added Vision Transformer\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # ConvNeXt\n",
    "    try:\n",
    "        models['convnext_tiny'] = timm.create_model('convnext_tiny', pretrained=True, num_classes=5)\n",
    "        print(\"Added ConvNeXt\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # EfficientNet variants\n",
    "    try:\n",
    "        models['efficientnet_v2_s'] = timm.create_model('tf_efficientnetv2_s', pretrained=True, num_classes=5)\n",
    "        print(\"Added EfficientNetV2\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # RegNet\n",
    "    try:\n",
    "        models['regnet_y_800mf'] = timm.create_model('regnetx_800mf', pretrained=True, num_classes=5)\n",
    "        print(\"Added RegNet\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback to proven architectures\n",
    "    models['densenet169'] = timm.create_model('densenet169', pretrained=True, num_classes=5)\n",
    "    models['resnext50'] = timm.create_model('resnext50_32x4d', pretrained=True, num_classes=5)\n",
    "    models['densenet201'] = timm.create_model('densenet201', pretrained=True, num_classes=5)\n",
    "    \n",
    "    return models\n",
    "\n",
    "def train_with_all_techniques(model, train_loader, test_loader, model_name, epochs=150):\n",
    "    \"\"\"Training with every technique combined\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    \n",
    "    # Different optimizers for different models\n",
    "    if 'vit' in model_name.lower():\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=0.3)\n",
    "    elif 'convnext' in model_name.lower():\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.05)\n",
    "    else:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.05)\n",
    "    \n",
    "    # Advanced scheduler with warm restarts\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=30, T_mult=2, eta_min=1e-7\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    patience = 40\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    print(f\"Training {model_name} with ALL techniques\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Random choice of augmentation technique\n",
    "            aug_choice = random.choice(['mixup', 'cutmix', 'normal'])\n",
    "            \n",
    "            if aug_choice == 'mixup' and random.random() < 0.4:\n",
    "                # Mixup\n",
    "                lam = np.random.beta(0.4, 0.4)\n",
    "                batch_size = images.size(0)\n",
    "                index = torch.randperm(batch_size).to(device)\n",
    "                mixed_images = lam * images + (1 - lam) * images[index, :]\n",
    "                y_a, y_b = labels, labels[index]\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(mixed_images)\n",
    "                        loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(mixed_images)\n",
    "                    loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == y_a).sum().item()\n",
    "                \n",
    "            elif aug_choice == 'cutmix' and random.random() < 0.4:\n",
    "                # CutMix\n",
    "                mixed_images, y_a, y_b, lam = cutmix_data(images, labels, alpha=1.0)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(mixed_images)\n",
    "                        loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(mixed_images)\n",
    "                    loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == y_a).sum().item()\n",
    "                \n",
    "            else:\n",
    "                # Normal training\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Advanced TTA evaluation\n",
    "        test_acc = evaluate_with_advanced_tta(model, test_loader)\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            patience_counter = 0\n",
    "            best_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 25 == 0 or patience_counter >= patience:\n",
    "            print(f\"  Epoch {epoch:3d}: Train {train_acc:.1f}%, Test+TTA {test_acc:.1f}%\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"  Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, best_acc\n",
    "\n",
    "def evaluate_with_advanced_tta(model, test_loader, num_tta=6):\n",
    "    \"\"\"Advanced TTA with multiple techniques\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            tta_outputs = []\n",
    "            \n",
    "            # Original\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "            tta_outputs.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Horizontal flip\n",
    "            flipped = torch.flip(images, [3])\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(flipped)\n",
    "            else:\n",
    "                outputs = model(flipped)\n",
    "            tta_outputs.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Multiple scales\n",
    "            for scale in [0.9, 1.1]:\n",
    "                scaled = F.interpolate(images, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "                scaled = F.interpolate(scaled, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(scaled)\n",
    "                else:\n",
    "                    outputs = model(scaled)\n",
    "                tta_outputs.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Crop variants\n",
    "            for crop_factor in [0.85, 0.95]:\n",
    "                size = int(224 * crop_factor)\n",
    "                start = (224 - size) // 2\n",
    "                cropped = images[:, :, start:start+size, start:start+size]\n",
    "                cropped = F.interpolate(cropped, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(cropped)\n",
    "                else:\n",
    "                    outputs = model(cropped)\n",
    "                tta_outputs.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Average all predictions\n",
    "            avg_output = torch.stack(tta_outputs).mean(0)\n",
    "            _, predicted = torch.max(avg_output, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "def extract_features_for_ml(model, images):\n",
    "    \"\"\"Extract CNN features for traditional ML\"\"\"\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    dataset = AdvancedDataset(images, np.zeros(len(images)), augment=False)\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            \n",
    "            # Remove final classification layer\n",
    "            if hasattr(model, 'classifier'):\n",
    "                feat = model.features(imgs)\n",
    "                feat = F.adaptive_avg_pool2d(feat, (1, 1))\n",
    "                feat = feat.view(feat.size(0), -1)\n",
    "            elif hasattr(model, 'fc'):\n",
    "                feat = model.forward_head(imgs, pre_logits=True)\n",
    "            else:\n",
    "                # For ViT and other models\n",
    "                feat = model.forward_features(imgs)\n",
    "                if len(feat.shape) > 2:\n",
    "                    feat = feat.mean(dim=1)\n",
    "            \n",
    "            features.append(feat.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(features)\n",
    "\n",
    "def progressive_training(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"Progressive training: easy distinctions first\"\"\"\n",
    "    print(f\"Progressive training for {model_name}\")\n",
    "    \n",
    "    # Phase 1: Binary young vs old (classes 0,1,2 vs 3,4)\n",
    "    y_binary_train = np.array([0 if y <= 2 else 1 for y in y_train])\n",
    "    y_binary_test = np.array([0 if y <= 2 else 1 for y in y_test])\n",
    "    \n",
    "    # Modify model for binary classification\n",
    "    if hasattr(model, 'classifier'):\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 2)\n",
    "    elif hasattr(model, 'fc'):\n",
    "        model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Train binary classifier\n",
    "    binary_dataset_train = AdvancedDataset(X_train, y_binary_train, augment=True)\n",
    "    binary_dataset_test = AdvancedDataset(X_test, y_binary_test, augment=False)\n",
    "    binary_loader_train = DataLoader(binary_dataset_train, batch_size=16, shuffle=True, num_workers=0)\n",
    "    binary_loader_test = DataLoader(binary_dataset_test, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(30):\n",
    "        for images, labels in binary_loader_train:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    # Phase 2: Modify for 5-class and fine-tune\n",
    "    if hasattr(model, 'classifier'):\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 5)\n",
    "    elif hasattr(model, 'fc'):\n",
    "        model.fc = nn.Linear(model.fc.in_features, 5)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Fine-tune on all classes\n",
    "    full_dataset_train = AdvancedDataset(X_train, y_train, augment=True)\n",
    "    full_dataset_test = AdvancedDataset(X_test, y_test, augment=False)\n",
    "    full_loader_train = DataLoader(full_dataset_train, batch_size=16, shuffle=True, num_workers=0)\n",
    "    full_loader_test = DataLoader(full_dataset_test, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Lower learning rate for fine-tuning\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0002, weight_decay=0.01)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for images, labels in full_loader_train:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        test_acc = evaluate_with_advanced_tta(model, full_loader_test)\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"  Progressive epoch {epoch}: {test_acc:.1f}%\")\n",
    "    \n",
    "    return model, best_acc\n",
    "\n",
    "# Main comprehensive pipeline\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"deer_age_comprehensive_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Comprehensive results saved to: {output_dir}\")\n",
    "\n",
    "print(\"Loading images...\")\n",
    "X, y = load_images(color_path, grayscale_path)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "# PHASE 1: Advanced Architecture Survey\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: ADVANCED ARCHITECTURE SURVEY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "diverse_models = create_diverse_models()\n",
    "phase1_results = []\n",
    "\n",
    "for model_name, model_template in diverse_models.items():\n",
    "    print(f\"\\n--- Testing {model_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Self-supervised pretraining\n",
    "        pretrainer = SelfSupervisedPretrainer(model_template.to(device))\n",
    "        pretrained_model = pretrainer.pretrain(X_train, epochs=30)\n",
    "        \n",
    "        # Reset for main task\n",
    "        if hasattr(pretrained_model, 'classifier'):\n",
    "            pretrained_model.classifier = nn.Linear(pretrained_model.classifier.in_features, 5)\n",
    "        elif hasattr(pretrained_model, 'fc'):\n",
    "            pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 5)\n",
    "        \n",
    "        pretrained_model = pretrained_model.to(device)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = AdvancedDataset(X_train, y_train, augment=True)\n",
    "        test_dataset = AdvancedDataset(X_test, y_test, augment=False)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=0)\n",
    "        \n",
    "        # Train with all techniques\n",
    "        trained_model, accuracy = train_with_all_techniques(\n",
    "            pretrained_model, train_loader, test_loader, model_name, epochs=120\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        acc_str = f\"{accuracy:.1f}\".replace('.', 'p')\n",
    "        model_filename = f\"{model_name}_pretrained_{acc_str}pct.pth\"\n",
    "        model_path = os.path.join(output_dir, model_filename)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': trained_model.state_dict(),\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'num_classes': 5,\n",
    "            'pretrained': True\n",
    "        }, model_path)\n",
    "        \n",
    "        phase1_results.append({\n",
    "            'model': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'filename': model_filename,\n",
    "            'trained_model': trained_model\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name} with pretraining: {accuracy:.1f}%\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# PHASE 2: Progressive Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: PROGRESSIVE TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "phase2_results = []\n",
    "for model_name in ['densenet169', 'resnext50_32x4d']:\n",
    "    print(f\"\\n--- Progressive training {model_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=5)\n",
    "        trained_model, accuracy = progressive_training(\n",
    "            model, X_train, y_train, X_test, y_test, model_name\n",
    "        )\n",
    "        \n",
    "        acc_str = f\"{accuracy:.1f}\".replace('.', 'p')\n",
    "        model_filename = f\"{model_name}_progressive_{acc_str}pct.pth\"\n",
    "        model_path = os.path.join(output_dir, model_filename)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': trained_model.state_dict(),\n",
    "            'model_name': f\"{model_name}_progressive\",\n",
    "            'accuracy': accuracy,\n",
    "            'num_classes': 5\n",
    "        }, model_path)\n",
    "        \n",
    "        phase2_results.append({\n",
    "            'model': f\"{model_name}_progressive\",\n",
    "            'accuracy': accuracy,\n",
    "            'filename': model_filename,\n",
    "            'trained_model': trained_model\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name} progressive: {accuracy:.1f}%\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with progressive {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# PHASE 3: CNN Feature Extraction + Traditional ML\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3: HYBRID CNN + TRADITIONAL ML\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "phase3_results = []\n",
    "if phase1_results:\n",
    "    # Use best CNN model for feature extraction\n",
    "    best_cnn = max(phase1_results, key=lambda x: x['accuracy'])\n",
    "    print(f\"Using {best_cnn['model']} for feature extraction...\")\n",
    "    \n",
    "    # Extract features\n",
    "    train_features = extract_features_for_ml(best_cnn['trained_model'], X_train)\n",
    "    test_features = extract_features_for_ml(best_cnn['trained_model'], X_test)\n",
    "    \n",
    "    # Traditional ML models\n",
    "    ml_models = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n",
    "        'XGBoost': XGBClassifier(n_estimators=200, max_depth=6, random_state=42),\n",
    "        'SVM': SVC(kernel='rbf', C=10, gamma='scale', random_state=42)\n",
    "    }\n",
    "    \n",
    "    for ml_name, ml_model in ml_models.items():\n",
    "        try:\n",
    "            print(f\"Training {ml_name} on CNN features...\")\n",
    "            ml_model.fit(train_features, y_train)\n",
    "            ml_predictions = ml_model.predict(test_features)\n",
    "            ml_accuracy = accuracy_score(y_test, ml_predictions) * 100\n",
    "            \n",
    "            phase3_results.append({\n",
    "                'model': f\"CNN+{ml_name}\",\n",
    "                'accuracy': ml_accuracy,\n",
    "                'type': 'hybrid'\n",
    "            })\n",
    "            \n",
    "            print(f\"CNN + {ml_name}: {ml_accuracy:.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {ml_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "# PHASE 4: Ordinal Regression\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 4: ORDINAL REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "phase4_results = []\n",
    "try:\n",
    "    print(\"Training ordinal regression model...\")\n",
    "    \n",
    "    # Create ordinal targets\n",
    "    y_train_ordinal = np.array([age_to_ordinal(y_train[i]*0.5 + 1.5) for i in range(len(y_train))])\n",
    "    y_test_ordinal = np.array([age_to_ordinal(y_test[i]*0.5 + 1.5) for i in range(len(y_test))])\n",
    "    \n",
    "    # Remove None values\n",
    "    valid_train = [i for i, val in enumerate(y_train_ordinal) if val is not None]\n",
    "    valid_test = [i for i, val in enumerate(y_test_ordinal) if val is not None]\n",
    "    \n",
    "    if valid_train and valid_test:\n",
    "        X_train_ord = X_train[valid_train]\n",
    "        y_train_ord = np.array([y_train_ordinal[i] for i in valid_train])\n",
    "        X_test_ord = X_test[valid_test]\n",
    "        y_test_ord = np.array([y_test_ordinal[i] for i in valid_test])\n",
    "        \n",
    "        # Create ordinal model\n",
    "        ordinal_model = timm.create_model('densenet169', pretrained=True, num_classes=4)\n",
    "        ordinal_model = ordinal_model.to(device)\n",
    "        \n",
    "        # Replace classifier for ordinal regression\n",
    "        if hasattr(ordinal_model, 'classifier'):\n",
    "            ordinal_model.classifier = nn.Linear(ordinal_model.classifier.in_features, 4)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_ord_dataset = AdvancedDataset(X_train_ord, np.zeros(len(X_train_ord)), augment=True, ordinal=True)\n",
    "        test_ord_dataset = AdvancedDataset(X_test_ord, np.zeros(len(X_test_ord)), augment=False, ordinal=True)\n",
    "        train_ord_loader = DataLoader(train_ord_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "        test_ord_loader = DataLoader(test_ord_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "        \n",
    "        # Train ordinal model\n",
    "        ordinal_criterion = OrdinalLoss()\n",
    "        ordinal_optimizer = optim.AdamW(ordinal_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        \n",
    "        best_ordinal_acc = 0.0\n",
    "        for epoch in range(50):\n",
    "            ordinal_model.train()\n",
    "            for images, targets in train_ord_loader:\n",
    "                images = images.to(device)\n",
    "                # Reconstruct ordinal targets from y_train_ord\n",
    "                batch_targets = torch.FloatTensor([y_train_ord[i % len(y_train_ord)] for i in range(len(images))]).to(device)\n",
    "                \n",
    "                ordinal_optimizer.zero_grad()\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = ordinal_model(images)\n",
    "                        loss = ordinal_criterion(outputs, batch_targets)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(ordinal_optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = ordinal_model(images)\n",
    "                    loss = ordinal_criterion(outputs, batch_targets)\n",
    "                    loss.backward()\n",
    "                    ordinal_optimizer.step()\n",
    "            \n",
    "            # Evaluate ordinal model (convert back to class predictions)\n",
    "            ordinal_model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, _ in test_ord_loader:\n",
    "                    images = images.to(device)\n",
    "                    outputs = torch.sigmoid(ordinal_model(images))\n",
    "                    # Convert ordinal predictions back to classes\n",
    "                    predicted_classes = (outputs > 0.5).sum(dim=1)\n",
    "                    \n",
    "                    batch_true_classes = torch.LongTensor([y_test[valid_test[i % len(valid_test)]] for i in range(len(images))]).to(device)\n",
    "                    \n",
    "                    total += len(images)\n",
    "                    correct += (predicted_classes == batch_true_classes).sum().item()\n",
    "            \n",
    "            ordinal_acc = 100 * correct / total\n",
    "            if ordinal_acc > best_ordinal_acc:\n",
    "                best_ordinal_acc = ordinal_acc\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"  Ordinal epoch {epoch}: {ordinal_acc:.1f}%\")\n",
    "        \n",
    "        phase4_results.append({\n",
    "            'model': 'Ordinal_Regression',\n",
    "            'accuracy': best_ordinal_acc,\n",
    "            'type': 'ordinal'\n",
    "        })\n",
    "        \n",
    "        print(f\"Ordinal regression: {best_ordinal_acc:.1f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with ordinal regression: {e}\")\n",
    "\n",
    "# FINAL RESULTS COMPILATION\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = []\n",
    "all_results.extend(phase1_results)\n",
    "all_results.extend(phase2_results)\n",
    "all_results.extend(phase3_results)\n",
    "all_results.extend(phase4_results)\n",
    "\n",
    "all_results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "print(\"PHASE 1 - ADVANCED ARCHITECTURES + PRETRAINING:\")\n",
    "for result in phase1_results:\n",
    "    print(f\"  {result['model']:25s} - {result['accuracy']:.1f}%\")\n",
    "\n",
    "print(\"\\nPHASE 2 - PROGRESSIVE TRAINING:\")\n",
    "for result in phase2_results:\n",
    "    print(f\"  {result['model']:25s} - {result['accuracy']:.1f}%\")\n",
    "\n",
    "print(\"\\nPHASE 3 - HYBRID CNN + TRADITIONAL ML:\")\n",
    "for result in phase3_results:\n",
    "    print(f\"  {result['model']:25s} - {result['accuracy']:.1f}%\")\n",
    "\n",
    "print(\"\\nPHASE 4 - ORDINAL REGRESSION:\")\n",
    "for result in phase4_results:\n",
    "    print(f\"  {result['model']:25s} - {result['accuracy']:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OVERALL BEST RESULTS:\")\n",
    "for i, result in enumerate(all_results[:10], 1):\n",
    "    status = \"TARGET ACHIEVED\" if result['accuracy'] >= 75.0 else f\"{75.0-result['accuracy']:.1f}% to go\"\n",
    "    print(f\"{i:2d}. {result['model']:25s} - {result['accuracy']:.1f}% - {status}\")\n",
    "\n",
    "if all_results:\n",
    "    best_overall = all_results[0]\n",
    "    print(f\"\\nBEST OVERALL: {best_overall['model']} - {best_overall['accuracy']:.1f}%\")\n",
    "    \n",
    "    if best_overall['accuracy'] >= 75.0:\n",
    "        print(\"SUCCESS: 75% TARGET ACHIEVED!\")\n",
    "    else:\n",
    "        gap = 75.0 - best_overall['accuracy']\n",
    "        print(f\"Best effort with 466 images: {gap:.1f}% short of 75% target\")\n",
    "        print(\"Consider multi-fold ensemble if this is insufficient\")\n",
    "\n",
    "print(f\"\\nAll models and results saved in: {output_dir}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d54b84-ad86-44ac-bbd2-5e459bb2897a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Mixed Precision: Enabled\n",
      "COLOR-ONLY models saved to: deer_age_color_only_20250802_082228\n",
      "Loading COLOR images only...\n",
      "Loading ONLY color images from: G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\n",
      "Color images loaded: 361\n",
      "Image shape: (361, 224, 224, 3)\n",
      "Class distribution: Counter({np.int64(4): 88, np.int64(3): 75, np.int64(2): 75, np.int64(1): 66, np.int64(0): 57})\n",
      "Train: 288, Test: 73\n",
      "\n",
      "================================================================================\n",
      "COLOR-ONLY WIDE FAMILY SURVEY\n",
      "================================================================================\n",
      "Hypothesis: Color information will boost accuracy to 75%+\n",
      "Strategy: Preserve RGB, test many architectures\n",
      "Testing 20 model families on COLOR images...\n",
      "\n",
      "==================================================\n",
      "Testing ResNeXt50\n",
      "==================================================\n",
      "Training ResNeXt50 on COLOR images\n",
      "  Epoch   0: Train 22.9%, Test+TTA 27.4%\n",
      "  Epoch  20: Train 74.0%, Test+TTA 60.3%\n",
      "  Epoch  40: Train 82.6%, Test+TTA 63.0%\n",
      "  Epoch  60: Train 86.1%, Test+TTA 63.0%\n",
      "  Epoch  61: Train 96.5%, Test+TTA 63.0%\n",
      "  Early stopping at epoch 61\n",
      "ResNeXt50: 68.5% - 6.5% to go\n",
      "\n",
      "==================================================\n",
      "Testing DenseNet169\n",
      "==================================================\n",
      "Training DenseNet169 on COLOR images\n",
      "  Epoch   0: Train 25.7%, Test+TTA 39.7%\n",
      "  Epoch  20: Train 87.8%, Test+TTA 65.8%\n",
      "  Epoch  40: Train 82.6%, Test+TTA 63.0%\n",
      "  Epoch  47: Train 75.3%, Test+TTA 65.8%\n",
      "  Early stopping at epoch 47\n",
      "DenseNet169: 68.5% - 6.5% to go\n",
      "\n",
      "==================================================\n",
      "Testing DenseNet201\n",
      "==================================================\n",
      "Training DenseNet201 on COLOR images\n",
      "  Epoch   0: Train 30.6%, Test+TTA 45.2%\n",
      "  Epoch  20: Train 68.4%, Test+TTA 65.8%\n",
      "  Epoch  30: Train 84.0%, Test+TTA 64.4%\n",
      "  Early stopping at epoch 30\n",
      "DenseNet201: 69.9% - 5.1% to go\n",
      "\n",
      "==================================================\n",
      "Testing EfficientNetV2_S\n",
      "==================================================\n",
      "Training EfficientNetV2_S on COLOR images\n",
      "  Epoch   0: Train 15.3%, Test+TTA 17.8%\n",
      "  Epoch  20: Train 21.5%, Test+TTA 24.7%\n",
      "  Epoch  40: Train 27.8%, Test+TTA 32.9%\n",
      "  Epoch  60: Train 32.6%, Test+TTA 26.0%\n",
      "  Epoch  65: Train 26.7%, Test+TTA 28.8%\n",
      "  Early stopping at epoch 65\n",
      "EfficientNetV2_S: 32.9% - 42.1% to go\n",
      "\n",
      "==================================================\n",
      "Testing EfficientNetB0\n",
      "==================================================\n",
      "Training EfficientNetB0 on COLOR images\n",
      "  Epoch   0: Train 22.9%, Test+TTA 32.9%\n",
      "  Epoch  20: Train 83.0%, Test+TTA 57.5%\n",
      "  Epoch  40: Train 74.3%, Test+TTA 68.5%\n",
      "  Epoch  54: Train 94.1%, Test+TTA 68.5%\n",
      "  Early stopping at epoch 54\n",
      "EfficientNetB0: 72.6% - 2.4% to go\n",
      "\n",
      "==================================================\n",
      "Testing EfficientNetB1\n",
      "==================================================\n",
      "Training EfficientNetB1 on COLOR images\n",
      "  Epoch   0: Train 27.4%, Test+TTA 30.1%\n",
      "  Epoch  20: Train 81.9%, Test+TTA 64.4%\n",
      "  Epoch  40: Train 88.5%, Test+TTA 58.9%\n",
      "  Epoch  44: Train 83.0%, Test+TTA 68.5%\n",
      "  Early stopping at epoch 44\n",
      "EfficientNetB1: 68.5% - 6.5% to go\n",
      "\n",
      "==================================================\n",
      "Testing EfficientNetB2\n",
      "==================================================\n",
      "Training EfficientNetB2 on COLOR images\n",
      "  Epoch   0: Train 23.3%, Test+TTA 35.6%\n",
      "  Epoch  20: Train 76.0%, Test+TTA 57.5%\n",
      "  Epoch  40: Train 79.5%, Test+TTA 61.6%\n",
      "  Epoch  41: Train 79.2%, Test+TTA 60.3%\n",
      "  Early stopping at epoch 41\n",
      "EfficientNetB2: 64.4% - 10.6% to go\n",
      "\n",
      "==================================================\n",
      "Testing EfficientNetB3\n",
      "==================================================\n",
      "Training EfficientNetB3 on COLOR images\n",
      "  Epoch   0: Train 20.5%, Test+TTA 31.5%\n",
      "  Epoch  20: Train 83.3%, Test+TTA 64.4%\n",
      "  Epoch  40: Train 75.0%, Test+TTA 67.1%\n",
      "  Epoch  49: Train 93.1%, Test+TTA 60.3%\n",
      "  Early stopping at epoch 49\n",
      "EfficientNetB3: 71.2% - 3.8% to go\n",
      "\n",
      "==================================================\n",
      "Testing ResNet50\n",
      "==================================================\n",
      "Training ResNet50 on COLOR images\n",
      "  Epoch   0: Train 20.1%, Test+TTA 27.4%\n",
      "  Epoch  20: Train 91.0%, Test+TTA 60.3%\n",
      "  Epoch  40: Train 92.4%, Test+TTA 60.3%\n",
      "  Epoch  57: Train 91.7%, Test+TTA 63.0%\n",
      "  Early stopping at epoch 57\n",
      "ResNet50: 68.5% - 6.5% to go\n",
      "\n",
      "==================================================\n",
      "Testing ResNet101\n",
      "==================================================\n",
      "Training ResNet101 on COLOR images\n",
      "  Epoch   0: Train 22.6%, Test+TTA 16.4%\n",
      "  Epoch  20: Train 78.8%, Test+TTA 60.3%\n",
      "  Epoch  40: Train 83.0%, Test+TTA 65.8%\n",
      "  Epoch  58: Train 86.5%, Test+TTA 65.8%\n",
      "  Early stopping at epoch 58\n",
      "ResNet101: 68.5% - 6.5% to go\n",
      "\n",
      "==================================================\n",
      "Testing ResNet152\n",
      "==================================================\n",
      "Training ResNet152 on COLOR images\n",
      "  Epoch   0: Train 20.1%, Test+TTA 24.7%\n",
      "  Epoch  20: Train 85.8%, Test+TTA 65.8%\n",
      "  Epoch  40: Train 86.5%, Test+TTA 67.1%\n",
      "  Epoch  54: Train 96.2%, Test+TTA 65.8%\n",
      "  Early stopping at epoch 54\n",
      "ResNet152: 69.9% - 5.1% to go\n",
      "\n",
      "==================================================\n",
      "Testing DenseNet121\n",
      "==================================================\n",
      "Training DenseNet121 on COLOR images\n",
      "  Epoch   0: Train 23.6%, Test+TTA 34.2%\n",
      "  Epoch  20: Train 82.6%, Test+TTA 58.9%\n",
      "  Epoch  38: Train 91.7%, Test+TTA 58.9%\n",
      "  Early stopping at epoch 38\n",
      "DenseNet121: 64.4% - 10.6% to go\n",
      "\n",
      "==================================================\n",
      "Testing ConvNeXt_Tiny\n",
      "==================================================\n",
      "Training ConvNeXt_Tiny on COLOR images\n",
      "  Epoch   0: Train 20.1%, Test+TTA 20.5%\n",
      "  Epoch  20: Train 78.1%, Test+TTA 56.2%\n",
      "  Epoch  40: Train 92.4%, Test+TTA 64.4%\n",
      "  Epoch  60: Train 100.0%, Test+TTA 67.1%\n",
      "  Epoch  63: Train 92.4%, Test+TTA 64.4%\n",
      "  Early stopping at epoch 63\n",
      "ConvNeXt_Tiny: 71.2% - 3.8% to go\n",
      "\n",
      "==================================================\n",
      "Testing ConvNeXt_Small\n",
      "==================================================\n",
      "Training ConvNeXt_Small on COLOR images\n",
      "  Epoch   0: Train 19.8%, Test+TTA 17.8%\n",
      "  Epoch  20: Train 93.1%, Test+TTA 58.9%\n",
      "  Epoch  40: Train 96.2%, Test+TTA 65.8%\n",
      "  Epoch  56: Train 98.3%, Test+TTA 67.1%\n",
      "  Early stopping at epoch 56\n",
      "ConvNeXt_Small: 68.5% - 6.5% to go\n",
      "\n",
      "==================================================\n",
      "Testing ViT_Base\n",
      "==================================================\n",
      "Training ViT_Base on COLOR images\n",
      "  Epoch   0: Train 22.6%, Test+TTA 24.7%\n",
      "  Epoch  20: Train 31.6%, Test+TTA 35.6%\n",
      "  Epoch  40: Train 53.1%, Test+TTA 31.5%\n",
      "  Epoch  60: Train 75.3%, Test+TTA 54.8%\n",
      "  Epoch  80: Train 82.3%, Test+TTA 60.3%\n",
      "ViT_Base: 63.0% - 12.0% to go\n",
      "\n",
      "==================================================\n",
      "Testing ViT_Small\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e52a0a875204d27ac4ad652a70c83f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ViT_Small on COLOR images\n",
      "  Epoch   0: Train 20.5%, Test+TTA 23.3%\n",
      "  Epoch  20: Train 51.0%, Test+TTA 39.7%\n",
      "  Epoch  40: Train 69.1%, Test+TTA 64.4%\n",
      "  Epoch  60: Train 80.6%, Test+TTA 63.0%\n",
      "  Epoch  67: Train 83.3%, Test+TTA 58.9%\n",
      "  Early stopping at epoch 67\n",
      "ViT_Small: 67.1% - 7.9% to go\n",
      "\n",
      "==================================================\n",
      "Testing RegNetX_800MF\n",
      "==================================================\n",
      "Error with RegNetX_800MF: Unknown model (regnetx_800mf)\n",
      "\n",
      "==================================================\n",
      "Testing RegNetY_800MF\n",
      "==================================================\n",
      "Error with RegNetY_800MF: Unknown model (regnety_800mf)\n",
      "\n",
      "==================================================\n",
      "Testing WideResNet50\n",
      "==================================================\n",
      "Training WideResNet50 on COLOR images\n",
      "  Epoch   0: Train 25.0%, Test+TTA 37.0%\n",
      "  Epoch  20: Train 84.0%, Test+TTA 65.8%\n",
      "  Epoch  39: Train 77.1%, Test+TTA 64.4%\n",
      "  Early stopping at epoch 39\n",
      "WideResNet50: 68.5% - 6.5% to go\n",
      "\n",
      "==================================================\n",
      "Testing MobileNetV3_Large\n",
      "==================================================\n",
      "Training MobileNetV3_Large on COLOR images\n",
      "  Epoch   0: Train 25.7%, Test+TTA 31.5%\n",
      "  Epoch  20: Train 84.7%, Test+TTA 53.4%\n",
      "  Epoch  40: Train 78.1%, Test+TTA 56.2%\n",
      "  Epoch  51: Train 77.4%, Test+TTA 53.4%\n",
      "  Early stopping at epoch 51\n",
      "MobileNetV3_Large: 64.4% - 10.6% to go\n",
      "\n",
      "================================================================================\n",
      "COLOR-ONLY RESULTS BY FAMILY\n",
      "================================================================================\n",
      "\n",
      "ResNet FAMILY:\n",
      "  ResNet152            - 69.9% - 5.1% to go\n",
      "  ResNeXt50            - 68.5% - 6.5% to go\n",
      "  ResNet50             - 68.5% - 6.5% to go\n",
      "  ResNet101            - 68.5% - 6.5% to go\n",
      "  WideResNet50         - 68.5% - 6.5% to go\n",
      "\n",
      "DenseNet FAMILY:\n",
      "  DenseNet201          - 69.9% - 5.1% to go\n",
      "  DenseNet169          - 68.5% - 6.5% to go\n",
      "  DenseNet121          - 64.4% - 10.6% to go\n",
      "\n",
      "EfficientNet FAMILY:\n",
      "  EfficientNetB0       - 72.6% - 2.4% to go\n",
      "  EfficientNetB3       - 71.2% - 3.8% to go\n",
      "  EfficientNetB1       - 68.5% - 6.5% to go\n",
      "  EfficientNetB2       - 64.4% - 10.6% to go\n",
      "  EfficientNetV2_S     - 32.9% - 42.1% to go\n",
      "\n",
      "ConvNeXt FAMILY:\n",
      "  ConvNeXt_Tiny        - 71.2% - 3.8% to go\n",
      "  ConvNeXt_Small       - 68.5% - 6.5% to go\n",
      "\n",
      "ViT FAMILY:\n",
      "  ViT_Small            - 67.1% - 7.9% to go\n",
      "  ViT_Base             - 63.0% - 12.0% to go\n",
      "\n",
      "Mobile FAMILY:\n",
      "  MobileNetV3_Large    - 64.4% - 10.6% to go\n",
      "\n",
      "================================================================================\n",
      "OVERALL TOP PERFORMERS (COLOR-ONLY)\n",
      "================================================================================\n",
      " 1. EfficientNetB0       - 72.6% - 2.4% to go\n",
      " 2. EfficientNetB3       - 71.2% - 3.8% to go\n",
      " 3. ConvNeXt_Tiny        - 71.2% - 3.8% to go\n",
      " 4. DenseNet201          - 69.9% - 5.1% to go\n",
      " 5. ResNet152            - 69.9% - 5.1% to go\n",
      " 6. ResNeXt50            - 68.5% - 6.5% to go\n",
      " 7. DenseNet169          - 68.5% - 6.5% to go\n",
      " 8. EfficientNetB1       - 68.5% - 6.5% to go\n",
      " 9. ResNet50             - 68.5% - 6.5% to go\n",
      "10. ResNet101            - 68.5% - 6.5% to go\n",
      "11. ConvNeXt_Small       - 68.5% - 6.5% to go\n",
      "12. WideResNet50         - 68.5% - 6.5% to go\n",
      "13. ViT_Small            - 67.1% - 7.9% to go\n",
      "14. EfficientNetB2       - 64.4% - 10.6% to go\n",
      "15. DenseNet121          - 64.4% - 10.6% to go\n",
      "\n",
      "BEST COLOR-ONLY MODEL: EfficientNetB0 - 72.6%\n",
      "COLOR-ONLY RESULT: 2.4% short of 75% target\n",
      "IMPROVEMENT: +6.6% vs mixed color/grayscale data\n",
      "\n",
      "Total models tested: 18\n",
      "All COLOR-ONLY models saved in: deer_age_color_only_20250802_082228\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Multi-family survey, but ONLY color images\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    MIXED_PRECISION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXED_PRECISION_AVAILABLE = False\n",
    "    class autocast:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    if MIXED_PRECISION_AVAILABLE:\n",
    "        scaler = GradScaler()\n",
    "        use_amp = True\n",
    "        print(\"Mixed Precision: Enabled\")\n",
    "    else:\n",
    "        use_amp = False\n",
    "else:\n",
    "    use_amp = False\n",
    "\n",
    "# ONLY COLOR IMAGES PATH\n",
    "color_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\"\n",
    "\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        age_str = parts[3]\n",
    "        try:\n",
    "            age = float(age_str.replace('p', '.'))\n",
    "            if age > 5.5:\n",
    "                age = 5.5\n",
    "            return age\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def age_to_class(age):\n",
    "    age_mapping = {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
    "    return age_mapping.get(age, None)\n",
    "\n",
    "def load_color_images_only(color_path, img_size=(224, 224)):\n",
    "    \"\"\"Load ONLY color images and preserve RGB color information\"\"\"\n",
    "    images = []\n",
    "    ages = []\n",
    "    \n",
    "    print(f\"Loading ONLY color images from: {color_path}\")\n",
    "    \n",
    "    if os.path.exists(color_path):\n",
    "        for filename in os.listdir(color_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(color_path, filename)\n",
    "                        img = cv2.imread(img_path)\n",
    "                        if img is not None:\n",
    "                            # PRESERVE COLOR: BGR -> RGB (no grayscale conversion)\n",
    "                            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                            img_resized = cv2.resize(img_rgb, img_size)\n",
    "                            assert img_resized.shape == (*img_size, 3), f\"Image {filename} not resized correctly: {img_resized.shape}\"\n",
    "                            images.append(img_resized)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    ages = np.array(ages)\n",
    "    \n",
    "    print(f\"Color images loaded: {len(images)}\")\n",
    "    print(f\"Image shape: {images.shape}\")\n",
    "    print(f\"Class distribution: {Counter(ages)}\")\n",
    "    \n",
    "    return images, ages\n",
    "\n",
    "def smart_augment(image):\n",
    "    \"\"\"Smart augmentation that preserves color information\"\"\"\n",
    "    # Ensure image is uint8\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    # Horizontal flip (deer can face either direction)\n",
    "    if random.random() < 0.6:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Rotation (not too much to preserve antler features)\n",
    "    if random.random() < 0.7:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Color jittering (vary lighting conditions)\n",
    "    if random.random() < 0.8:\n",
    "        # Brightness and contrast - use cv2.convertScaleAbs to ensure valid range\n",
    "        alpha = random.uniform(0.8, 1.2)\n",
    "        beta = random.randint(-15, 15)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Hue/Saturation shifts (seasonal color changes)\n",
    "    if random.random() < 0.5:\n",
    "        try:\n",
    "            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            # Hue shift - ensure values stay in valid range\n",
    "            h_shift = random.randint(-10, 10)\n",
    "            hsv[:, :, 0] = np.clip(hsv[:, :, 0].astype(np.int16) + h_shift, 0, 179).astype(np.uint8)\n",
    "            # Saturation shift\n",
    "            s_factor = random.uniform(0.8, 1.2)\n",
    "            hsv[:, :, 1] = np.clip(hsv[:, :, 1].astype(np.float32) * s_factor, 0, 255).astype(np.uint8)\n",
    "            image = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "        except:\n",
    "            pass  # Skip HSV if conversion fails\n",
    "    \n",
    "    # Gaussian noise\n",
    "    if random.random() < 0.3:\n",
    "        noise = np.random.normal(0, 3, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class ColorDeerDataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False):\n",
    "        self.X = X\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.augment = augment\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].copy()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        if self.augment:\n",
    "            image = smart_augment(image)\n",
    "        \n",
    "        image = torch.FloatTensor(image)\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Convert to CHW format\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Normalize with ImageNet statistics\n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def create_model(model_name, num_classes=5):\n",
    "    \"\"\"Create model with optimal regularization\"\"\"\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes, drop_rate=0.4)\n",
    "    \n",
    "    # Strategic freezing based on model type\n",
    "    if 'densenet' in model_name:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('denseblock4' in name or 'classifier' in name):\n",
    "                param.requires_grad = False\n",
    "    elif 'resnext' in model_name or 'resnet' in model_name:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('layer4' in name or 'fc' in name):\n",
    "                param.requires_grad = False\n",
    "    elif 'efficientnet' in model_name:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('blocks.6' in name or 'blocks.7' in name or 'classifier' in name):\n",
    "                param.requires_grad = False\n",
    "    elif 'convnext' in model_name:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('stages.3' in name or 'head' in name):\n",
    "                param.requires_grad = False\n",
    "    elif 'vit' in model_name:\n",
    "        # Freeze early transformer blocks\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'blocks.0.' in name or 'blocks.1.' in name or 'blocks.2.' in name:\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def train_model_optimized(model, train_loader, test_loader, model_name, epochs=100):\n",
    "    \"\"\"Optimized training with proven techniques\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # Architecture-specific optimizers\n",
    "    if 'vit' in model_name.lower():\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.05)\n",
    "    elif 'convnext' in model_name.lower():\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0008, weight_decay=0.05)\n",
    "    elif 'efficientnet' in model_name.lower():\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0006, weight_decay=0.04)\n",
    "    else:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0007, weight_decay=0.05)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-7)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    patience = 25\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    print(f\"Training {model_name} on COLOR images\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Occasional mixup\n",
    "            if random.random() < 0.3:\n",
    "                lam = np.random.beta(0.4, 0.4)\n",
    "                batch_size = images.size(0)\n",
    "                index = torch.randperm(batch_size).to(device)\n",
    "                mixed_images = lam * images + (1 - lam) * images[index, :]\n",
    "                y_a, y_b = labels, labels[index]\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(mixed_images)\n",
    "                        loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(mixed_images)\n",
    "                    loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == y_a).sum().item()\n",
    "            else:\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Test with TTA\n",
    "        test_acc = evaluate_with_tta(model, test_loader)\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            patience_counter = 0\n",
    "            best_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 20 == 0 or patience_counter >= patience:\n",
    "            print(f\"  Epoch {epoch:3d}: Train {train_acc:.1f}%, Test+TTA {test_acc:.1f}%\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"  Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, best_acc\n",
    "\n",
    "def evaluate_with_tta(model, test_loader):\n",
    "    \"\"\"Test-Time Augmentation evaluation\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Original\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs1 = model(images)\n",
    "            else:\n",
    "                outputs1 = model(images)\n",
    "            \n",
    "            # Horizontal flip\n",
    "            flipped = torch.flip(images, [3])\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs2 = model(flipped)\n",
    "            else:\n",
    "                outputs2 = model(flipped)\n",
    "            \n",
    "            # Slight scale variations\n",
    "            scaled_up = F.interpolate(images, scale_factor=1.1, mode='bilinear', align_corners=False)\n",
    "            scaled_up = F.interpolate(scaled_up, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs3 = model(scaled_up)\n",
    "            else:\n",
    "                outputs3 = model(scaled_up)\n",
    "            \n",
    "            scaled_down = F.interpolate(images, scale_factor=0.9, mode='bilinear', align_corners=False)\n",
    "            scaled_down = F.interpolate(scaled_down, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs4 = model(scaled_down)\n",
    "            else:\n",
    "                outputs4 = model(scaled_down)\n",
    "            \n",
    "            # Average all TTA predictions\n",
    "            avg_outputs = (F.softmax(outputs1, dim=1) + F.softmax(outputs2, dim=1) + \n",
    "                          F.softmax(outputs3, dim=1) + F.softmax(outputs4, dim=1)) / 4\n",
    "            _, predicted = torch.max(avg_outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "# Main execution\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"deer_age_color_only_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"COLOR-ONLY models saved to: {output_dir}\")\n",
    "\n",
    "# Load ONLY color images with preserved color information\n",
    "print(\"Loading COLOR images only...\")\n",
    "X, y = load_color_images_only(color_path)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ColorDeerDataset(X_train, y_train, augment=True)\n",
    "test_dataset = ColorDeerDataset(X_test, y_test, augment=False)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COLOR-ONLY WIDE FAMILY SURVEY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"Hypothesis: Color information will boost accuracy to 75%+\")\n",
    "print(\"Strategy: Preserve RGB, test many architectures\")\n",
    "\n",
    "# Comprehensive model families\n",
    "model_families = [\n",
    "    # Proven performers (from previous tests)\n",
    "    ('resnext50_32x4d', 'ResNeXt50'),\n",
    "    ('densenet169', 'DenseNet169'),\n",
    "    ('densenet201', 'DenseNet201'),\n",
    "    ('tf_efficientnetv2_s', 'EfficientNetV2_S'),\n",
    "    \n",
    "    # EfficientNet family\n",
    "    ('efficientnet_b0', 'EfficientNetB0'),\n",
    "    ('efficientnet_b1', 'EfficientNetB1'),\n",
    "    ('efficientnet_b2', 'EfficientNetB2'),\n",
    "    ('efficientnet_b3', 'EfficientNetB3'),\n",
    "    \n",
    "    # ResNet family\n",
    "    ('resnet50', 'ResNet50'),\n",
    "    ('resnet101', 'ResNet101'),\n",
    "    ('resnet152', 'ResNet152'),\n",
    "    \n",
    "    # DenseNet family\n",
    "    ('densenet121', 'DenseNet121'),\n",
    "    \n",
    "    # Modern architectures\n",
    "    ('convnext_tiny', 'ConvNeXt_Tiny'),\n",
    "    ('convnext_small', 'ConvNeXt_Small'),\n",
    "    \n",
    "    # Vision Transformers\n",
    "    ('vit_base_patch16_224', 'ViT_Base'),\n",
    "    ('vit_small_patch16_224', 'ViT_Small'),\n",
    "    \n",
    "    # RegNet family\n",
    "    ('regnetx_800mf', 'RegNetX_800MF'),\n",
    "    ('regnety_800mf', 'RegNetY_800MF'),\n",
    "    \n",
    "    # Additional strong performers\n",
    "    ('wide_resnet50_2', 'WideResNet50'),\n",
    "    ('mobilenetv3_large_100', 'MobileNetV3_Large'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "best_accuracy = 0\n",
    "best_family = None\n",
    "\n",
    "print(f\"Testing {len(model_families)} model families on COLOR images...\")\n",
    "\n",
    "for model_timm_name, display_name in model_families:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing {display_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        model = create_model(model_timm_name)\n",
    "        trained_model, accuracy = train_model_optimized(\n",
    "            model, train_loader, test_loader, display_name\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        acc_str = f\"{accuracy:.1f}\".replace('.', 'p')\n",
    "        model_filename = f\"{display_name}_color_{acc_str}pct.pth\"\n",
    "        model_path = os.path.join(output_dir, model_filename)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': trained_model.state_dict(),\n",
    "            'model_name': display_name,\n",
    "            'timm_name': model_timm_name,\n",
    "            'accuracy': accuracy,\n",
    "            'num_classes': 5,\n",
    "            'color_only': True\n",
    "        }, model_path)\n",
    "        \n",
    "        results.append({\n",
    "            'model': display_name,\n",
    "            'timm_name': model_timm_name,\n",
    "            'accuracy': accuracy,\n",
    "            'filename': model_filename\n",
    "        })\n",
    "        \n",
    "        status = \"TARGET ACHIEVED!\" if accuracy >= 75.0 else f\"{75.0-accuracy:.1f}% to go\"\n",
    "        print(f\"{display_name}: {accuracy:.1f}% - {status}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_family = display_name\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, trained_model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {display_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Results by family\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COLOR-ONLY RESULTS BY FAMILY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "families = {\n",
    "    'ResNet': ['ResNet50', 'ResNet101', 'ResNet152', 'ResNeXt50', 'WideResNet50'],\n",
    "    'DenseNet': ['DenseNet121', 'DenseNet169', 'DenseNet201'],\n",
    "    'EfficientNet': ['EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2', 'EfficientNetB3', 'EfficientNetV2_S'],\n",
    "    'ConvNeXt': ['ConvNeXt_Tiny', 'ConvNeXt_Small'],\n",
    "    'ViT': ['ViT_Base', 'ViT_Small'],\n",
    "    'RegNet': ['RegNetX_800MF', 'RegNetY_800MF'],\n",
    "    'Mobile': ['MobileNetV3_Large']\n",
    "}\n",
    "\n",
    "for family_name, family_models in families.items():\n",
    "    family_results = [r for r in results if r['model'] in family_models]\n",
    "    if family_results:\n",
    "        print(f\"\\n{family_name} FAMILY:\")\n",
    "        for result in family_results:\n",
    "            status = \"SUCCESS!\" if result['accuracy'] >= 75.0 else f\"{75.0-result['accuracy']:.1f}% to go\"\n",
    "            print(f\"  {result['model']:20s} - {result['accuracy']:.1f}% - {status}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OVERALL TOP PERFORMERS (COLOR-ONLY)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for i, result in enumerate(results[:15], 1):\n",
    "    status = \"TARGET ACHIEVED!\" if result['accuracy'] >= 75.0 else f\"{75.0-result['accuracy']:.1f}% to go\"\n",
    "    print(f\"{i:2d}. {result['model']:20s} - {result['accuracy']:.1f}% - {status}\")\n",
    "\n",
    "if results:\n",
    "    best_result = results[0]\n",
    "    print(f\"\\nBEST COLOR-ONLY MODEL: {best_result['model']} - {best_result['accuracy']:.1f}%\")\n",
    "    \n",
    "    if best_result['accuracy'] >= 75.0:\n",
    "        print(\"SUCCESS: COLOR INFORMATION ACHIEVED 75% TARGET!\")\n",
    "        print(\"The hypothesis was correct - color data was crucial!\")\n",
    "    else:\n",
    "        gap = 75.0 - best_result['accuracy']\n",
    "        print(f\"COLOR-ONLY RESULT: {gap:.1f}% short of 75% target\")\n",
    "        \n",
    "        # Compare to previous best (66% with mixed data)\n",
    "        improvement = best_result['accuracy'] - 66.0\n",
    "        if improvement > 0:\n",
    "            print(f\"IMPROVEMENT: +{improvement:.1f}% vs mixed color/grayscale data\")\n",
    "        else:\n",
    "            print(\"No improvement over mixed data - hypothesis incorrect\")\n",
    "\n",
    "print(f\"\\nTotal models tested: {len(results)}\")\n",
    "print(f\"All COLOR-ONLY models saved in: {output_dir}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55974fea-da66-460d-abc8-9dbfea6056a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Mixed Precision: Enabled\n",
      "================================================================================\n",
      "EFFICIENTNET-B0 COMPREHENSIVE HYPERPARAMETER OPTIMIZATION\n",
      "================================================================================\n",
      "Start time: 2025-08-02 09:49:32.963024\n",
      "Results directory: efficientnet_b0_optimization_20250802_094932\n",
      "Goal: Push beyond 72.6% to maximum possible accuracy\n",
      "Strategy: Systematic exploration of hyperparameter space\n",
      "\n",
      "Loading color images...\n",
      "Train: 288, Test: 73\n",
      "\n",
      "Generating hyperparameter configurations...\n",
      "Total configurations to test: 200\n",
      "Estimated runtime: 3000 minutes = 50.0 hours\n",
      "Trial 1: LR=0.000700, BS=20, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 18.4%, Test 15.1%\n",
      "    Epoch  15: Train 67.4%, Test 61.6%\n",
      "    Epoch  30: Train 61.1%, Test 58.9%\n",
      "    Epoch  45: Train 81.2%, Test 60.3%\n",
      "    Epoch  60: Train 80.6%, Test 60.3%\n",
      "    Epoch  75: Train 69.4%, Test 63.0%\n",
      "Trial   1/200: 63.0% - NEW BEST!\n",
      "    Time: 1.9min | Total: 2.0min | Est. remaining: 373.1min\n",
      "    Best so far: 63.0%\n",
      "\n",
      "Trial 2: LR=0.000100, BS=12, Drop=0.600, Aug=heavy\n",
      "    Epoch   0: Train 21.9%, Test 20.5%\n",
      "    Epoch  15: Train 27.8%, Test 37.0%\n",
      "    Epoch  30: Train 31.6%, Test 38.4%\n",
      "    Epoch  45: Train 29.2%, Test 37.0%\n",
      "    Epoch  60: Train 31.2%, Test 41.1%\n",
      "    Epoch  75: Train 35.1%, Test 39.7%\n",
      "Trial   2/200: 41.1% - 21.9% below best\n",
      "    Time: 2.3min | Total: 4.3min | Est. remaining: 451.9min\n",
      "    Best so far: 63.0%\n",
      "\n",
      "Trial 3: LR=0.002000, BS=12, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 22.2%, Test 11.0%\n",
      "    Epoch  15: Train 39.9%, Test 43.8%\n",
      "    Epoch  30: Train 62.8%, Test 45.2%\n",
      "    Epoch  45: Train 68.8%, Test 49.3%\n",
      "    Epoch  60: Train 78.1%, Test 50.7%\n",
      "    Epoch  75: Train 74.3%, Test 52.1%\n",
      "Trial   3/200: 52.1% - 11.0% below best\n",
      "    Time: 1.4min | Total: 5.7min | Est. remaining: 278.5min\n",
      "    Best so far: 63.0%\n",
      "\n",
      "Trial 4: LR=0.000100, BS=16, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 22.6%, Test 16.4%\n",
      "    Epoch  15: Train 17.4%, Test 16.4%\n",
      "    Epoch  30: Train 18.8%, Test 17.8%\n",
      "    Epoch  45: Train 21.2%, Test 16.4%\n",
      "    Epoch  60: Train 21.5%, Test 16.4%\n",
      "    Epoch  75: Train 24.7%, Test 17.8%\n",
      "Trial   4/200: 16.4% - 46.6% below best\n",
      "    Time: 1.8min | Total: 7.6min | Est. remaining: 361.8min\n",
      "    Best so far: 63.0%\n",
      "\n",
      "Trial 5: LR=0.001000, BS=24, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 19.4%, Test 20.5%\n",
      "    Epoch  15: Train 53.5%, Test 56.2%\n",
      "    Epoch  30: Train 63.5%, Test 56.2%\n",
      "    Epoch  45: Train 66.7%, Test 57.5%\n",
      "    Epoch  60: Train 68.1%, Test 56.2%\n",
      "    Epoch  75: Train 71.2%, Test 56.2%\n",
      "Trial   5/200: 58.9% - 4.1% below best\n",
      "    Time: 1.2min | Total: 8.7min | Est. remaining: 226.8min\n",
      "    Best so far: 63.0%\n",
      "\n",
      "Trial 6: LR=0.000300, BS=12, Drop=0.200, Aug=medium\n",
      "    Epoch   0: Train 20.5%, Test 23.3%\n",
      "    Epoch  15: Train 25.3%, Test 23.3%\n",
      "    Epoch  30: Train 24.0%, Test 27.4%\n",
      "    Epoch  45: Train 22.9%, Test 27.4%\n",
      "    Epoch  60: Train 24.0%, Test 24.7%\n",
      "    Epoch  75: Train 25.0%, Test 28.8%\n",
      "Trial   6/200: 28.8% - 34.2% below best\n",
      "    Time: 1.5min | Total: 10.2min | Est. remaining: 284.4min\n",
      "    Best so far: 63.0%\n",
      "\n",
      "Trial 7: LR=0.000500, BS=20, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 13.5%, Test 19.2%\n",
      "    Epoch  15: Train 84.0%, Test 57.5%\n",
      "    Epoch  30: Train 85.1%, Test 57.5%\n",
      "    Epoch  45: Train 84.7%, Test 58.9%\n",
      "    Epoch  60: Train 84.7%, Test 58.9%\n",
      "    Epoch  75: Train 77.8%, Test 57.5%\n",
      "Trial   7/200: 60.3% - 2.7% below best\n",
      "    Time: 1.8min | Total: 12.0min | Est. remaining: 356.1min\n",
      "    Best so far: 63.0%\n",
      "\n",
      "Trial 8: LR=0.000700, BS=12, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 19.1%, Test 34.2%\n",
      "    Epoch  15: Train 79.2%, Test 64.4%\n",
      "    Epoch  30: Train 85.4%, Test 63.0%\n",
      "    Epoch  45: Train 84.0%, Test 67.1%\n",
      "    Epoch  60: Train 82.3%, Test 67.1%\n",
      "    Epoch  75: Train 91.3%, Test 67.1%\n",
      "Trial   8/200: 64.4% - NEW BEST!\n",
      "    Time: 1.8min | Total: 13.8min | Est. remaining: 346.7min\n",
      "    Best so far: 64.4%\n",
      "\n",
      "Trial 9: LR=0.002000, BS=20, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 27.8%, Test 21.9%\n",
      "    Epoch  15: Train 91.7%, Test 64.4%\n",
      "    Epoch  30: Train 94.4%, Test 61.6%\n",
      "    Epoch  45: Train 72.9%, Test 61.6%\n",
      "    Epoch  60: Train 94.8%, Test 60.3%\n",
      "    Epoch  75: Train 97.6%, Test 58.9%\n",
      "Trial   9/200: 58.9% - 5.5% below best\n",
      "    Time: 2.6min | Total: 16.4min | Est. remaining: 493.4min\n",
      "    Best so far: 64.4%\n",
      "\n",
      "Trial 10: LR=0.000300, BS=24, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 21.2%, Test 28.8%\n",
      "    Epoch  15: Train 61.1%, Test 50.7%\n",
      "    Epoch  30: Train 68.1%, Test 49.3%\n",
      "    Epoch  45: Train 63.2%, Test 53.4%\n",
      "    Epoch  60: Train 67.4%, Test 52.1%\n",
      "    Epoch  75: Train 63.9%, Test 52.1%\n",
      "Trial  10/200: 52.1% - 12.3% below best\n",
      "    Time: 1.9min | Total: 18.3min | Est. remaining: 351.8min\n",
      "    Best so far: 64.4%\n",
      "    Checkpoint saved at trial 10\n",
      "\n",
      "Trial 11: LR=0.000700, BS=24, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 24.0%, Test 21.9%\n",
      "    Epoch  15: Train 80.9%, Test 60.3%\n",
      "    Epoch  30: Train 83.3%, Test 61.6%\n",
      "    Epoch  45: Train 85.4%, Test 61.6%\n",
      "    Epoch  60: Train 82.3%, Test 61.6%\n",
      "    Epoch  75: Train 66.0%, Test 63.0%\n",
      "Trial  11/200: 60.3% - 4.1% below best\n",
      "    Time: 1.2min | Total: 19.5min | Est. remaining: 227.6min\n",
      "    Best so far: 64.4%\n",
      "\n",
      "Trial 12: LR=0.000700, BS=16, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 21.2%, Test 12.3%\n",
      "    Epoch  15: Train 20.8%, Test 17.8%\n",
      "    Epoch  30: Train 30.2%, Test 21.9%\n",
      "    Epoch  45: Train 32.6%, Test 26.0%\n",
      "    Epoch  60: Train 35.4%, Test 27.4%\n",
      "    Epoch  75: Train 36.8%, Test 28.8%\n",
      "Trial  12/200: 27.4% - 37.0% below best\n",
      "    Time: 1.1min | Total: 20.6min | Est. remaining: 215.9min\n",
      "    Best so far: 64.4%\n",
      "\n",
      "Trial 13: LR=0.001000, BS=12, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 18.4%, Test 20.5%\n",
      "    Epoch  15: Train 23.3%, Test 23.3%\n",
      "    Epoch  30: Train 30.6%, Test 32.9%\n",
      "    Epoch  45: Train 33.7%, Test 35.6%\n",
      "    Epoch  60: Train 30.2%, Test 34.2%\n",
      "    Epoch  75: Train 34.0%, Test 37.0%\n",
      "Trial  13/200: 34.2% - 30.1% below best\n",
      "    Time: 2.0min | Total: 22.7min | Est. remaining: 375.8min\n",
      "    Best so far: 64.4%\n",
      "\n",
      "Trial 14: LR=0.000500, BS=24, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 18.4%, Test 20.5%\n",
      "    Epoch  15: Train 21.5%, Test 23.3%\n",
      "    Epoch  30: Train 24.7%, Test 31.5%\n",
      "    Epoch  45: Train 25.3%, Test 37.0%\n",
      "    Epoch  60: Train 29.2%, Test 37.0%\n",
      "    Epoch  75: Train 22.9%, Test 32.9%\n",
      "Trial  14/200: 35.6% - 28.8% below best\n",
      "    Time: 1.7min | Total: 24.3min | Est. remaining: 310.9min\n",
      "    Best so far: 64.4%\n",
      "\n",
      "Trial 15: LR=0.000700, BS=24, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 23.3%, Test 12.3%\n",
      "    Epoch  15: Train 70.1%, Test 58.9%\n",
      "    Epoch  30: Train 75.3%, Test 54.8%\n",
      "    Epoch  45: Train 63.2%, Test 56.2%\n",
      "    Epoch  60: Train 92.4%, Test 60.3%\n",
      "    Epoch  75: Train 89.9%, Test 57.5%\n",
      "Trial  15/200: 57.5% - 6.8% below best\n",
      "    Time: 1.4min | Total: 25.7min | Est. remaining: 252.8min\n",
      "    Best so far: 64.4%\n",
      "\n",
      "Trial 16: LR=0.002000, BS=20, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 25.0%, Test 35.6%\n",
      "    Epoch  15: Train 85.1%, Test 56.2%\n",
      "    Epoch  30: Train 86.1%, Test 61.6%\n",
      "    Epoch  45: Train 89.2%, Test 56.2%\n",
      "    Epoch  60: Train 91.0%, Test 54.8%\n",
      "    Epoch  75: Train 94.8%, Test 58.9%\n",
      "Trial  16/200: 57.5% - 6.8% below best\n",
      "    Time: 1.8min | Total: 27.5min | Est. remaining: 326.1min\n",
      "    Best so far: 64.4%\n",
      "\n",
      "Trial 17: LR=0.001000, BS=20, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 19.4%, Test 24.7%\n",
      "    Epoch  15: Train 92.0%, Test 65.8%\n",
      "    Epoch  30: Train 92.7%, Test 64.4%\n",
      "    Epoch  45: Train 87.2%, Test 61.6%\n",
      "    Epoch  60: Train 94.1%, Test 65.8%\n",
      "    Epoch  75: Train 85.8%, Test 65.8%\n",
      "Trial  17/200: 67.1% - NEW BEST!\n",
      "    Time: 1.5min | Total: 29.0min | Est. remaining: 276.4min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 18: LR=0.001000, BS=20, Drop=0.400, Aug=medium\n",
      "    Epoch   0: Train 19.1%, Test 16.4%\n",
      "    Epoch  15: Train 62.2%, Test 53.4%\n",
      "    Epoch  30: Train 75.7%, Test 61.6%\n",
      "    Epoch  45: Train 71.5%, Test 58.9%\n",
      "    Epoch  60: Train 73.3%, Test 58.9%\n",
      "    Epoch  75: Train 75.0%, Test 60.3%\n",
      "Trial  18/200: 61.6% - 5.5% below best\n",
      "    Time: 1.4min | Total: 30.4min | Est. remaining: 261.7min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 19: LR=0.001000, BS=12, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 21.5%, Test 28.8%\n",
      "    Epoch  15: Train 54.2%, Test 54.8%\n",
      "    Epoch  30: Train 53.1%, Test 57.5%\n",
      "    Epoch  45: Train 66.7%, Test 58.9%\n",
      "    Epoch  60: Train 77.8%, Test 61.6%\n",
      "    Epoch  75: Train 70.8%, Test 63.0%\n",
      "Trial  19/200: 64.4% - 2.7% below best\n",
      "    Time: 1.9min | Total: 32.3min | Est. remaining: 342.1min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 20: LR=0.000700, BS=20, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 19.8%, Test 13.7%\n",
      "    Epoch  15: Train 59.7%, Test 54.8%\n",
      "    Epoch  30: Train 90.3%, Test 57.5%\n",
      "    Epoch  45: Train 75.7%, Test 61.6%\n",
      "    Epoch  60: Train 88.5%, Test 58.9%\n",
      "    Epoch  75: Train 95.8%, Test 60.3%\n",
      "Trial  20/200: 58.9% - 8.2% below best\n",
      "    Time: 1.1min | Total: 33.4min | Est. remaining: 204.2min\n",
      "    Best so far: 67.1%\n",
      "    Checkpoint saved at trial 20\n",
      "\n",
      "Trial 21: LR=0.000500, BS=20, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 25.7%, Test 41.1%\n",
      "    Epoch  15: Train 81.6%, Test 57.5%\n",
      "    Epoch  30: Train 59.0%, Test 64.4%\n",
      "    Epoch  45: Train 68.8%, Test 58.9%\n",
      "    Epoch  60: Train 81.2%, Test 60.3%\n",
      "    Epoch  75: Train 70.8%, Test 56.2%\n",
      "Trial  21/200: 63.0% - 4.1% below best\n",
      "    Time: 2.2min | Total: 35.6min | Est. remaining: 392.4min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 22: LR=0.000500, BS=12, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 27.1%, Test 20.5%\n",
      "    Epoch  15: Train 59.7%, Test 49.3%\n",
      "    Epoch  30: Train 74.7%, Test 56.2%\n",
      "    Epoch  45: Train 74.0%, Test 57.5%\n",
      "    Epoch  60: Train 77.1%, Test 58.9%\n",
      "    Epoch  75: Train 77.4%, Test 61.6%\n",
      "Trial  22/200: 60.3% - 6.8% below best\n",
      "    Time: 1.6min | Total: 37.2min | Est. remaining: 278.1min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 23: LR=0.002000, BS=24, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 18.1%, Test 17.8%\n",
      "    Epoch  15: Train 28.1%, Test 43.8%\n",
      "    Epoch  30: Train 38.2%, Test 50.7%\n",
      "    Epoch  45: Train 35.4%, Test 49.3%\n",
      "    Epoch  60: Train 49.3%, Test 50.7%\n",
      "    Epoch  75: Train 39.6%, Test 49.3%\n",
      "Trial  23/200: 50.7% - 16.4% below best\n",
      "    Time: 1.7min | Total: 38.9min | Est. remaining: 304.9min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 24: LR=0.000300, BS=16, Drop=0.600, Aug=heavy\n",
      "    Epoch   0: Train 24.0%, Test 24.7%\n",
      "    Epoch  15: Train 31.2%, Test 39.7%\n",
      "    Epoch  30: Train 33.7%, Test 42.5%\n",
      "    Epoch  45: Train 35.4%, Test 45.2%\n",
      "    Epoch  60: Train 33.3%, Test 42.5%\n",
      "    Epoch  75: Train 34.4%, Test 43.8%\n",
      "Trial  24/200: 39.7% - 27.4% below best\n",
      "    Time: 1.9min | Total: 40.8min | Est. remaining: 334.0min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 25: LR=0.000500, BS=16, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 21.2%, Test 20.5%\n",
      "    Epoch  15: Train 89.2%, Test 60.3%\n",
      "    Epoch  30: Train 88.9%, Test 58.9%\n",
      "    Epoch  45: Train 89.6%, Test 58.9%\n",
      "    Epoch  60: Train 88.5%, Test 60.3%\n",
      "    Epoch  75: Train 92.4%, Test 60.3%\n",
      "Trial  25/200: 61.6% - 5.5% below best\n",
      "    Time: 2.7min | Total: 43.5min | Est. remaining: 469.8min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 26: LR=0.000500, BS=20, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 22.2%, Test 19.2%\n",
      "    Epoch  15: Train 26.0%, Test 26.0%\n",
      "    Epoch  30: Train 23.3%, Test 31.5%\n",
      "    Epoch  45: Train 30.6%, Test 28.8%\n",
      "    Epoch  60: Train 30.2%, Test 31.5%\n",
      "    Epoch  75: Train 36.1%, Test 34.2%\n",
      "Trial  26/200: 31.5% - 35.6% below best\n",
      "    Time: 1.0min | Total: 44.5min | Est. remaining: 170.6min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 27: LR=0.000700, BS=20, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 17.4%, Test 19.2%\n",
      "    Epoch  15: Train 26.0%, Test 21.9%\n",
      "    Epoch  30: Train 31.2%, Test 24.7%\n",
      "    Epoch  45: Train 29.5%, Test 24.7%\n",
      "    Epoch  60: Train 34.4%, Test 26.0%\n",
      "    Epoch  75: Train 34.7%, Test 30.1%\n",
      "Trial  27/200: 32.9% - 34.2% below best\n",
      "    Time: 1.2min | Total: 45.6min | Est. remaining: 199.3min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 28: LR=0.000700, BS=16, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 20.5%, Test 28.8%\n",
      "    Epoch  15: Train 64.2%, Test 63.0%\n",
      "    Epoch  30: Train 76.0%, Test 67.1%\n",
      "    Epoch  45: Train 84.7%, Test 63.0%\n",
      "    Epoch  60: Train 84.7%, Test 64.4%\n",
      "    Epoch  75: Train 78.1%, Test 64.4%\n",
      "Trial  28/200: 61.6% - 5.5% below best\n",
      "    Time: 1.8min | Total: 47.4min | Est. remaining: 305.5min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 29: LR=0.000500, BS=24, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 20.8%, Test 16.4%\n",
      "    Epoch  15: Train 17.0%, Test 19.2%\n",
      "    Epoch  30: Train 19.4%, Test 23.3%\n",
      "    Epoch  45: Train 25.3%, Test 27.4%\n",
      "    Epoch  60: Train 20.5%, Test 26.0%\n",
      "    Epoch  75: Train 25.7%, Test 27.4%\n",
      "Trial  29/200: 27.4% - 39.7% below best\n",
      "    Time: 1.1min | Total: 48.5min | Est. remaining: 180.5min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 30: LR=0.002000, BS=12, Drop=0.200, Aug=medium\n",
      "    Epoch   0: Train 22.2%, Test 13.7%\n",
      "    Epoch  15: Train 79.5%, Test 60.3%\n",
      "    Epoch  30: Train 92.0%, Test 64.4%\n",
      "    Epoch  45: Train 86.5%, Test 64.4%\n",
      "    Epoch  60: Train 88.9%, Test 65.8%\n",
      "    Epoch  75: Train 86.8%, Test 63.0%\n",
      "Trial  30/200: 63.0% - 4.1% below best\n",
      "    Time: 2.9min | Total: 51.3min | Est. remaining: 488.3min\n",
      "    Best so far: 67.1%\n",
      "    Checkpoint saved at trial 30\n",
      "\n",
      "Trial 31: LR=0.000500, BS=24, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 24.3%, Test 45.2%\n",
      "    Epoch  15: Train 90.3%, Test 58.9%\n",
      "    Epoch  30: Train 73.3%, Test 54.8%\n",
      "    Epoch  45: Train 84.7%, Test 67.1%\n",
      "    Epoch  60: Train 99.7%, Test 65.8%\n",
      "    Epoch  75: Train 85.4%, Test 65.8%\n",
      "Trial  31/200: 65.8% - 1.4% below best\n",
      "    Time: 1.7min | Total: 53.0min | Est. remaining: 280.8min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 32: LR=0.000100, BS=24, Drop=0.400, Aug=medium\n",
      "    Epoch   0: Train 18.8%, Test 16.4%\n",
      "    Epoch  15: Train 39.9%, Test 42.5%\n",
      "    Epoch  30: Train 48.3%, Test 43.8%\n",
      "    Epoch  45: Train 50.0%, Test 46.6%\n",
      "    Epoch  60: Train 50.7%, Test 49.3%\n",
      "    Epoch  75: Train 44.1%, Test 47.9%\n",
      "Trial  32/200: 46.6% - 20.5% below best\n",
      "    Time: 1.1min | Total: 54.1min | Est. remaining: 184.5min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 33: LR=0.001000, BS=20, Drop=0.200, Aug=medium\n",
      "    Epoch   0: Train 21.9%, Test 27.4%\n",
      "    Epoch  15: Train 26.7%, Test 31.5%\n",
      "    Epoch  30: Train 30.6%, Test 34.2%\n",
      "    Epoch  45: Train 44.4%, Test 38.4%\n",
      "    Epoch  60: Train 46.9%, Test 39.7%\n",
      "    Epoch  75: Train 43.1%, Test 37.0%\n",
      "Trial  33/200: 38.4% - 28.8% below best\n",
      "    Time: 1.4min | Total: 55.5min | Est. remaining: 232.6min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 34: LR=0.001000, BS=12, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 21.2%, Test 19.2%\n",
      "    Epoch  15: Train 78.8%, Test 60.3%\n",
      "    Epoch  30: Train 89.6%, Test 60.3%\n",
      "    Epoch  45: Train 91.7%, Test 63.0%\n",
      "    Epoch  60: Train 81.9%, Test 57.5%\n",
      "    Epoch  75: Train 89.6%, Test 57.5%\n",
      "Trial  34/200: 57.5% - 9.6% below best\n",
      "    Time: 1.3min | Total: 56.8min | Est. remaining: 223.4min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 35: LR=0.001000, BS=16, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 23.3%, Test 30.1%\n",
      "    Epoch  15: Train 78.5%, Test 61.6%\n",
      "    Epoch  30: Train 90.3%, Test 63.0%\n",
      "    Epoch  45: Train 83.3%, Test 61.6%\n",
      "    Epoch  60: Train 89.9%, Test 61.6%\n",
      "    Epoch  75: Train 88.2%, Test 61.6%\n",
      "Trial  35/200: 61.6% - 5.5% below best\n",
      "    Time: 1.6min | Total: 58.4min | Est. remaining: 257.4min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 36: LR=0.000100, BS=12, Drop=0.400, Aug=medium\n",
      "    Epoch   0: Train 19.1%, Test 19.2%\n",
      "    Epoch  15: Train 80.2%, Test 53.4%\n",
      "    Epoch  30: Train 79.9%, Test 57.5%\n",
      "    Epoch  45: Train 72.6%, Test 58.9%\n",
      "    Epoch  60: Train 76.4%, Test 58.9%\n",
      "    Epoch  75: Train 75.0%, Test 56.2%\n",
      "Trial  36/200: 56.2% - 11.0% below best\n",
      "    Time: 2.5min | Total: 60.9min | Est. remaining: 417.1min\n",
      "    Best so far: 67.1%\n",
      "\n",
      "Trial 37: LR=0.002000, BS=24, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 21.2%, Test 20.5%\n",
      "    Epoch  15: Train 68.1%, Test 67.1%\n",
      "    Epoch  30: Train 91.7%, Test 68.5%\n",
      "    Epoch  45: Train 96.5%, Test 68.5%\n",
      "    Epoch  60: Train 91.7%, Test 71.2%\n",
      "    Epoch  75: Train 79.9%, Test 69.9%\n",
      "Trial  37/200: 69.9% - NEW BEST!\n",
      "    Time: 1.0min | Total: 61.9min | Est. remaining: 161.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 38: LR=0.000500, BS=12, Drop=0.400, Aug=medium\n",
      "    Epoch   0: Train 22.6%, Test 37.0%\n",
      "    Epoch  15: Train 80.6%, Test 54.8%\n",
      "    Epoch  30: Train 79.5%, Test 58.9%\n",
      "    Epoch  45: Train 78.5%, Test 58.9%\n",
      "    Epoch  60: Train 87.2%, Test 56.2%\n",
      "    Epoch  75: Train 79.9%, Test 56.2%\n",
      "Trial  38/200: 58.9% - 11.0% below best\n",
      "    Time: 2.7min | Total: 64.6min | Est. remaining: 436.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 39: LR=0.000100, BS=20, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 17.4%, Test 30.1%\n",
      "    Epoch  15: Train 78.1%, Test 58.9%\n",
      "    Epoch  30: Train 91.0%, Test 58.9%\n",
      "    Epoch  45: Train 78.5%, Test 56.2%\n",
      "    Epoch  60: Train 79.5%, Test 60.3%\n",
      "    Epoch  75: Train 72.6%, Test 61.6%\n",
      "Trial  39/200: 58.9% - 11.0% below best\n",
      "    Time: 2.3min | Total: 66.9min | Est. remaining: 362.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 40: LR=0.002000, BS=20, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 22.9%, Test 38.4%\n",
      "    Epoch  15: Train 85.8%, Test 56.2%\n",
      "    Epoch  30: Train 82.6%, Test 65.8%\n",
      "    Epoch  45: Train 74.7%, Test 68.5%\n",
      "    Epoch  60: Train 77.4%, Test 67.1%\n",
      "    Epoch  75: Train 89.2%, Test 67.1%\n",
      "Trial  40/200: 67.1% - 2.7% below best\n",
      "    Time: 2.1min | Total: 69.0min | Est. remaining: 337.7min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 40\n",
      "\n",
      "Trial 41: LR=0.000100, BS=24, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 17.4%, Test 17.8%\n",
      "    Epoch  15: Train 18.1%, Test 17.8%\n",
      "    Epoch  30: Train 19.1%, Test 24.7%\n",
      "    Epoch  45: Train 18.4%, Test 23.3%\n",
      "    Epoch  60: Train 17.4%, Test 21.9%\n",
      "    Epoch  75: Train 16.7%, Test 23.3%\n",
      "Trial  41/200: 26.0% - 43.8% below best\n",
      "    Time: 1.0min | Total: 70.0min | Est. remaining: 153.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 42: LR=0.000100, BS=12, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 14.6%, Test 6.8%\n",
      "    Epoch  15: Train 59.0%, Test 43.8%\n",
      "    Epoch  30: Train 68.4%, Test 57.5%\n",
      "    Epoch  45: Train 77.1%, Test 56.2%\n",
      "    Epoch  60: Train 83.0%, Test 56.2%\n",
      "    Epoch  75: Train 58.0%, Test 60.3%\n",
      "Trial  42/200: 56.2% - 13.7% below best\n",
      "    Time: 2.7min | Total: 72.6min | Est. remaining: 420.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 43: LR=0.000300, BS=24, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 22.6%, Test 20.5%\n",
      "    Epoch  15: Train 26.7%, Test 24.7%\n",
      "    Epoch  30: Train 21.5%, Test 27.4%\n",
      "    Epoch  45: Train 24.0%, Test 26.0%\n",
      "    Epoch  60: Train 26.4%, Test 30.1%\n",
      "    Epoch  75: Train 22.6%, Test 26.0%\n",
      "Trial  43/200: 24.7% - 45.2% below best\n",
      "    Time: 1.7min | Total: 74.3min | Est. remaining: 266.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 44: LR=0.001000, BS=24, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 31.6%, Test 37.0%\n",
      "    Epoch  15: Train 74.7%, Test 63.0%\n",
      "    Epoch  30: Train 89.6%, Test 60.3%\n",
      "    Epoch  45: Train 83.7%, Test 58.9%\n",
      "    Epoch  60: Train 99.7%, Test 60.3%\n",
      "    Epoch  75: Train 87.8%, Test 60.3%\n",
      "Trial  44/200: 61.6% - 8.2% below best\n",
      "    Time: 2.0min | Total: 76.4min | Est. remaining: 318.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 45: LR=0.000300, BS=12, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 20.8%, Test 26.0%\n",
      "    Epoch  15: Train 74.7%, Test 65.8%\n",
      "    Epoch  30: Train 83.3%, Test 65.8%\n",
      "    Epoch  45: Train 90.3%, Test 63.0%\n",
      "    Epoch  60: Train 89.9%, Test 67.1%\n",
      "    Epoch  75: Train 91.7%, Test 63.0%\n",
      "Trial  45/200: 65.8% - 4.1% below best\n",
      "    Time: 3.2min | Total: 79.6min | Est. remaining: 498.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 46: LR=0.000100, BS=12, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 21.2%, Test 20.5%\n",
      "    Epoch  15: Train 21.5%, Test 30.1%\n",
      "    Epoch  30: Train 17.7%, Test 30.1%\n",
      "    Epoch  45: Train 20.1%, Test 28.8%\n",
      "    Epoch  60: Train 29.5%, Test 28.8%\n",
      "    Epoch  75: Train 24.3%, Test 34.2%\n",
      "Trial  46/200: 27.4% - 42.5% below best\n",
      "    Time: 1.6min | Total: 81.2min | Est. remaining: 242.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 47: LR=0.000100, BS=20, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 22.9%, Test 17.8%\n",
      "    Epoch  15: Train 27.8%, Test 31.5%\n",
      "    Epoch  30: Train 46.2%, Test 50.7%\n",
      "    Epoch  45: Train 52.1%, Test 49.3%\n",
      "    Epoch  60: Train 49.7%, Test 50.7%\n",
      "    Epoch  75: Train 46.9%, Test 50.7%\n",
      "Trial  47/200: 52.1% - 17.8% below best\n",
      "    Time: 1.1min | Total: 82.3min | Est. remaining: 175.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 48: LR=0.002000, BS=24, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 18.1%, Test 20.5%\n",
      "    Epoch  15: Train 29.9%, Test 26.0%\n",
      "    Epoch  30: Train 33.7%, Test 35.6%\n",
      "    Epoch  45: Train 40.3%, Test 38.4%\n",
      "    Epoch  60: Train 39.9%, Test 43.8%\n",
      "    Epoch  75: Train 46.5%, Test 41.1%\n",
      "Trial  48/200: 43.8% - 26.0% below best\n",
      "    Time: 1.7min | Total: 84.0min | Est. remaining: 260.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 49: LR=0.000700, BS=16, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 23.3%, Test 16.4%\n",
      "    Epoch  15: Train 26.0%, Test 23.3%\n",
      "    Epoch  30: Train 24.0%, Test 21.9%\n",
      "    Epoch  45: Train 25.0%, Test 23.3%\n",
      "    Epoch  60: Train 22.2%, Test 21.9%\n",
      "    Epoch  75: Train 29.2%, Test 23.3%\n",
      "Trial  49/200: 24.7% - 45.2% below best\n",
      "    Time: 1.5min | Total: 85.5min | Est. remaining: 228.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 50: LR=0.000100, BS=24, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 22.9%, Test 17.8%\n",
      "    Epoch  15: Train 19.1%, Test 17.8%\n",
      "    Epoch  30: Train 17.0%, Test 16.4%\n",
      "    Epoch  45: Train 19.8%, Test 16.4%\n",
      "    Epoch  60: Train 19.8%, Test 16.4%\n",
      "    Epoch  75: Train 19.8%, Test 17.8%\n",
      "Trial  50/200: 16.4% - 53.4% below best\n",
      "    Time: 1.3min | Total: 86.8min | Est. remaining: 197.3min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 50\n",
      "\n",
      "Trial 51: LR=0.000300, BS=16, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 24.3%, Test 39.7%\n",
      "    Epoch  15: Train 94.1%, Test 58.9%\n",
      "    Epoch  30: Train 99.0%, Test 64.4%\n",
      "    Epoch  45: Train 95.1%, Test 64.4%\n",
      "    Epoch  60: Train 90.6%, Test 63.0%\n",
      "    Epoch  75: Train 92.4%, Test 63.0%\n",
      "Trial  51/200: 61.6% - 8.2% below best\n",
      "    Time: 2.3min | Total: 89.1min | Est. remaining: 337.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 52: LR=0.000100, BS=24, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 18.1%, Test 26.0%\n",
      "    Epoch  15: Train 29.2%, Test 35.6%\n",
      "    Epoch  30: Train 33.7%, Test 42.5%\n",
      "    Epoch  45: Train 35.1%, Test 41.1%\n",
      "    Epoch  60: Train 34.7%, Test 43.8%\n",
      "    Epoch  75: Train 36.5%, Test 46.6%\n",
      "Trial  52/200: 46.6% - 23.3% below best\n",
      "    Time: 1.7min | Total: 90.8min | Est. remaining: 251.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 53: LR=0.001000, BS=12, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 21.2%, Test 23.3%\n",
      "    Epoch  15: Train 75.7%, Test 57.5%\n",
      "    Epoch  30: Train 83.0%, Test 58.9%\n",
      "    Epoch  45: Train 90.3%, Test 61.6%\n",
      "    Epoch  60: Train 91.7%, Test 60.3%\n",
      "    Epoch  75: Train 84.0%, Test 63.0%\n",
      "Trial  53/200: 63.0% - 6.8% below best\n",
      "    Time: 2.1min | Total: 92.9min | Est. remaining: 312.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 54: LR=0.002000, BS=16, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 23.3%, Test 19.2%\n",
      "    Epoch  15: Train 27.4%, Test 30.1%\n",
      "    Epoch  30: Train 31.9%, Test 35.6%\n",
      "    Epoch  45: Train 33.7%, Test 38.4%\n",
      "    Epoch  60: Train 35.1%, Test 42.5%\n",
      "    Epoch  75: Train 34.0%, Test 39.7%\n",
      "Trial  54/200: 41.1% - 28.8% below best\n",
      "    Time: 1.6min | Total: 94.5min | Est. remaining: 227.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 55: LR=0.000700, BS=20, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 19.1%, Test 17.8%\n",
      "    Epoch  15: Train 61.8%, Test 52.1%\n",
      "    Epoch  30: Train 66.3%, Test 54.8%\n",
      "    Epoch  45: Train 67.7%, Test 61.6%\n",
      "    Epoch  60: Train 73.6%, Test 64.4%\n",
      "    Epoch  75: Train 68.8%, Test 67.1%\n",
      "Trial  55/200: 67.1% - 2.7% below best\n",
      "    Time: 1.9min | Total: 96.4min | Est. remaining: 279.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 56: LR=0.000100, BS=12, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 20.8%, Test 30.1%\n",
      "    Epoch  15: Train 54.9%, Test 50.7%\n",
      "    Epoch  30: Train 68.8%, Test 60.3%\n",
      "    Epoch  45: Train 63.5%, Test 61.6%\n",
      "    Epoch  60: Train 80.6%, Test 64.4%\n",
      "    Epoch  75: Train 69.8%, Test 65.8%\n",
      "Trial  56/200: 61.6% - 8.2% below best\n",
      "    Time: 2.4min | Total: 98.8min | Est. remaining: 346.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 57: LR=0.000100, BS=20, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 17.7%, Test 12.3%\n",
      "    Epoch  15: Train 29.2%, Test 21.9%\n",
      "    Epoch  30: Train 33.3%, Test 28.8%\n",
      "    Epoch  45: Train 41.7%, Test 31.5%\n",
      "    Epoch  60: Train 34.7%, Test 34.2%\n",
      "    Epoch  75: Train 39.6%, Test 38.4%\n",
      "Trial  57/200: 32.9% - 37.0% below best\n",
      "    Time: 1.5min | Total: 100.3min | Est. remaining: 216.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 58: LR=0.002000, BS=12, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 21.2%, Test 13.7%\n",
      "    Epoch  15: Train 66.0%, Test 56.2%\n",
      "    Epoch  30: Train 75.7%, Test 61.6%\n",
      "    Epoch  45: Train 86.8%, Test 58.9%\n",
      "    Epoch  60: Train 92.7%, Test 61.6%\n",
      "    Epoch  75: Train 92.0%, Test 65.8%\n",
      "Trial  58/200: 63.0% - 6.8% below best\n",
      "    Time: 2.0min | Total: 102.3min | Est. remaining: 283.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 59: LR=0.001000, BS=12, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 24.3%, Test 38.4%\n",
      "    Epoch  15: Train 73.6%, Test 61.6%\n",
      "    Epoch  30: Train 85.4%, Test 61.6%\n",
      "    Epoch  45: Train 85.1%, Test 61.6%\n",
      "    Epoch  60: Train 70.5%, Test 65.8%\n",
      "    Epoch  75: Train 83.3%, Test 61.6%\n",
      "Trial  59/200: 63.0% - 6.8% below best\n",
      "    Time: 2.9min | Total: 105.3min | Est. remaining: 411.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 60: LR=0.000700, BS=16, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 19.1%, Test 35.6%\n",
      "    Epoch  15: Train 56.9%, Test 56.2%\n",
      "    Epoch  30: Train 63.9%, Test 60.3%\n",
      "    Epoch  45: Train 70.1%, Test 58.9%\n",
      "    Epoch  60: Train 87.8%, Test 64.4%\n",
      "    Epoch  75: Train 69.8%, Test 61.6%\n",
      "Trial  60/200: 60.3% - 9.6% below best\n",
      "    Time: 1.9min | Total: 107.2min | Est. remaining: 269.0min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 60\n",
      "\n",
      "Trial 61: LR=0.002000, BS=16, Drop=0.600, Aug=heavy\n",
      "    Epoch   0: Train 25.3%, Test 19.2%\n",
      "    Epoch  15: Train 30.6%, Test 42.5%\n",
      "    Epoch  30: Train 39.6%, Test 41.1%\n",
      "    Epoch  45: Train 36.1%, Test 42.5%\n",
      "    Epoch  60: Train 44.1%, Test 47.9%\n",
      "    Epoch  75: Train 46.9%, Test 45.2%\n",
      "Trial  61/200: 45.2% - 24.7% below best\n",
      "    Time: 1.5min | Total: 108.7min | Est. remaining: 205.4min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 62: LR=0.000100, BS=16, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 21.5%, Test 30.1%\n",
      "    Epoch  15: Train 61.1%, Test 54.8%\n",
      "    Epoch  30: Train 69.8%, Test 57.5%\n",
      "    Epoch  45: Train 71.2%, Test 61.6%\n",
      "    Epoch  60: Train 77.8%, Test 61.6%\n",
      "    Epoch  75: Train 88.9%, Test 61.6%\n",
      "Trial  62/200: 60.3% - 9.6% below best\n",
      "    Time: 2.1min | Total: 110.8min | Est. remaining: 292.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 63: LR=0.001000, BS=16, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 22.2%, Test 23.3%\n",
      "    Epoch  15: Train 26.7%, Test 30.1%\n",
      "    Epoch  30: Train 27.4%, Test 38.4%\n",
      "    Epoch  45: Train 43.4%, Test 45.2%\n",
      "    Epoch  60: Train 39.2%, Test 47.9%\n",
      "    Epoch  75: Train 27.4%, Test 45.2%\n",
      "Trial  63/200: 45.2% - 24.7% below best\n",
      "    Time: 1.8min | Total: 112.6min | Est. remaining: 243.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 64: LR=0.002000, BS=16, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 22.6%, Test 26.0%\n",
      "    Epoch  15: Train 67.4%, Test 17.8%\n",
      "    Epoch  30: Train 66.3%, Test 39.7%\n",
      "    Epoch  45: Train 67.4%, Test 34.2%\n",
      "    Epoch  60: Train 71.9%, Test 43.8%\n",
      "    Epoch  75: Train 89.9%, Test 52.1%\n",
      "Trial  64/200: 49.3% - 20.5% below best\n",
      "    Time: 2.2min | Total: 114.7min | Est. remaining: 293.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 65: LR=0.001000, BS=24, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 26.7%, Test 35.6%\n",
      "    Epoch  15: Train 86.5%, Test 60.3%\n",
      "    Epoch  30: Train 72.6%, Test 65.8%\n",
      "    Epoch  45: Train 87.2%, Test 68.5%\n",
      "    Epoch  60: Train 86.5%, Test 65.8%\n",
      "    Epoch  75: Train 77.8%, Test 65.8%\n",
      "Trial  65/200: 64.4% - 5.5% below best\n",
      "    Time: 2.0min | Total: 116.8min | Est. remaining: 272.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 66: LR=0.000100, BS=12, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 20.1%, Test 19.2%\n",
      "    Epoch  15: Train 22.9%, Test 20.5%\n",
      "    Epoch  30: Train 33.3%, Test 31.5%\n",
      "    Epoch  45: Train 40.3%, Test 31.5%\n",
      "    Epoch  60: Train 39.6%, Test 41.1%\n",
      "    Epoch  75: Train 41.3%, Test 37.0%\n",
      "Trial  66/200: 38.4% - 31.5% below best\n",
      "    Time: 2.1min | Total: 118.8min | Est. remaining: 279.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 67: LR=0.000100, BS=24, Drop=0.400, Aug=medium\n",
      "    Epoch   0: Train 21.5%, Test 24.7%\n",
      "    Epoch  15: Train 20.1%, Test 26.0%\n",
      "    Epoch  30: Train 14.2%, Test 24.7%\n",
      "    Epoch  45: Train 22.9%, Test 24.7%\n",
      "    Epoch  60: Train 19.1%, Test 24.7%\n",
      "    Epoch  75: Train 22.2%, Test 24.7%\n",
      "Trial  67/200: 26.0% - 43.8% below best\n",
      "    Time: 1.3min | Total: 120.2min | Est. remaining: 178.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 68: LR=0.000100, BS=12, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 21.2%, Test 26.0%\n",
      "    Epoch  15: Train 48.3%, Test 53.4%\n",
      "    Epoch  30: Train 60.1%, Test 57.5%\n",
      "    Epoch  45: Train 53.8%, Test 54.8%\n",
      "    Epoch  60: Train 49.7%, Test 52.1%\n",
      "    Epoch  75: Train 58.7%, Test 54.8%\n",
      "Trial  68/200: 53.4% - 16.4% below best\n",
      "    Time: 2.5min | Total: 122.7min | Est. remaining: 331.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 69: LR=0.000500, BS=16, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 22.2%, Test 26.0%\n",
      "    Epoch  15: Train 27.8%, Test 27.4%\n",
      "    Epoch  30: Train 33.0%, Test 26.0%\n",
      "    Epoch  45: Train 31.2%, Test 27.4%\n",
      "    Epoch  60: Train 26.7%, Test 26.0%\n",
      "    Epoch  75: Train 29.5%, Test 28.8%\n",
      "Trial  69/200: 28.8% - 41.1% below best\n",
      "    Time: 1.3min | Total: 124.0min | Est. remaining: 176.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 70: LR=0.002000, BS=24, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 24.7%, Test 21.9%\n",
      "    Epoch  15: Train 62.8%, Test 56.2%\n",
      "    Epoch  30: Train 89.6%, Test 53.4%\n",
      "    Epoch  45: Train 84.7%, Test 54.8%\n",
      "    Epoch  60: Train 78.5%, Test 54.8%\n",
      "    Epoch  75: Train 75.0%, Test 54.8%\n",
      "Trial  70/200: 54.8% - 15.1% below best\n",
      "    Time: 1.1min | Total: 125.1min | Est. remaining: 137.7min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 70\n",
      "\n",
      "Trial 71: LR=0.000500, BS=16, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 19.1%, Test 27.4%\n",
      "    Epoch  15: Train 71.2%, Test 58.9%\n",
      "    Epoch  30: Train 81.6%, Test 60.3%\n",
      "    Epoch  45: Train 85.4%, Test 61.6%\n",
      "    Epoch  60: Train 78.5%, Test 60.3%\n",
      "    Epoch  75: Train 79.2%, Test 61.6%\n",
      "Trial  71/200: 63.0% - 6.8% below best\n",
      "    Time: 2.4min | Total: 127.5min | Est. remaining: 304.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 72: LR=0.001000, BS=12, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 27.4%, Test 41.1%\n",
      "    Epoch  15: Train 79.5%, Test 50.7%\n",
      "    Epoch  30: Train 85.8%, Test 64.4%\n",
      "    Epoch  45: Train 83.3%, Test 65.8%\n",
      "    Epoch  60: Train 75.3%, Test 63.0%\n",
      "    Epoch  75: Train 74.3%, Test 65.8%\n",
      "Trial  72/200: 65.8% - 4.1% below best\n",
      "    Time: 2.5min | Total: 130.0min | Est. remaining: 319.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 73: LR=0.000500, BS=16, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 16.7%, Test 26.0%\n",
      "    Epoch  15: Train 53.8%, Test 45.2%\n",
      "    Epoch  30: Train 60.8%, Test 52.1%\n",
      "    Epoch  45: Train 66.7%, Test 56.2%\n",
      "    Epoch  60: Train 72.2%, Test 58.9%\n",
      "    Epoch  75: Train 69.4%, Test 58.9%\n",
      "Trial  73/200: 58.9% - 11.0% below best\n",
      "    Time: 1.6min | Total: 131.6min | Est. remaining: 202.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 74: LR=0.000300, BS=12, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 22.9%, Test 38.4%\n",
      "    Epoch  15: Train 74.0%, Test 64.4%\n",
      "    Epoch  30: Train 67.0%, Test 65.8%\n",
      "    Epoch  45: Train 76.7%, Test 63.0%\n",
      "    Epoch  60: Train 64.9%, Test 63.0%\n",
      "    Epoch  75: Train 69.4%, Test 65.8%\n",
      "Trial  74/200: 63.0% - 6.8% below best\n",
      "    Time: 2.9min | Total: 134.4min | Est. remaining: 361.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 75: LR=0.000100, BS=12, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 27.1%, Test 31.5%\n",
      "    Epoch  15: Train 56.6%, Test 58.9%\n",
      "    Epoch  30: Train 78.5%, Test 58.9%\n",
      "    Epoch  45: Train 82.6%, Test 63.0%\n",
      "    Epoch  60: Train 72.2%, Test 60.3%\n",
      "    Epoch  75: Train 72.2%, Test 63.0%\n",
      "Trial  75/200: 58.9% - 11.0% below best\n",
      "    Time: 3.0min | Total: 137.5min | Est. remaining: 380.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 76: LR=0.002000, BS=20, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 21.9%, Test 21.9%\n",
      "    Epoch  15: Train 30.9%, Test 35.6%\n",
      "    Epoch  30: Train 46.5%, Test 42.5%\n",
      "    Epoch  45: Train 48.6%, Test 42.5%\n",
      "    Epoch  60: Train 57.6%, Test 43.8%\n",
      "    Epoch  75: Train 48.6%, Test 46.6%\n",
      "Trial  76/200: 42.5% - 27.4% below best\n",
      "    Time: 1.4min | Total: 138.9min | Est. remaining: 174.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 77: LR=0.000100, BS=16, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 19.4%, Test 16.4%\n",
      "    Epoch  15: Train 21.9%, Test 23.3%\n",
      "    Epoch  30: Train 22.9%, Test 23.3%\n",
      "    Epoch  45: Train 24.0%, Test 20.5%\n",
      "    Epoch  60: Train 23.6%, Test 23.3%\n",
      "    Epoch  75: Train 26.4%, Test 23.3%\n",
      "Trial  77/200: 27.4% - 42.5% below best\n",
      "    Time: 1.2min | Total: 140.1min | Est. remaining: 150.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 78: LR=0.000100, BS=12, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 21.5%, Test 17.8%\n",
      "    Epoch  15: Train 25.0%, Test 16.4%\n",
      "    Epoch  30: Train 24.3%, Test 16.4%\n",
      "    Epoch  45: Train 21.5%, Test 16.4%\n",
      "    Epoch  60: Train 22.2%, Test 13.7%\n",
      "    Epoch  75: Train 26.7%, Test 15.1%\n",
      "Trial  78/200: 19.2% - 50.7% below best\n",
      "    Time: 1.6min | Total: 141.7min | Est. remaining: 201.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 79: LR=0.002000, BS=16, Drop=0.600, Aug=heavy\n",
      "    Epoch   0: Train 23.6%, Test 34.2%\n",
      "    Epoch  15: Train 88.5%, Test 60.3%\n",
      "    Epoch  30: Train 77.8%, Test 57.5%\n",
      "    Epoch  45: Train 89.6%, Test 61.6%\n",
      "    Epoch  60: Train 97.9%, Test 57.5%\n",
      "    Epoch  75: Train 86.5%, Test 58.9%\n",
      "Trial  79/200: 58.9% - 11.0% below best\n",
      "    Time: 2.7min | Total: 144.4min | Est. remaining: 323.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 80: LR=0.000100, BS=24, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 18.1%, Test 17.8%\n",
      "    Epoch  15: Train 46.9%, Test 53.4%\n",
      "    Epoch  30: Train 72.9%, Test 63.0%\n",
      "    Epoch  45: Train 70.1%, Test 60.3%\n",
      "    Epoch  60: Train 74.0%, Test 61.6%\n",
      "    Epoch  75: Train 59.0%, Test 60.3%\n",
      "Trial  80/200: 61.6% - 8.2% below best\n",
      "    Time: 1.1min | Total: 145.5min | Est. remaining: 128.3min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 80\n",
      "\n",
      "Trial 81: LR=0.000300, BS=24, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 28.1%, Test 39.7%\n",
      "    Epoch  15: Train 96.2%, Test 60.3%\n",
      "    Epoch  30: Train 95.8%, Test 64.4%\n",
      "    Epoch  45: Train 92.7%, Test 65.8%\n",
      "    Epoch  60: Train 75.7%, Test 61.6%\n",
      "    Epoch  75: Train 90.6%, Test 61.6%\n",
      "Trial  81/200: 63.0% - 6.8% below best\n",
      "    Time: 1.9min | Total: 147.4min | Est. remaining: 223.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 82: LR=0.001000, BS=24, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 21.5%, Test 19.2%\n",
      "    Epoch  15: Train 21.5%, Test 20.5%\n",
      "    Epoch  30: Train 23.6%, Test 24.7%\n",
      "    Epoch  45: Train 24.7%, Test 24.7%\n",
      "    Epoch  60: Train 21.9%, Test 27.4%\n",
      "    Epoch  75: Train 22.9%, Test 26.0%\n",
      "Trial  82/200: 23.3% - 46.6% below best\n",
      "    Time: 1.3min | Total: 148.7min | Est. remaining: 159.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 83: LR=0.000700, BS=20, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 22.6%, Test 27.4%\n",
      "    Epoch  15: Train 20.5%, Test 24.7%\n",
      "    Epoch  30: Train 26.0%, Test 28.8%\n",
      "    Epoch  45: Train 27.8%, Test 32.9%\n",
      "    Epoch  60: Train 31.2%, Test 30.1%\n",
      "    Epoch  75: Train 28.1%, Test 32.9%\n",
      "Trial  83/200: 34.2% - 35.6% below best\n",
      "    Time: 1.1min | Total: 149.9min | Est. remaining: 134.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 84: LR=0.000700, BS=24, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 20.5%, Test 15.1%\n",
      "    Epoch  15: Train 76.4%, Test 49.3%\n",
      "    Epoch  30: Train 85.4%, Test 63.0%\n",
      "    Epoch  45: Train 92.4%, Test 58.9%\n",
      "    Epoch  60: Train 93.1%, Test 61.6%\n",
      "    Epoch  75: Train 83.7%, Test 58.9%\n",
      "Trial  84/200: 57.5% - 12.3% below best\n",
      "    Time: 1.9min | Total: 151.7min | Est. remaining: 218.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 85: LR=0.000300, BS=20, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 22.2%, Test 27.4%\n",
      "    Epoch  15: Train 39.2%, Test 43.8%\n",
      "    Epoch  30: Train 49.7%, Test 56.2%\n",
      "    Epoch  45: Train 61.5%, Test 56.2%\n",
      "    Epoch  60: Train 67.0%, Test 58.9%\n",
      "    Epoch  75: Train 68.4%, Test 61.6%\n",
      "Trial  85/200: 57.5% - 12.3% below best\n",
      "    Time: 1.8min | Total: 153.6min | Est. remaining: 209.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 86: LR=0.000700, BS=24, Drop=0.600, Aug=heavy\n",
      "    Epoch   0: Train 22.6%, Test 15.1%\n",
      "    Epoch  15: Train 17.4%, Test 30.1%\n",
      "    Epoch  30: Train 24.7%, Test 32.9%\n",
      "    Epoch  45: Train 26.4%, Test 35.6%\n",
      "    Epoch  60: Train 26.0%, Test 38.4%\n",
      "    Epoch  75: Train 24.3%, Test 37.0%\n",
      "Trial  86/200: 37.0% - 32.9% below best\n",
      "    Time: 1.4min | Total: 154.9min | Est. remaining: 155.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 87: LR=0.002000, BS=16, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 27.1%, Test 15.1%\n",
      "    Epoch  15: Train 94.8%, Test 53.4%\n",
      "    Epoch  30: Train 91.0%, Test 56.2%\n",
      "    Epoch  45: Train 74.3%, Test 60.3%\n",
      "    Epoch  60: Train 91.3%, Test 58.9%\n",
      "    Epoch  75: Train 92.7%, Test 58.9%\n",
      "Trial  87/200: 57.5% - 12.3% below best\n",
      "    Time: 2.4min | Total: 157.3min | Est. remaining: 269.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 88: LR=0.002000, BS=16, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 19.1%, Test 41.1%\n",
      "    Epoch  15: Train 85.4%, Test 54.8%\n",
      "    Epoch  30: Train 92.0%, Test 58.9%\n",
      "    Epoch  45: Train 85.1%, Test 57.5%\n",
      "    Epoch  60: Train 80.6%, Test 57.5%\n",
      "    Epoch  75: Train 97.6%, Test 58.9%\n",
      "Trial  88/200: 58.9% - 11.0% below best\n",
      "    Time: 1.1min | Total: 158.4min | Est. remaining: 127.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 89: LR=0.000500, BS=12, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 28.1%, Test 39.7%\n",
      "    Epoch  15: Train 88.5%, Test 56.2%\n",
      "    Epoch  30: Train 83.3%, Test 63.0%\n",
      "    Epoch  45: Train 94.8%, Test 60.3%\n",
      "    Epoch  60: Train 94.8%, Test 57.5%\n",
      "    Epoch  75: Train 80.9%, Test 57.5%\n",
      "Trial  89/200: 57.5% - 12.3% below best\n",
      "    Time: 2.8min | Total: 161.2min | Est. remaining: 305.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 90: LR=0.001000, BS=12, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 22.6%, Test 19.2%\n",
      "    Epoch  15: Train 25.7%, Test 28.8%\n",
      "    Epoch  30: Train 31.2%, Test 30.1%\n",
      "    Epoch  45: Train 24.7%, Test 30.1%\n",
      "    Epoch  60: Train 34.7%, Test 32.9%\n",
      "    Epoch  75: Train 30.6%, Test 32.9%\n",
      "Trial  90/200: 32.9% - 37.0% below best\n",
      "    Time: 1.4min | Total: 162.6min | Est. remaining: 155.7min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 90\n",
      "\n",
      "Trial 91: LR=0.000500, BS=20, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 24.7%, Test 23.3%\n",
      "    Epoch  15: Train 80.2%, Test 56.2%\n",
      "    Epoch  30: Train 92.7%, Test 56.2%\n",
      "    Epoch  45: Train 92.4%, Test 56.2%\n",
      "    Epoch  60: Train 92.0%, Test 54.8%\n",
      "    Epoch  75: Train 77.4%, Test 56.2%\n",
      "Trial  91/200: 56.2% - 13.7% below best\n",
      "    Time: 1.5min | Total: 164.1min | Est. remaining: 162.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 92: LR=0.001000, BS=16, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 20.5%, Test 26.0%\n",
      "    Epoch  15: Train 26.4%, Test 24.7%\n",
      "    Epoch  30: Train 30.2%, Test 27.4%\n",
      "    Epoch  45: Train 34.7%, Test 35.6%\n",
      "    Epoch  60: Train 37.5%, Test 35.6%\n",
      "    Epoch  75: Train 39.6%, Test 37.0%\n",
      "Trial  92/200: 39.7% - 30.1% below best\n",
      "    Time: 1.3min | Total: 165.4min | Est. remaining: 137.4min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 93: LR=0.001000, BS=24, Drop=0.200, Aug=medium\n",
      "    Epoch   0: Train 22.6%, Test 47.9%\n",
      "    Epoch  15: Train 67.0%, Test 60.3%\n",
      "    Epoch  30: Train 95.8%, Test 63.0%\n",
      "    Epoch  45: Train 87.8%, Test 60.3%\n",
      "    Epoch  60: Train 88.9%, Test 61.6%\n",
      "    Epoch  75: Train 86.5%, Test 61.6%\n",
      "Trial  93/200: 61.6% - 8.2% below best\n",
      "    Time: 1.8min | Total: 167.1min | Est. remaining: 187.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 94: LR=0.001000, BS=24, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 22.2%, Test 34.2%\n",
      "    Epoch  15: Train 35.4%, Test 56.2%\n",
      "    Epoch  30: Train 64.6%, Test 61.6%\n",
      "    Epoch  45: Train 61.8%, Test 54.8%\n",
      "    Epoch  60: Train 61.5%, Test 56.2%\n",
      "    Epoch  75: Train 49.3%, Test 57.5%\n",
      "Trial  94/200: 56.2% - 13.7% below best\n",
      "    Time: 1.1min | Total: 168.3min | Est. remaining: 118.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 95: LR=0.000700, BS=24, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 25.7%, Test 27.4%\n",
      "    Epoch  15: Train 76.0%, Test 53.4%\n",
      "    Epoch  30: Train 78.1%, Test 56.2%\n",
      "    Epoch  45: Train 83.0%, Test 54.8%\n",
      "    Epoch  60: Train 79.5%, Test 54.8%\n",
      "    Epoch  75: Train 69.4%, Test 53.4%\n",
      "Trial  95/200: 57.5% - 12.3% below best\n",
      "    Time: 1.9min | Total: 170.2min | Est. remaining: 202.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 96: LR=0.000700, BS=24, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 17.0%, Test 21.9%\n",
      "    Epoch  15: Train 87.2%, Test 54.8%\n",
      "    Epoch  30: Train 81.9%, Test 57.5%\n",
      "    Epoch  45: Train 96.9%, Test 63.0%\n",
      "    Epoch  60: Train 81.6%, Test 63.0%\n",
      "    Epoch  75: Train 83.7%, Test 63.0%\n",
      "Trial  96/200: 64.4% - 5.5% below best\n",
      "    Time: 1.8min | Total: 172.0min | Est. remaining: 183.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 97: LR=0.000300, BS=12, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 18.1%, Test 13.7%\n",
      "    Epoch  15: Train 19.4%, Test 9.6%\n",
      "    Epoch  30: Train 24.3%, Test 20.5%\n",
      "    Epoch  45: Train 28.1%, Test 17.8%\n",
      "    Epoch  60: Train 27.4%, Test 21.9%\n",
      "    Epoch  75: Train 27.1%, Test 21.9%\n",
      "Trial  97/200: 16.4% - 53.4% below best\n",
      "    Time: 2.0min | Total: 173.9min | Est. remaining: 204.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 98: LR=0.002000, BS=20, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 22.6%, Test 37.0%\n",
      "    Epoch  15: Train 58.3%, Test 61.6%\n",
      "    Epoch  30: Train 97.6%, Test 68.5%\n",
      "    Epoch  45: Train 79.5%, Test 68.5%\n",
      "    Epoch  60: Train 80.2%, Test 65.8%\n",
      "    Epoch  75: Train 89.6%, Test 67.1%\n",
      "Trial  98/200: 68.5% - 1.4% below best\n",
      "    Time: 1.8min | Total: 175.7min | Est. remaining: 183.4min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 99: LR=0.001000, BS=12, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 25.3%, Test 42.5%\n",
      "    Epoch  15: Train 67.0%, Test 67.1%\n",
      "    Epoch  30: Train 80.9%, Test 63.0%\n",
      "    Epoch  45: Train 76.7%, Test 67.1%\n",
      "    Epoch  60: Train 76.7%, Test 72.6%\n",
      "    Epoch  75: Train 66.7%, Test 68.5%\n",
      "Trial  99/200: 69.9% - 0.0% below best\n",
      "    Time: 1.8min | Total: 177.6min | Est. remaining: 186.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 100: LR=0.000100, BS=24, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 19.8%, Test 26.0%\n",
      "    Epoch  15: Train 77.8%, Test 57.5%\n",
      "    Epoch  30: Train 87.2%, Test 58.9%\n",
      "    Epoch  45: Train 87.2%, Test 61.6%\n",
      "    Epoch  60: Train 88.9%, Test 61.6%\n",
      "    Epoch  75: Train 74.7%, Test 61.6%\n",
      "Trial 100/200: 60.3% - 9.6% below best\n",
      "    Time: 1.9min | Total: 179.5min | Est. remaining: 187.5min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 100\n",
      "\n",
      "Trial 101: LR=0.000100, BS=12, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 22.9%, Test 15.1%\n",
      "    Epoch  15: Train 58.7%, Test 54.8%\n",
      "    Epoch  30: Train 77.8%, Test 57.5%\n",
      "    Epoch  45: Train 74.3%, Test 53.4%\n",
      "    Epoch  60: Train 71.5%, Test 56.2%\n",
      "    Epoch  75: Train 69.1%, Test 53.4%\n",
      "Trial 101/200: 53.4% - 16.4% below best\n",
      "    Time: 2.3min | Total: 181.8min | Est. remaining: 230.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 102: LR=0.000300, BS=16, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 21.9%, Test 19.2%\n",
      "    Epoch  15: Train 21.5%, Test 15.1%\n",
      "    Epoch  30: Train 20.1%, Test 15.1%\n",
      "    Epoch  45: Train 27.8%, Test 23.3%\n",
      "    Epoch  60: Train 19.4%, Test 21.9%\n",
      "    Epoch  75: Train 27.4%, Test 20.5%\n",
      "Trial 102/200: 16.4% - 53.4% below best\n",
      "    Time: 1.3min | Total: 183.1min | Est. remaining: 125.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 103: LR=0.001000, BS=12, Drop=0.200, Aug=medium\n",
      "    Epoch   0: Train 22.2%, Test 21.9%\n",
      "    Epoch  15: Train 72.6%, Test 54.8%\n",
      "    Epoch  30: Train 81.2%, Test 57.5%\n",
      "    Epoch  45: Train 87.8%, Test 61.6%\n",
      "    Epoch  60: Train 89.9%, Test 58.9%\n",
      "    Epoch  75: Train 88.2%, Test 60.3%\n",
      "Trial 103/200: 58.9% - 11.0% below best\n",
      "    Time: 1.7min | Total: 184.8min | Est. remaining: 168.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 104: LR=0.001000, BS=16, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 17.0%, Test 21.9%\n",
      "    Epoch  15: Train 57.3%, Test 64.4%\n",
      "    Epoch  30: Train 80.9%, Test 64.4%\n",
      "    Epoch  45: Train 70.8%, Test 60.3%\n",
      "    Epoch  60: Train 71.9%, Test 61.6%\n",
      "    Epoch  75: Train 74.0%, Test 61.6%\n",
      "Trial 104/200: 64.4% - 5.5% below best\n",
      "    Time: 1.6min | Total: 186.4min | Est. remaining: 156.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 105: LR=0.000300, BS=20, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 17.0%, Test 17.8%\n",
      "    Epoch  15: Train 66.0%, Test 56.2%\n",
      "    Epoch  30: Train 79.5%, Test 56.2%\n",
      "    Epoch  45: Train 82.6%, Test 58.9%\n",
      "    Epoch  60: Train 84.7%, Test 60.3%\n",
      "    Epoch  75: Train 88.9%, Test 60.3%\n",
      "Trial 105/200: 57.5% - 12.3% below best\n",
      "    Time: 2.4min | Total: 188.9min | Est. remaining: 231.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 106: LR=0.000100, BS=20, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 16.7%, Test 26.0%\n",
      "    Epoch  15: Train 57.6%, Test 56.2%\n",
      "    Epoch  30: Train 66.7%, Test 57.5%\n",
      "    Epoch  45: Train 63.2%, Test 60.3%\n",
      "    Epoch  60: Train 59.7%, Test 56.2%\n",
      "    Epoch  75: Train 59.4%, Test 57.5%\n",
      "Trial 106/200: 58.9% - 11.0% below best\n",
      "    Time: 2.2min | Total: 191.1min | Est. remaining: 206.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 107: LR=0.000300, BS=12, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 24.3%, Test 39.7%\n",
      "    Epoch  15: Train 83.0%, Test 61.6%\n",
      "    Epoch  30: Train 79.2%, Test 60.3%\n",
      "    Epoch  45: Train 87.2%, Test 58.9%\n",
      "    Epoch  60: Train 83.3%, Test 60.3%\n",
      "    Epoch  75: Train 93.1%, Test 58.9%\n",
      "Trial 107/200: 60.3% - 9.6% below best\n",
      "    Time: 2.8min | Total: 193.9min | Est. remaining: 263.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 108: LR=0.000500, BS=16, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 20.1%, Test 27.4%\n",
      "    Epoch  15: Train 68.4%, Test 57.5%\n",
      "    Epoch  30: Train 75.7%, Test 61.6%\n",
      "    Epoch  45: Train 82.3%, Test 65.8%\n",
      "    Epoch  60: Train 85.4%, Test 63.0%\n",
      "    Epoch  75: Train 73.6%, Test 64.4%\n",
      "Trial 108/200: 64.4% - 5.5% below best\n",
      "    Time: 1.6min | Total: 195.5min | Est. remaining: 150.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 109: LR=0.000700, BS=20, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 20.1%, Test 26.0%\n",
      "    Epoch  15: Train 23.6%, Test 32.9%\n",
      "    Epoch  30: Train 24.3%, Test 37.0%\n",
      "    Epoch  45: Train 33.3%, Test 38.4%\n",
      "    Epoch  60: Train 31.6%, Test 39.7%\n",
      "    Epoch  75: Train 37.2%, Test 38.4%\n",
      "Trial 109/200: 43.8% - 26.0% below best\n",
      "    Time: 1.2min | Total: 196.8min | Est. remaining: 110.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 110: LR=0.000700, BS=24, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 20.5%, Test 28.8%\n",
      "    Epoch  15: Train 31.2%, Test 26.0%\n",
      "    Epoch  30: Train 32.6%, Test 30.1%\n",
      "    Epoch  45: Train 28.8%, Test 27.4%\n",
      "    Epoch  60: Train 27.1%, Test 24.7%\n",
      "    Epoch  75: Train 28.8%, Test 28.8%\n",
      "Trial 110/200: 26.0% - 43.8% below best\n",
      "    Time: 1.0min | Total: 197.8min | Est. remaining: 92.4min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 110\n",
      "\n",
      "Trial 111: LR=0.000700, BS=24, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 18.1%, Test 19.2%\n",
      "    Epoch  15: Train 90.6%, Test 50.7%\n",
      "    Epoch  30: Train 86.8%, Test 57.5%\n",
      "    Epoch  45: Train 92.7%, Test 57.5%\n",
      "    Epoch  60: Train 89.2%, Test 58.9%\n",
      "    Epoch  75: Train 92.0%, Test 60.3%\n",
      "Trial 111/200: 57.5% - 12.3% below best\n",
      "    Time: 1.6min | Total: 199.4min | Est. remaining: 139.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 112: LR=0.001000, BS=12, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 22.9%, Test 32.9%\n",
      "    Epoch  15: Train 83.3%, Test 60.3%\n",
      "    Epoch  30: Train 86.5%, Test 64.4%\n",
      "    Epoch  45: Train 89.2%, Test 61.6%\n",
      "    Epoch  60: Train 84.0%, Test 63.0%\n",
      "    Epoch  75: Train 81.9%, Test 64.4%\n",
      "Trial 112/200: 63.0% - 6.8% below best\n",
      "    Time: 3.2min | Total: 202.5min | Est. remaining: 278.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 113: LR=0.001000, BS=16, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 20.1%, Test 27.4%\n",
      "    Epoch  15: Train 26.0%, Test 20.5%\n",
      "    Epoch  30: Train 25.3%, Test 20.5%\n",
      "    Epoch  45: Train 28.5%, Test 21.9%\n",
      "    Epoch  60: Train 30.2%, Test 21.9%\n",
      "    Epoch  75: Train 26.7%, Test 23.3%\n",
      "Trial 113/200: 23.3% - 46.6% below best\n",
      "    Time: 1.5min | Total: 204.0min | Est. remaining: 127.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 114: LR=0.000300, BS=16, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 19.1%, Test 20.5%\n",
      "    Epoch  15: Train 74.3%, Test 53.4%\n",
      "    Epoch  30: Train 83.3%, Test 58.9%\n",
      "    Epoch  45: Train 84.0%, Test 57.5%\n",
      "    Epoch  60: Train 81.6%, Test 56.2%\n",
      "    Epoch  75: Train 92.4%, Test 58.9%\n",
      "Trial 114/200: 58.9% - 11.0% below best\n",
      "    Time: 1.4min | Total: 205.4min | Est. remaining: 118.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 115: LR=0.001000, BS=20, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 27.1%, Test 41.1%\n",
      "    Epoch  15: Train 92.0%, Test 54.8%\n",
      "    Epoch  30: Train 88.2%, Test 58.9%\n",
      "    Epoch  45: Train 94.1%, Test 63.0%\n",
      "    Epoch  60: Train 94.1%, Test 60.3%\n",
      "    Epoch  75: Train 100.0%, Test 61.6%\n",
      "Trial 115/200: 61.6% - 8.2% below best\n",
      "    Time: 2.2min | Total: 207.6min | Est. remaining: 189.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 116: LR=0.001000, BS=16, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 26.0%, Test 38.4%\n",
      "    Epoch  15: Train 78.1%, Test 65.8%\n",
      "    Epoch  30: Train 87.5%, Test 64.4%\n",
      "    Epoch  45: Train 87.2%, Test 71.2%\n",
      "    Epoch  60: Train 91.7%, Test 69.9%\n",
      "    Epoch  75: Train 93.8%, Test 68.5%\n",
      "Trial 116/200: 68.5% - 1.4% below best\n",
      "    Time: 1.3min | Total: 208.9min | Est. remaining: 110.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 117: LR=0.001000, BS=16, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 16.0%, Test 24.7%\n",
      "    Epoch  15: Train 73.3%, Test 57.5%\n",
      "    Epoch  30: Train 71.9%, Test 54.8%\n",
      "    Epoch  45: Train 82.3%, Test 60.3%\n",
      "    Epoch  60: Train 75.3%, Test 54.8%\n",
      "    Epoch  75: Train 84.0%, Test 56.2%\n",
      "Trial 117/200: 56.2% - 13.7% below best\n",
      "    Time: 1.6min | Total: 210.5min | Est. remaining: 132.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 118: LR=0.000100, BS=16, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 22.2%, Test 12.3%\n",
      "    Epoch  15: Train 77.4%, Test 52.1%\n",
      "    Epoch  30: Train 74.3%, Test 61.6%\n",
      "    Epoch  45: Train 87.8%, Test 58.9%\n",
      "    Epoch  60: Train 83.3%, Test 57.5%\n",
      "    Epoch  75: Train 86.1%, Test 61.6%\n",
      "Trial 118/200: 60.3% - 9.6% below best\n",
      "    Time: 2.8min | Total: 213.2min | Est. remaining: 225.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 119: LR=0.000100, BS=20, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 20.5%, Test 17.8%\n",
      "    Epoch  15: Train 22.2%, Test 20.5%\n",
      "    Epoch  30: Train 24.3%, Test 20.5%\n",
      "    Epoch  45: Train 23.6%, Test 26.0%\n",
      "    Epoch  60: Train 26.0%, Test 21.9%\n",
      "    Epoch  75: Train 20.8%, Test 28.8%\n",
      "Trial 119/200: 26.0% - 43.8% below best\n",
      "    Time: 1.4min | Total: 214.7min | Est. remaining: 117.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 120: LR=0.001000, BS=20, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 18.8%, Test 20.5%\n",
      "    Epoch  15: Train 22.6%, Test 27.4%\n",
      "    Epoch  30: Train 29.9%, Test 28.8%\n",
      "    Epoch  45: Train 32.6%, Test 32.9%\n",
      "    Epoch  60: Train 36.1%, Test 34.2%\n",
      "    Epoch  75: Train 30.6%, Test 34.2%\n",
      "Trial 120/200: 34.2% - 35.6% below best\n",
      "    Time: 1.7min | Total: 216.4min | Est. remaining: 138.0min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 120\n",
      "\n",
      "Trial 121: LR=0.000100, BS=20, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 17.4%, Test 16.4%\n",
      "    Epoch  15: Train 21.9%, Test 21.9%\n",
      "    Epoch  30: Train 20.8%, Test 23.3%\n",
      "    Epoch  45: Train 21.5%, Test 23.3%\n",
      "    Epoch  60: Train 19.8%, Test 21.9%\n",
      "    Epoch  75: Train 21.5%, Test 23.3%\n",
      "Trial 121/200: 23.3% - 46.6% below best\n",
      "    Time: 1.8min | Total: 218.2min | Est. remaining: 141.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 122: LR=0.000100, BS=20, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 22.2%, Test 13.7%\n",
      "    Epoch  15: Train 24.0%, Test 15.1%\n",
      "    Epoch  30: Train 25.0%, Test 16.4%\n",
      "    Epoch  45: Train 24.7%, Test 17.8%\n",
      "    Epoch  60: Train 25.3%, Test 12.3%\n",
      "    Epoch  75: Train 22.6%, Test 19.2%\n",
      "Trial 122/200: 19.2% - 50.7% below best\n",
      "    Time: 1.5min | Total: 219.7min | Est. remaining: 113.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 123: LR=0.000100, BS=24, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 21.5%, Test 16.4%\n",
      "    Epoch  15: Train 51.4%, Test 52.1%\n",
      "    Epoch  30: Train 57.3%, Test 57.5%\n",
      "    Epoch  45: Train 63.2%, Test 58.9%\n",
      "    Epoch  60: Train 81.9%, Test 58.9%\n",
      "    Epoch  75: Train 63.9%, Test 56.2%\n",
      "Trial 123/200: 58.9% - 11.0% below best\n",
      "    Time: 1.8min | Total: 221.5min | Est. remaining: 137.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 124: LR=0.000700, BS=12, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 21.2%, Test 28.8%\n",
      "    Epoch  15: Train 75.3%, Test 53.4%\n",
      "    Epoch  30: Train 68.1%, Test 56.2%\n",
      "    Epoch  45: Train 79.9%, Test 60.3%\n",
      "    Epoch  60: Train 79.9%, Test 57.5%\n",
      "    Epoch  75: Train 88.9%, Test 60.3%\n",
      "Trial 124/200: 57.5% - 12.3% below best\n",
      "    Time: 2.7min | Total: 224.2min | Est. remaining: 206.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 125: LR=0.000700, BS=20, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 15.3%, Test 31.5%\n",
      "    Epoch  15: Train 75.7%, Test 57.5%\n",
      "    Epoch  30: Train 83.0%, Test 63.0%\n",
      "    Epoch  45: Train 92.7%, Test 69.9%\n",
      "    Epoch  60: Train 85.8%, Test 65.8%\n",
      "    Epoch  75: Train 84.7%, Test 67.1%\n",
      "Trial 125/200: 67.1% - 2.7% below best\n",
      "    Time: 2.4min | Total: 226.6min | Est. remaining: 182.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 126: LR=0.000100, BS=24, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 21.2%, Test 19.2%\n",
      "    Epoch  15: Train 70.8%, Test 43.8%\n",
      "    Epoch  30: Train 66.0%, Test 52.1%\n",
      "    Epoch  45: Train 94.1%, Test 53.4%\n",
      "    Epoch  60: Train 96.9%, Test 53.4%\n",
      "    Epoch  75: Train 82.6%, Test 54.8%\n",
      "Trial 126/200: 56.2% - 13.7% below best\n",
      "    Time: 1.9min | Total: 228.5min | Est. remaining: 137.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 127: LR=0.000100, BS=16, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 19.1%, Test 17.8%\n",
      "    Epoch  15: Train 19.8%, Test 16.4%\n",
      "    Epoch  30: Train 22.9%, Test 20.5%\n",
      "    Epoch  45: Train 23.3%, Test 17.8%\n",
      "    Epoch  60: Train 17.7%, Test 17.8%\n",
      "    Epoch  75: Train 19.1%, Test 16.4%\n",
      "Trial 127/200: 17.8% - 52.1% below best\n",
      "    Time: 1.5min | Total: 230.0min | Est. remaining: 110.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 128: LR=0.000300, BS=24, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 21.9%, Test 17.8%\n",
      "    Epoch  15: Train 49.0%, Test 46.6%\n",
      "    Epoch  30: Train 49.3%, Test 49.3%\n",
      "    Epoch  45: Train 64.2%, Test 53.4%\n",
      "    Epoch  60: Train 75.0%, Test 52.1%\n",
      "    Epoch  75: Train 62.8%, Test 54.8%\n",
      "Trial 128/200: 54.8% - 15.1% below best\n",
      "    Time: 1.3min | Total: 231.3min | Est. remaining: 96.4min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 129: LR=0.000100, BS=16, Drop=0.400, Aug=medium\n",
      "    Epoch   0: Train 18.8%, Test 20.5%\n",
      "    Epoch  15: Train 19.4%, Test 17.8%\n",
      "    Epoch  30: Train 20.1%, Test 19.2%\n",
      "    Epoch  45: Train 20.5%, Test 20.5%\n",
      "    Epoch  60: Train 22.6%, Test 20.5%\n",
      "    Epoch  75: Train 21.9%, Test 19.2%\n",
      "Trial 129/200: 21.9% - 47.9% below best\n",
      "    Time: 1.3min | Total: 232.6min | Est. remaining: 90.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 130: LR=0.000100, BS=24, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 22.2%, Test 20.5%\n",
      "    Epoch  15: Train 18.4%, Test 15.1%\n",
      "    Epoch  30: Train 20.1%, Test 13.7%\n",
      "    Epoch  45: Train 23.6%, Test 15.1%\n",
      "    Epoch  60: Train 18.8%, Test 13.7%\n",
      "    Epoch  75: Train 23.6%, Test 13.7%\n",
      "Trial 130/200: 16.4% - 53.4% below best\n",
      "    Time: 1.7min | Total: 234.3min | Est. remaining: 116.3min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 130\n",
      "\n",
      "Trial 131: LR=0.000100, BS=12, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 20.8%, Test 31.5%\n",
      "    Epoch  15: Train 71.5%, Test 50.7%\n",
      "    Epoch  30: Train 71.9%, Test 57.5%\n",
      "    Epoch  45: Train 65.6%, Test 60.3%\n",
      "    Epoch  60: Train 78.8%, Test 60.3%\n",
      "    Epoch  75: Train 71.9%, Test 60.3%\n",
      "Trial 131/200: 57.5% - 12.3% below best\n",
      "    Time: 2.8min | Total: 237.1min | Est. remaining: 195.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 132: LR=0.000700, BS=20, Drop=0.400, Aug=medium\n",
      "    Epoch   0: Train 18.1%, Test 20.5%\n",
      "    Epoch  15: Train 25.0%, Test 24.7%\n",
      "    Epoch  30: Train 36.5%, Test 34.2%\n",
      "    Epoch  45: Train 30.9%, Test 37.0%\n",
      "    Epoch  60: Train 40.3%, Test 35.6%\n",
      "    Epoch  75: Train 34.4%, Test 38.4%\n",
      "Trial 132/200: 37.0% - 32.9% below best\n",
      "    Time: 1.4min | Total: 238.5min | Est. remaining: 96.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 133: LR=0.000700, BS=16, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 22.9%, Test 28.8%\n",
      "    Epoch  15: Train 25.3%, Test 24.7%\n",
      "    Epoch  30: Train 29.2%, Test 35.6%\n",
      "    Epoch  45: Train 30.2%, Test 28.8%\n",
      "    Epoch  60: Train 32.3%, Test 27.4%\n",
      "    Epoch  75: Train 29.9%, Test 32.9%\n",
      "Trial 133/200: 31.5% - 38.4% below best\n",
      "    Time: 1.3min | Total: 239.8min | Est. remaining: 85.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 134: LR=0.002000, BS=16, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 22.6%, Test 20.5%\n",
      "    Epoch  15: Train 37.2%, Test 43.8%\n",
      "    Epoch  30: Train 43.4%, Test 49.3%\n",
      "    Epoch  45: Train 51.7%, Test 50.7%\n",
      "    Epoch  60: Train 43.8%, Test 52.1%\n",
      "    Epoch  75: Train 44.4%, Test 52.1%\n",
      "Trial 134/200: 53.4% - 16.4% below best\n",
      "    Time: 1.6min | Total: 241.3min | Est. remaining: 102.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 135: LR=0.001000, BS=20, Drop=0.200, Aug=medium\n",
      "    Epoch   0: Train 20.1%, Test 19.2%\n",
      "    Epoch  15: Train 33.3%, Test 34.2%\n",
      "    Epoch  30: Train 31.6%, Test 35.6%\n",
      "    Epoch  45: Train 35.8%, Test 37.0%\n",
      "    Epoch  60: Train 38.9%, Test 35.6%\n",
      "    Epoch  75: Train 38.2%, Test 38.4%\n",
      "Trial 135/200: 32.9% - 37.0% below best\n",
      "    Time: 1.2min | Total: 242.5min | Est. remaining: 76.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 136: LR=0.000700, BS=16, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 17.4%, Test 20.5%\n",
      "    Epoch  15: Train 22.6%, Test 24.7%\n",
      "    Epoch  30: Train 26.7%, Test 27.4%\n",
      "    Epoch  45: Train 31.2%, Test 28.8%\n",
      "    Epoch  60: Train 30.9%, Test 32.9%\n",
      "    Epoch  75: Train 29.2%, Test 31.5%\n",
      "Trial 136/200: 32.9% - 37.0% below best\n",
      "    Time: 1.3min | Total: 243.9min | Est. remaining: 85.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 137: LR=0.000300, BS=16, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 20.5%, Test 17.8%\n",
      "    Epoch  15: Train 36.8%, Test 47.9%\n",
      "    Epoch  30: Train 51.0%, Test 53.4%\n",
      "    Epoch  45: Train 61.5%, Test 61.6%\n",
      "    Epoch  60: Train 66.0%, Test 61.6%\n",
      "    Epoch  75: Train 68.4%, Test 61.6%\n",
      "Trial 137/200: 61.6% - 8.2% below best\n",
      "    Time: 1.4min | Total: 245.3min | Est. remaining: 87.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 138: LR=0.002000, BS=20, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 26.4%, Test 31.5%\n",
      "    Epoch  15: Train 48.6%, Test 54.8%\n",
      "    Epoch  30: Train 79.5%, Test 58.9%\n",
      "    Epoch  45: Train 67.4%, Test 60.3%\n",
      "    Epoch  60: Train 63.5%, Test 57.5%\n",
      "    Epoch  75: Train 66.7%, Test 58.9%\n",
      "Trial 138/200: 60.3% - 9.6% below best\n",
      "    Time: 1.1min | Total: 246.4min | Est. remaining: 71.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 139: LR=0.000700, BS=24, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 22.6%, Test 28.8%\n",
      "    Epoch  15: Train 78.1%, Test 58.9%\n",
      "    Epoch  30: Train 79.5%, Test 57.5%\n",
      "    Epoch  45: Train 97.9%, Test 60.3%\n",
      "    Epoch  60: Train 92.4%, Test 57.5%\n",
      "    Epoch  75: Train 79.2%, Test 57.5%\n",
      "Trial 139/200: 58.9% - 11.0% below best\n",
      "    Time: 1.8min | Total: 248.2min | Est. remaining: 111.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 140: LR=0.002000, BS=20, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 24.3%, Test 30.1%\n",
      "    Epoch  15: Train 68.4%, Test 58.9%\n",
      "    Epoch  30: Train 83.3%, Test 65.8%\n",
      "    Epoch  45: Train 89.2%, Test 63.0%\n",
      "    Epoch  60: Train 73.3%, Test 64.4%\n",
      "    Epoch  75: Train 84.0%, Test 65.8%\n",
      "Trial 140/200: 58.9% - 11.0% below best\n",
      "    Time: 2.2min | Total: 250.5min | Est. remaining: 134.5min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 140\n",
      "\n",
      "Trial 141: LR=0.002000, BS=12, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 22.6%, Test 23.3%\n",
      "    Epoch  15: Train 32.3%, Test 35.6%\n",
      "    Epoch  30: Train 39.2%, Test 42.5%\n",
      "    Epoch  45: Train 39.9%, Test 45.2%\n",
      "    Epoch  60: Train 45.5%, Test 46.6%\n",
      "    Epoch  75: Train 49.0%, Test 42.5%\n",
      "Trial 141/200: 41.1% - 28.8% below best\n",
      "    Time: 1.7min | Total: 252.2min | Est. remaining: 98.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 142: LR=0.000700, BS=16, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 20.8%, Test 19.2%\n",
      "    Epoch  15: Train 61.8%, Test 58.9%\n",
      "    Epoch  30: Train 82.6%, Test 54.8%\n",
      "    Epoch  45: Train 67.0%, Test 64.4%\n",
      "    Epoch  60: Train 84.0%, Test 56.2%\n",
      "    Epoch  75: Train 75.3%, Test 58.9%\n",
      "Trial 142/200: 58.9% - 11.0% below best\n",
      "    Time: 1.3min | Total: 253.5min | Est. remaining: 76.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 143: LR=0.001000, BS=16, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 18.8%, Test 23.3%\n",
      "    Epoch  15: Train 61.8%, Test 47.9%\n",
      "    Epoch  30: Train 72.2%, Test 47.9%\n",
      "    Epoch  45: Train 77.4%, Test 47.9%\n",
      "    Epoch  60: Train 62.5%, Test 50.7%\n",
      "    Epoch  75: Train 63.2%, Test 50.7%\n",
      "Trial 143/200: 50.7% - 19.2% below best\n",
      "    Time: 1.5min | Total: 255.0min | Est. remaining: 85.4min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 144: LR=0.001000, BS=12, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 26.7%, Test 32.9%\n",
      "    Epoch  15: Train 90.3%, Test 60.3%\n",
      "    Epoch  30: Train 85.8%, Test 64.4%\n",
      "    Epoch  45: Train 89.6%, Test 60.3%\n",
      "    Epoch  60: Train 84.0%, Test 61.6%\n",
      "    Epoch  75: Train 82.3%, Test 63.0%\n",
      "Trial 144/200: 61.6% - 8.2% below best\n",
      "    Time: 2.7min | Total: 257.6min | Est. remaining: 149.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 145: LR=0.000100, BS=12, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 25.3%, Test 24.7%\n",
      "    Epoch  15: Train 85.8%, Test 58.9%\n",
      "    Epoch  30: Train 89.2%, Test 57.5%\n",
      "    Epoch  45: Train 88.9%, Test 60.3%\n",
      "    Epoch  60: Train 90.3%, Test 61.6%\n",
      "    Epoch  75: Train 89.6%, Test 63.0%\n",
      "Trial 145/200: 64.4% - 5.5% below best\n",
      "    Time: 2.4min | Total: 260.1min | Est. remaining: 133.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 146: LR=0.001000, BS=24, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 22.6%, Test 24.7%\n",
      "    Epoch  15: Train 75.0%, Test 57.5%\n",
      "    Epoch  30: Train 80.6%, Test 61.6%\n",
      "    Epoch  45: Train 83.3%, Test 60.3%\n",
      "    Epoch  60: Train 75.7%, Test 56.2%\n",
      "    Epoch  75: Train 91.3%, Test 60.3%\n",
      "Trial 146/200: 58.9% - 11.0% below best\n",
      "    Time: 1.7min | Total: 261.8min | Est. remaining: 91.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 147: LR=0.000500, BS=20, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 23.3%, Test 20.5%\n",
      "    Epoch  15: Train 30.2%, Test 28.8%\n",
      "    Epoch  30: Train 27.8%, Test 32.9%\n",
      "    Epoch  45: Train 31.2%, Test 30.1%\n",
      "    Epoch  60: Train 27.4%, Test 30.1%\n",
      "    Epoch  75: Train 28.1%, Test 30.1%\n",
      "Trial 147/200: 27.4% - 42.5% below best\n",
      "    Time: 1.1min | Total: 262.9min | Est. remaining: 60.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 148: LR=0.001000, BS=16, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 26.7%, Test 41.1%\n",
      "    Epoch  15: Train 65.3%, Test 60.3%\n",
      "    Epoch  30: Train 76.7%, Test 56.2%\n",
      "    Epoch  45: Train 76.0%, Test 58.9%\n",
      "    Epoch  60: Train 62.2%, Test 58.9%\n",
      "    Epoch  75: Train 72.6%, Test 57.5%\n",
      "Trial 148/200: 60.3% - 9.6% below best\n",
      "    Time: 2.5min | Total: 265.4min | Est. remaining: 128.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 149: LR=0.002000, BS=16, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 20.8%, Test 16.4%\n",
      "    Epoch  15: Train 84.0%, Test 54.8%\n",
      "    Epoch  30: Train 84.4%, Test 53.4%\n",
      "    Epoch  45: Train 99.0%, Test 61.6%\n",
      "    Epoch  60: Train 90.6%, Test 58.9%\n",
      "    Epoch  75: Train 97.6%, Test 61.6%\n",
      "Trial 149/200: 60.3% - 9.6% below best\n",
      "    Time: 1.9min | Total: 267.3min | Est. remaining: 98.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 150: LR=0.002000, BS=20, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 21.5%, Test 28.8%\n",
      "    Epoch  15: Train 36.1%, Test 42.5%\n",
      "    Epoch  30: Train 46.2%, Test 45.2%\n",
      "    Epoch  45: Train 45.8%, Test 49.3%\n",
      "    Epoch  60: Train 57.3%, Test 49.3%\n",
      "    Epoch  75: Train 53.1%, Test 46.6%\n",
      "Trial 150/200: 49.3% - 20.5% below best\n",
      "    Time: 1.1min | Total: 268.4min | Est. remaining: 56.3min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 150\n",
      "\n",
      "Trial 151: LR=0.000300, BS=16, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 20.8%, Test 23.3%\n",
      "    Epoch  15: Train 74.0%, Test 53.4%\n",
      "    Epoch  30: Train 87.2%, Test 61.6%\n",
      "    Epoch  45: Train 88.5%, Test 57.5%\n",
      "    Epoch  60: Train 81.9%, Test 56.2%\n",
      "    Epoch  75: Train 87.5%, Test 61.6%\n",
      "Trial 151/200: 58.9% - 11.0% below best\n",
      "    Time: 2.7min | Total: 271.1min | Est. remaining: 131.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 152: LR=0.000300, BS=24, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 22.9%, Test 16.4%\n",
      "    Epoch  15: Train 78.8%, Test 49.3%\n",
      "    Epoch  30: Train 85.1%, Test 53.4%\n",
      "    Epoch  45: Train 83.3%, Test 53.4%\n",
      "    Epoch  60: Train 89.9%, Test 53.4%\n",
      "    Epoch  75: Train 89.2%, Test 52.1%\n",
      "Trial 152/200: 50.7% - 19.2% below best\n",
      "    Time: 1.8min | Total: 272.9min | Est. remaining: 87.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 153: LR=0.001000, BS=12, Drop=0.600, Aug=heavy\n",
      "    Epoch   0: Train 20.1%, Test 27.4%\n",
      "    Epoch  15: Train 23.3%, Test 26.0%\n",
      "    Epoch  30: Train 24.0%, Test 27.4%\n",
      "    Epoch  45: Train 26.4%, Test 27.4%\n",
      "    Epoch  60: Train 25.7%, Test 27.4%\n",
      "    Epoch  75: Train 30.2%, Test 27.4%\n",
      "Trial 153/200: 30.1% - 39.7% below best\n",
      "    Time: 1.7min | Total: 274.6min | Est. remaining: 79.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 154: LR=0.000100, BS=24, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 19.4%, Test 27.4%\n",
      "    Epoch  15: Train 70.8%, Test 49.3%\n",
      "    Epoch  30: Train 54.5%, Test 60.3%\n",
      "    Epoch  45: Train 64.9%, Test 60.3%\n",
      "    Epoch  60: Train 60.8%, Test 60.3%\n",
      "    Epoch  75: Train 61.5%, Test 63.0%\n",
      "Trial 154/200: 61.6% - 8.2% below best\n",
      "    Time: 2.0min | Total: 276.6min | Est. remaining: 92.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 155: LR=0.000100, BS=20, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 17.7%, Test 19.2%\n",
      "    Epoch  15: Train 19.1%, Test 21.9%\n",
      "    Epoch  30: Train 19.4%, Test 20.5%\n",
      "    Epoch  45: Train 19.4%, Test 19.2%\n",
      "    Epoch  60: Train 19.8%, Test 19.2%\n",
      "    Epoch  75: Train 22.2%, Test 20.5%\n",
      "Trial 155/200: 24.7% - 45.2% below best\n",
      "    Time: 1.1min | Total: 277.7min | Est. remaining: 48.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 156: LR=0.000300, BS=24, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 19.4%, Test 13.7%\n",
      "    Epoch  15: Train 77.8%, Test 58.9%\n",
      "    Epoch  30: Train 93.4%, Test 61.6%\n",
      "    Epoch  45: Train 94.4%, Test 56.2%\n",
      "    Epoch  60: Train 92.7%, Test 60.3%\n",
      "    Epoch  75: Train 93.4%, Test 61.6%\n",
      "Trial 156/200: 61.6% - 8.2% below best\n",
      "    Time: 1.8min | Total: 279.5min | Est. remaining: 80.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 157: LR=0.001000, BS=24, Drop=0.400, Aug=medium\n",
      "    Epoch   0: Train 19.4%, Test 23.3%\n",
      "    Epoch  15: Train 58.3%, Test 49.3%\n",
      "    Epoch  30: Train 54.2%, Test 52.1%\n",
      "    Epoch  45: Train 63.9%, Test 56.2%\n",
      "    Epoch  60: Train 52.1%, Test 50.7%\n",
      "    Epoch  75: Train 49.3%, Test 54.8%\n",
      "Trial 157/200: 54.8% - 15.1% below best\n",
      "    Time: 1.4min | Total: 280.9min | Est. remaining: 58.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 158: LR=0.002000, BS=16, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 20.5%, Test 11.0%\n",
      "    Epoch  15: Train 42.4%, Test 37.0%\n",
      "    Epoch  30: Train 61.1%, Test 54.8%\n",
      "    Epoch  45: Train 73.3%, Test 56.2%\n",
      "    Epoch  60: Train 74.0%, Test 52.1%\n",
      "    Epoch  75: Train 67.0%, Test 53.4%\n",
      "Trial 158/200: 53.4% - 16.4% below best\n",
      "    Time: 1.2min | Total: 282.1min | Est. remaining: 51.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 159: LR=0.000500, BS=24, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 16.7%, Test 23.3%\n",
      "    Epoch  15: Train 25.3%, Test 31.5%\n",
      "    Epoch  30: Train 28.5%, Test 37.0%\n",
      "    Epoch  45: Train 28.1%, Test 38.4%\n",
      "    Epoch  60: Train 33.0%, Test 37.0%\n",
      "    Epoch  75: Train 34.7%, Test 39.7%\n",
      "Trial 159/200: 39.7% - 30.1% below best\n",
      "    Time: 1.0min | Total: 283.1min | Est. remaining: 39.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 160: LR=0.000100, BS=12, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 25.3%, Test 15.1%\n",
      "    Epoch  15: Train 66.3%, Test 46.6%\n",
      "    Epoch  30: Train 70.1%, Test 56.2%\n",
      "    Epoch  45: Train 80.9%, Test 49.3%\n",
      "    Epoch  60: Train 81.2%, Test 57.5%\n",
      "    Epoch  75: Train 79.5%, Test 53.4%\n",
      "Trial 160/200: 56.2% - 13.7% below best\n",
      "    Time: 2.6min | Total: 285.7min | Est. remaining: 104.8min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 160\n",
      "\n",
      "Trial 161: LR=0.002000, BS=16, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 30.2%, Test 23.3%\n",
      "    Epoch  15: Train 72.6%, Test 60.3%\n",
      "    Epoch  30: Train 89.2%, Test 63.0%\n",
      "    Epoch  45: Train 96.2%, Test 61.6%\n",
      "    Epoch  60: Train 97.6%, Test 60.3%\n",
      "    Epoch  75: Train 92.0%, Test 57.5%\n",
      "Trial 161/200: 60.3% - 9.6% below best\n",
      "    Time: 2.0min | Total: 287.7min | Est. remaining: 76.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 162: LR=0.000100, BS=24, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 21.5%, Test 16.4%\n",
      "    Epoch  15: Train 26.0%, Test 23.3%\n",
      "    Epoch  30: Train 26.4%, Test 28.8%\n",
      "    Epoch  45: Train 31.6%, Test 28.8%\n",
      "    Epoch  60: Train 33.7%, Test 30.1%\n",
      "    Epoch  75: Train 32.3%, Test 30.1%\n",
      "Trial 162/200: 28.8% - 41.1% below best\n",
      "    Time: 1.7min | Total: 289.4min | Est. remaining: 64.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 163: LR=0.002000, BS=20, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 19.1%, Test 17.8%\n",
      "    Epoch  15: Train 29.5%, Test 24.7%\n",
      "    Epoch  30: Train 42.7%, Test 34.2%\n",
      "    Epoch  45: Train 42.4%, Test 42.5%\n",
      "    Epoch  60: Train 47.9%, Test 47.9%\n",
      "    Epoch  75: Train 45.8%, Test 41.1%\n",
      "Trial 163/200: 46.6% - 23.3% below best\n",
      "    Time: 1.2min | Total: 290.5min | Est. remaining: 42.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 164: LR=0.002000, BS=24, Drop=0.600, Aug=medium\n",
      "    Epoch   0: Train 23.6%, Test 15.1%\n",
      "    Epoch  15: Train 29.2%, Test 32.9%\n",
      "    Epoch  30: Train 27.1%, Test 35.6%\n",
      "    Epoch  45: Train 34.0%, Test 39.7%\n",
      "    Epoch  60: Train 41.3%, Test 42.5%\n",
      "    Epoch  75: Train 32.6%, Test 43.8%\n",
      "Trial 164/200: 39.7% - 30.1% below best\n",
      "    Time: 1.3min | Total: 291.9min | Est. remaining: 48.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 165: LR=0.001000, BS=20, Drop=0.400, Aug=medium\n",
      "    Epoch   0: Train 23.6%, Test 24.7%\n",
      "    Epoch  15: Train 24.3%, Test 17.8%\n",
      "    Epoch  30: Train 26.0%, Test 17.8%\n",
      "    Epoch  45: Train 29.5%, Test 19.2%\n",
      "    Epoch  60: Train 32.6%, Test 16.4%\n",
      "    Epoch  75: Train 29.2%, Test 21.9%\n",
      "Trial 165/200: 19.2% - 50.7% below best\n",
      "    Time: 1.2min | Total: 293.0min | Est. remaining: 41.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 166: LR=0.002000, BS=12, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 19.8%, Test 19.2%\n",
      "    Epoch  15: Train 87.2%, Test 60.3%\n",
      "    Epoch  30: Train 96.9%, Test 60.3%\n",
      "    Epoch  45: Train 80.6%, Test 57.5%\n",
      "    Epoch  60: Train 99.3%, Test 64.4%\n",
      "    Epoch  75: Train 93.1%, Test 61.6%\n",
      "Trial 166/200: 64.4% - 5.5% below best\n",
      "    Time: 2.9min | Total: 295.9min | Est. remaining: 97.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 167: LR=0.002000, BS=16, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 23.6%, Test 46.6%\n",
      "    Epoch  15: Train 79.9%, Test 56.2%\n",
      "    Epoch  30: Train 85.4%, Test 63.0%\n",
      "    Epoch  45: Train 69.8%, Test 63.0%\n",
      "    Epoch  60: Train 80.6%, Test 64.4%\n",
      "    Epoch  75: Train 79.9%, Test 61.6%\n",
      "Trial 167/200: 63.0% - 6.8% below best\n",
      "    Time: 1.5min | Total: 297.4min | Est. remaining: 50.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 168: LR=0.002000, BS=16, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 21.9%, Test 21.9%\n",
      "    Epoch  15: Train 29.2%, Test 45.2%\n",
      "    Epoch  30: Train 48.3%, Test 50.7%\n",
      "    Epoch  45: Train 45.8%, Test 52.1%\n",
      "    Epoch  60: Train 47.6%, Test 56.2%\n",
      "    Epoch  75: Train 48.6%, Test 53.4%\n",
      "Trial 168/200: 52.1% - 17.8% below best\n",
      "    Time: 1.3min | Total: 298.8min | Est. remaining: 42.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 169: LR=0.002000, BS=20, Drop=0.600, Aug=heavy\n",
      "    Epoch   0: Train 20.1%, Test 27.4%\n",
      "    Epoch  15: Train 66.3%, Test 52.1%\n",
      "    Epoch  30: Train 68.4%, Test 61.6%\n",
      "    Epoch  45: Train 67.4%, Test 58.9%\n",
      "    Epoch  60: Train 87.2%, Test 60.3%\n",
      "    Epoch  75: Train 75.7%, Test 60.3%\n",
      "Trial 169/200: 58.9% - 11.0% below best\n",
      "    Time: 1.7min | Total: 300.5min | Est. remaining: 53.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 170: LR=0.000100, BS=16, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 21.2%, Test 20.5%\n",
      "    Epoch  15: Train 54.5%, Test 52.1%\n",
      "    Epoch  30: Train 70.1%, Test 56.2%\n",
      "    Epoch  45: Train 76.0%, Test 57.5%\n",
      "    Epoch  60: Train 80.6%, Test 56.2%\n",
      "    Epoch  75: Train 77.4%, Test 54.8%\n",
      "Trial 170/200: 57.5% - 12.3% below best\n",
      "    Time: 1.6min | Total: 302.0min | Est. remaining: 46.9min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 170\n",
      "\n",
      "Trial 171: LR=0.001000, BS=16, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 19.4%, Test 16.4%\n",
      "    Epoch  15: Train 29.5%, Test 32.9%\n",
      "    Epoch  30: Train 41.3%, Test 41.1%\n",
      "    Epoch  45: Train 40.6%, Test 45.2%\n",
      "    Epoch  60: Train 53.1%, Test 49.3%\n",
      "    Epoch  75: Train 52.8%, Test 50.7%\n",
      "Trial 171/200: 45.2% - 24.7% below best\n",
      "    Time: 1.5min | Total: 303.5min | Est. remaining: 42.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 172: LR=0.001000, BS=16, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 16.0%, Test 23.3%\n",
      "    Epoch  15: Train 69.1%, Test 61.6%\n",
      "    Epoch  30: Train 90.3%, Test 64.4%\n",
      "    Epoch  45: Train 81.6%, Test 61.6%\n",
      "    Epoch  60: Train 74.3%, Test 65.8%\n",
      "    Epoch  75: Train 96.5%, Test 64.4%\n",
      "Trial 172/200: 61.6% - 8.2% below best\n",
      "    Time: 1.6min | Total: 305.1min | Est. remaining: 44.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 173: LR=0.002000, BS=20, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 18.4%, Test 19.2%\n",
      "    Epoch  15: Train 26.4%, Test 28.8%\n",
      "    Epoch  30: Train 39.6%, Test 37.0%\n",
      "    Epoch  45: Train 40.6%, Test 45.2%\n",
      "    Epoch  60: Train 35.1%, Test 49.3%\n",
      "    Epoch  75: Train 35.1%, Test 47.9%\n",
      "Trial 173/200: 46.6% - 23.3% below best\n",
      "    Time: 1.0min | Total: 306.1min | Est. remaining: 27.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 174: LR=0.000700, BS=24, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 16.3%, Test 17.8%\n",
      "    Epoch  15: Train 29.9%, Test 23.3%\n",
      "    Epoch  30: Train 32.3%, Test 30.1%\n",
      "    Epoch  45: Train 37.2%, Test 34.2%\n",
      "    Epoch  60: Train 31.9%, Test 35.6%\n",
      "    Epoch  75: Train 35.1%, Test 35.6%\n",
      "Trial 174/200: 35.6% - 34.2% below best\n",
      "    Time: 1.1min | Total: 307.2min | Est. remaining: 29.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 175: LR=0.001000, BS=24, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 19.4%, Test 21.9%\n",
      "    Epoch  15: Train 84.7%, Test 60.3%\n",
      "    Epoch  30: Train 60.8%, Test 61.6%\n",
      "    Epoch  45: Train 79.9%, Test 63.0%\n",
      "    Epoch  60: Train 78.1%, Test 61.6%\n",
      "    Epoch  75: Train 72.9%, Test 61.6%\n",
      "Trial 175/200: 61.6% - 8.2% below best\n",
      "    Time: 1.8min | Total: 309.0min | Est. remaining: 44.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 176: LR=0.001000, BS=24, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 18.4%, Test 24.7%\n",
      "    Epoch  15: Train 81.2%, Test 38.4%\n",
      "    Epoch  30: Train 96.5%, Test 45.2%\n",
      "    Epoch  45: Train 79.9%, Test 54.8%\n",
      "    Epoch  60: Train 68.4%, Test 63.0%\n",
      "    Epoch  75: Train 80.6%, Test 61.6%\n",
      "Trial 176/200: 63.0% - 6.8% below best\n",
      "    Time: 1.6min | Total: 310.6min | Est. remaining: 38.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 177: LR=0.000300, BS=24, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 19.4%, Test 15.1%\n",
      "    Epoch  15: Train 23.3%, Test 17.8%\n",
      "    Epoch  30: Train 26.7%, Test 17.8%\n",
      "    Epoch  45: Train 22.9%, Test 17.8%\n",
      "    Epoch  60: Train 23.6%, Test 17.8%\n",
      "    Epoch  75: Train 25.3%, Test 16.4%\n",
      "Trial 177/200: 17.8% - 52.1% below best\n",
      "    Time: 1.1min | Total: 311.7min | Est. remaining: 24.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 178: LR=0.000700, BS=20, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 18.4%, Test 16.4%\n",
      "    Epoch  15: Train 22.9%, Test 17.8%\n",
      "    Epoch  30: Train 19.8%, Test 30.1%\n",
      "    Epoch  45: Train 29.2%, Test 26.0%\n",
      "    Epoch  60: Train 25.3%, Test 26.0%\n",
      "    Epoch  75: Train 23.6%, Test 34.2%\n",
      "Trial 178/200: 28.8% - 41.1% below best\n",
      "    Time: 1.2min | Total: 312.8min | Est. remaining: 25.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 179: LR=0.000100, BS=20, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 19.4%, Test 31.5%\n",
      "    Epoch  15: Train 19.8%, Test 26.0%\n",
      "    Epoch  30: Train 22.6%, Test 26.0%\n",
      "    Epoch  45: Train 21.2%, Test 26.0%\n",
      "    Epoch  60: Train 20.5%, Test 24.7%\n",
      "    Epoch  75: Train 20.8%, Test 26.0%\n",
      "Trial 179/200: 26.0% - 43.8% below best\n",
      "    Time: 1.2min | Total: 314.0min | Est. remaining: 25.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 180: LR=0.000500, BS=12, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 20.8%, Test 20.5%\n",
      "    Epoch  15: Train 80.9%, Test 60.3%\n",
      "    Epoch  30: Train 79.9%, Test 64.4%\n",
      "    Epoch  45: Train 93.4%, Test 69.9%\n",
      "    Epoch  60: Train 85.1%, Test 68.5%\n",
      "    Epoch  75: Train 89.9%, Test 67.1%\n",
      "Trial 180/200: 67.1% - 2.7% below best\n",
      "    Time: 2.1min | Total: 316.1min | Est. remaining: 41.4min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 180\n",
      "\n",
      "Trial 181: LR=0.002000, BS=16, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 18.1%, Test 27.4%\n",
      "    Epoch  15: Train 33.7%, Test 38.4%\n",
      "    Epoch  30: Train 32.6%, Test 41.1%\n",
      "    Epoch  45: Train 33.7%, Test 39.7%\n",
      "    Epoch  60: Train 34.0%, Test 46.6%\n",
      "    Epoch  75: Train 33.7%, Test 37.0%\n",
      "Trial 181/200: 43.8% - 26.0% below best\n",
      "    Time: 1.3min | Total: 317.4min | Est. remaining: 24.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 182: LR=0.000300, BS=24, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 16.7%, Test 24.7%\n",
      "    Epoch  15: Train 51.0%, Test 49.3%\n",
      "    Epoch  30: Train 80.2%, Test 57.5%\n",
      "    Epoch  45: Train 77.8%, Test 63.0%\n",
      "    Epoch  60: Train 80.6%, Test 63.0%\n",
      "    Epoch  75: Train 91.7%, Test 65.8%\n",
      "Trial 182/200: 60.3% - 9.6% below best\n",
      "    Time: 1.4min | Total: 318.7min | Est. remaining: 24.5min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 183: LR=0.001000, BS=12, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 27.1%, Test 45.2%\n",
      "    Epoch  15: Train 89.2%, Test 67.1%\n",
      "    Epoch  30: Train 92.0%, Test 65.8%\n",
      "    Epoch  45: Train 95.5%, Test 65.8%\n",
      "    Epoch  60: Train 95.5%, Test 67.1%\n",
      "    Epoch  75: Train 83.0%, Test 64.4%\n",
      "Trial 183/200: 64.4% - 5.5% below best\n",
      "    Time: 2.9min | Total: 321.7min | Est. remaining: 49.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 184: LR=0.002000, BS=16, Drop=0.500, Aug=light\n",
      "    Epoch   0: Train 21.2%, Test 15.1%\n",
      "    Epoch  15: Train 31.2%, Test 28.8%\n",
      "    Epoch  30: Train 28.5%, Test 30.1%\n",
      "    Epoch  45: Train 29.2%, Test 34.2%\n",
      "    Epoch  60: Train 34.0%, Test 37.0%\n",
      "    Epoch  75: Train 41.7%, Test 34.2%\n",
      "Trial 184/200: 35.6% - 34.2% below best\n",
      "    Time: 1.1min | Total: 322.8min | Est. remaining: 17.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 185: LR=0.002000, BS=20, Drop=0.300, Aug=heavy\n",
      "    Epoch   0: Train 28.5%, Test 37.0%\n",
      "    Epoch  15: Train 83.7%, Test 61.6%\n",
      "    Epoch  30: Train 82.6%, Test 61.6%\n",
      "    Epoch  45: Train 78.1%, Test 63.0%\n",
      "    Epoch  60: Train 82.3%, Test 65.8%\n",
      "    Epoch  75: Train 88.2%, Test 67.1%\n",
      "Trial 185/200: 68.5% - 1.4% below best\n",
      "    Time: 1.5min | Total: 324.2min | Est. remaining: 22.2min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 186: LR=0.000700, BS=16, Drop=0.200, Aug=medium\n",
      "    Epoch   0: Train 14.6%, Test 23.3%\n",
      "    Epoch  15: Train 23.6%, Test 32.9%\n",
      "    Epoch  30: Train 35.4%, Test 35.6%\n",
      "    Epoch  45: Train 36.5%, Test 39.7%\n",
      "    Epoch  60: Train 33.3%, Test 39.7%\n",
      "    Epoch  75: Train 34.0%, Test 42.5%\n",
      "Trial 186/200: 41.1% - 28.8% below best\n",
      "    Time: 1.3min | Total: 325.6min | Est. remaining: 18.8min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 187: LR=0.000300, BS=20, Drop=0.400, Aug=light\n",
      "    Epoch   0: Train 17.4%, Test 19.2%\n",
      "    Epoch  15: Train 77.4%, Test 52.1%\n",
      "    Epoch  30: Train 92.4%, Test 61.6%\n",
      "    Epoch  45: Train 88.9%, Test 63.0%\n",
      "    Epoch  60: Train 78.5%, Test 58.9%\n",
      "    Epoch  75: Train 81.9%, Test 58.9%\n",
      "Trial 187/200: 61.6% - 8.2% below best\n",
      "    Time: 1.8min | Total: 327.4min | Est. remaining: 23.9min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 188: LR=0.000700, BS=16, Drop=0.200, Aug=medium\n",
      "    Epoch   0: Train 18.1%, Test 19.2%\n",
      "    Epoch  15: Train 34.0%, Test 32.9%\n",
      "    Epoch  30: Train 36.5%, Test 37.0%\n",
      "    Epoch  45: Train 40.6%, Test 38.4%\n",
      "    Epoch  60: Train 40.3%, Test 41.1%\n",
      "    Epoch  75: Train 45.5%, Test 41.1%\n",
      "Trial 188/200: 41.1% - 28.8% below best\n",
      "    Time: 1.5min | Total: 328.9min | Est. remaining: 18.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 189: LR=0.000700, BS=12, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 22.9%, Test 35.6%\n",
      "    Epoch  15: Train 75.3%, Test 63.0%\n",
      "    Epoch  30: Train 81.9%, Test 64.4%\n",
      "    Epoch  45: Train 75.3%, Test 64.4%\n",
      "    Epoch  60: Train 79.5%, Test 64.4%\n",
      "    Epoch  75: Train 80.6%, Test 61.6%\n",
      "Trial 189/200: 63.0% - 6.8% below best\n",
      "    Time: 1.6min | Total: 330.6min | Est. remaining: 18.1min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 190: LR=0.001000, BS=12, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 24.7%, Test 34.2%\n",
      "    Epoch  15: Train 65.6%, Test 61.6%\n",
      "    Epoch  30: Train 62.2%, Test 57.5%\n",
      "    Epoch  45: Train 74.0%, Test 61.6%\n",
      "    Epoch  60: Train 74.0%, Test 60.3%\n",
      "    Epoch  75: Train 77.8%, Test 63.0%\n",
      "Trial 190/200: 64.4% - 5.5% below best\n",
      "    Time: 1.8min | Total: 332.4min | Est. remaining: 18.3min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 190\n",
      "\n",
      "Trial 191: LR=0.002000, BS=12, Drop=0.200, Aug=heavy\n",
      "    Epoch   0: Train 20.5%, Test 19.2%\n",
      "    Epoch  15: Train 78.1%, Test 53.4%\n",
      "    Epoch  30: Train 83.0%, Test 63.0%\n",
      "    Epoch  45: Train 78.5%, Test 65.8%\n",
      "    Epoch  60: Train 88.9%, Test 60.3%\n",
      "    Epoch  75: Train 91.0%, Test 63.0%\n",
      "Trial 191/200: 56.2% - 13.7% below best\n",
      "    Time: 2.1min | Total: 334.6min | Est. remaining: 19.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 192: LR=0.000700, BS=20, Drop=0.200, Aug=light\n",
      "    Epoch   0: Train 20.8%, Test 31.5%\n",
      "    Epoch  15: Train 80.2%, Test 52.1%\n",
      "    Epoch  30: Train 87.2%, Test 61.6%\n",
      "    Epoch  45: Train 93.1%, Test 60.3%\n",
      "    Epoch  60: Train 91.7%, Test 61.6%\n",
      "    Epoch  75: Train 99.0%, Test 60.3%\n",
      "Trial 192/200: 58.9% - 11.0% below best\n",
      "    Time: 1.9min | Total: 336.5min | Est. remaining: 15.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 193: LR=0.000700, BS=12, Drop=0.500, Aug=medium\n",
      "    Epoch   0: Train 22.2%, Test 37.0%\n",
      "    Epoch  15: Train 79.5%, Test 61.6%\n",
      "    Epoch  30: Train 78.5%, Test 60.3%\n",
      "    Epoch  45: Train 87.8%, Test 61.6%\n",
      "    Epoch  60: Train 83.0%, Test 63.0%\n",
      "    Epoch  75: Train 69.8%, Test 61.6%\n",
      "Trial 193/200: 63.0% - 6.8% below best\n",
      "    Time: 2.8min | Total: 339.3min | Est. remaining: 19.4min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 194: LR=0.000300, BS=16, Drop=0.200, Aug=medium\n",
      "    Epoch   0: Train 20.5%, Test 26.0%\n",
      "    Epoch  15: Train 80.9%, Test 50.7%\n",
      "    Epoch  30: Train 79.5%, Test 53.4%\n",
      "    Epoch  45: Train 83.7%, Test 54.8%\n",
      "    Epoch  60: Train 75.0%, Test 53.4%\n",
      "    Epoch  75: Train 86.1%, Test 54.8%\n",
      "Trial 194/200: 56.2% - 13.7% below best\n",
      "    Time: 2.4min | Total: 341.7min | Est. remaining: 14.4min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 195: LR=0.001000, BS=16, Drop=0.200, Aug=medium\n",
      "    Epoch   0: Train 16.0%, Test 15.1%\n",
      "    Epoch  15: Train 75.3%, Test 52.1%\n",
      "    Epoch  30: Train 84.7%, Test 64.4%\n",
      "    Epoch  45: Train 80.2%, Test 60.3%\n",
      "    Epoch  60: Train 89.2%, Test 58.9%\n",
      "    Epoch  75: Train 93.8%, Test 63.0%\n",
      "Trial 195/200: 61.6% - 8.2% below best\n",
      "    Time: 1.4min | Total: 343.1min | Est. remaining: 7.0min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 196: LR=0.000700, BS=24, Drop=0.300, Aug=light\n",
      "    Epoch   0: Train 22.9%, Test 38.4%\n",
      "    Epoch  15: Train 83.0%, Test 58.9%\n",
      "    Epoch  30: Train 88.5%, Test 58.9%\n",
      "    Epoch  45: Train 85.1%, Test 56.2%\n",
      "    Epoch  60: Train 88.2%, Test 60.3%\n",
      "    Epoch  75: Train 88.5%, Test 60.3%\n",
      "Trial 196/200: 60.3% - 9.6% below best\n",
      "    Time: 1.6min | Total: 344.6min | Est. remaining: 6.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 197: LR=0.001000, BS=16, Drop=0.500, Aug=heavy\n",
      "    Epoch   0: Train 18.4%, Test 26.0%\n",
      "    Epoch  15: Train 65.6%, Test 54.8%\n",
      "    Epoch  30: Train 71.2%, Test 63.0%\n",
      "    Epoch  45: Train 50.7%, Test 58.9%\n",
      "    Epoch  60: Train 80.6%, Test 61.6%\n",
      "    Epoch  75: Train 76.7%, Test 64.4%\n",
      "Trial 197/200: 63.0% - 6.8% below best\n",
      "    Time: 1.9min | Total: 346.5min | Est. remaining: 5.6min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 198: LR=0.000500, BS=24, Drop=0.600, Aug=light\n",
      "    Epoch   0: Train 24.7%, Test 28.8%\n",
      "    Epoch  15: Train 85.4%, Test 60.3%\n",
      "    Epoch  30: Train 91.0%, Test 61.6%\n",
      "    Epoch  45: Train 88.2%, Test 63.0%\n",
      "    Epoch  60: Train 86.5%, Test 64.4%\n",
      "    Epoch  75: Train 97.9%, Test 65.8%\n",
      "Trial 198/200: 63.0% - 6.8% below best\n",
      "    Time: 1.6min | Total: 348.1min | Est. remaining: 3.3min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 199: LR=0.000700, BS=20, Drop=0.400, Aug=heavy\n",
      "    Epoch   0: Train 19.4%, Test 24.7%\n",
      "    Epoch  15: Train 82.3%, Test 53.4%\n",
      "    Epoch  30: Train 62.8%, Test 60.3%\n",
      "    Epoch  45: Train 66.0%, Test 64.4%\n",
      "    Epoch  60: Train 60.1%, Test 60.3%\n",
      "    Epoch  75: Train 66.0%, Test 60.3%\n",
      "Trial 199/200: 60.3% - 9.6% below best\n",
      "    Time: 1.7min | Total: 349.9min | Est. remaining: 1.7min\n",
      "    Best so far: 69.9%\n",
      "\n",
      "Trial 200: LR=0.002000, BS=12, Drop=0.300, Aug=medium\n",
      "    Epoch   0: Train 19.8%, Test 21.9%\n",
      "    Epoch  15: Train 81.9%, Test 50.7%\n",
      "    Epoch  30: Train 79.9%, Test 50.7%\n",
      "    Epoch  45: Train 79.5%, Test 52.1%\n",
      "    Epoch  60: Train 81.9%, Test 61.6%\n",
      "    Epoch  75: Train 91.3%, Test 58.9%\n",
      "Trial 200/200: 57.5% - 12.3% below best\n",
      "    Time: 2.5min | Total: 352.4min | Est. remaining: 0.0min\n",
      "    Best so far: 69.9%\n",
      "    Checkpoint saved at trial 200\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION COMPLETE\n",
      "================================================================================\n",
      "Total time: 5:52:22.774636\n",
      "Trials completed: 200\n",
      "Best accuracy achieved: 69.9%\n",
      "Gap to 75%: 5.1%\n",
      "\n",
      "Top 10 configurations:\n",
      " 1. 69.9% - Trial 37\n",
      " 2. 69.9% - Trial 99\n",
      " 3. 68.5% - Trial 98\n",
      " 4. 68.5% - Trial 116\n",
      " 5. 68.5% - Trial 185\n",
      " 6. 67.1% - Trial 17\n",
      " 7. 67.1% - Trial 40\n",
      " 8. 67.1% - Trial 55\n",
      " 9. 67.1% - Trial 125\n",
      "10. 67.1% - Trial 180\n",
      "\n",
      "Final results saved to: efficientnet_b0_optimization_20250802_094932\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization for Efficient-B0 model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import json\n",
    "import itertools\n",
    "from datetime import datetime, timedelta\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    MIXED_PRECISION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXED_PRECISION_AVAILABLE = False\n",
    "    class autocast:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    if MIXED_PRECISION_AVAILABLE:\n",
    "        scaler = GradScaler()\n",
    "        use_amp = True\n",
    "        print(\"Mixed Precision: Enabled\")\n",
    "    else:\n",
    "        use_amp = False\n",
    "else:\n",
    "    use_amp = False\n",
    "\n",
    "color_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\"\n",
    "\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        age_str = parts[3]\n",
    "        try:\n",
    "            age = float(age_str.replace('p', '.'))\n",
    "            if age > 5.5:\n",
    "                age = 5.5\n",
    "            return age\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def age_to_class(age):\n",
    "    age_mapping = {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
    "    return age_mapping.get(age, None)\n",
    "\n",
    "def load_color_images_only(color_path, img_size=(224, 224)):\n",
    "    images = []\n",
    "    ages = []\n",
    "    \n",
    "    if os.path.exists(color_path):\n",
    "        for filename in os.listdir(color_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(color_path, filename)\n",
    "                        img = cv2.imread(img_path)\n",
    "                        if img is not None:\n",
    "                            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                            img_resized = cv2.resize(img_rgb, img_size)\n",
    "                            images.append(img_resized)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    ages = np.array(ages)\n",
    "    \n",
    "    return images, ages\n",
    "\n",
    "def create_augmented_image(image, aug_intensity='medium'):\n",
    "    \"\"\"Parameterized augmentation with different intensities\"\"\"\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    # Augmentation parameters by intensity\n",
    "    aug_params = {\n",
    "        'light': {\n",
    "            'flip_prob': 0.3, 'rot_prob': 0.2, 'rot_range': 5,\n",
    "            'color_prob': 0.4, 'alpha_range': (0.95, 1.05), 'beta_range': 5,\n",
    "            'hsv_prob': 0.2, 'hue_range': 5, 'sat_range': (0.95, 1.05),\n",
    "            'noise_prob': 0.1, 'noise_std': 2\n",
    "        },\n",
    "        'medium': {\n",
    "            'flip_prob': 0.5, 'rot_prob': 0.4, 'rot_range': 10,\n",
    "            'color_prob': 0.6, 'alpha_range': (0.9, 1.1), 'beta_range': 10,\n",
    "            'hsv_prob': 0.3, 'hue_range': 8, 'sat_range': (0.9, 1.1),\n",
    "            'noise_prob': 0.2, 'noise_std': 3\n",
    "        },\n",
    "        'heavy': {\n",
    "            'flip_prob': 0.7, 'rot_prob': 0.6, 'rot_range': 15,\n",
    "            'color_prob': 0.8, 'alpha_range': (0.8, 1.2), 'beta_range': 15,\n",
    "            'hsv_prob': 0.5, 'hue_range': 12, 'sat_range': (0.8, 1.2),\n",
    "            'noise_prob': 0.3, 'noise_std': 5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    params = aug_params[aug_intensity]\n",
    "    \n",
    "    # Horizontal flip\n",
    "    if random.random() < params['flip_prob']:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    # Rotation\n",
    "    if random.random() < params['rot_prob']:\n",
    "        angle = random.uniform(-params['rot_range'], params['rot_range'])\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    # Color jittering\n",
    "    if random.random() < params['color_prob']:\n",
    "        alpha = random.uniform(*params['alpha_range'])\n",
    "        beta = random.randint(-params['beta_range'], params['beta_range'])\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # HSV shifts\n",
    "    if random.random() < params['hsv_prob']:\n",
    "        try:\n",
    "            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            h_shift = random.randint(-params['hue_range'], params['hue_range'])\n",
    "            hsv[:, :, 0] = np.clip(hsv[:, :, 0].astype(np.int16) + h_shift, 0, 179).astype(np.uint8)\n",
    "            s_factor = random.uniform(*params['sat_range'])\n",
    "            hsv[:, :, 1] = np.clip(hsv[:, :, 1].astype(np.float32) * s_factor, 0, 255).astype(np.uint8)\n",
    "            image = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Gaussian noise\n",
    "    if random.random() < params['noise_prob']:\n",
    "        noise = np.random.normal(0, params['noise_std'], image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class OptimizedDeerDataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False, aug_intensity='medium', img_size=224):\n",
    "        self.X = X\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.augment = augment\n",
    "        self.aug_intensity = aug_intensity\n",
    "        self.img_size = img_size\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].copy()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        # Resize if needed\n",
    "        if image.shape[:2] != (self.img_size, self.img_size):\n",
    "            image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        \n",
    "        if self.augment:\n",
    "            image = create_augmented_image(image, self.aug_intensity)\n",
    "        \n",
    "        image = torch.FloatTensor(image)\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def create_efficientnet_b0(num_classes=5, drop_rate=0.4, freezing_strategy='standard'):\n",
    "    \"\"\"Create EfficientNetB0 with different freezing strategies\"\"\"\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=num_classes, drop_rate=drop_rate)\n",
    "    \n",
    "    if freezing_strategy == 'none':\n",
    "        # No freezing - train everything\n",
    "        pass\n",
    "    elif freezing_strategy == 'light':\n",
    "        # Freeze only first 2 blocks\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'blocks.0.' in name or 'blocks.1.' in name:\n",
    "                param.requires_grad = False\n",
    "    elif freezing_strategy == 'standard':\n",
    "        # Freeze first 4 blocks (original approach)\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('blocks.6.' in name or 'blocks.7.' in name or 'classifier' in name):\n",
    "                param.requires_grad = False\n",
    "    elif freezing_strategy == 'heavy':\n",
    "        # Freeze everything except last block and classifier\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('blocks.7.' in name or 'classifier' in name):\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def train_with_config(config, X_train, y_train, X_test, y_test, trial_num):\n",
    "    \"\"\"Train model with specific hyperparameter configuration\"\"\"\n",
    "    \n",
    "    # Create datasets with config parameters\n",
    "    train_dataset = OptimizedDeerDataset(\n",
    "        X_train, y_train, \n",
    "        augment=True, \n",
    "        aug_intensity=config['aug_intensity'],\n",
    "        img_size=config['img_size']\n",
    "    )\n",
    "    test_dataset = OptimizedDeerDataset(\n",
    "        X_test, y_test, \n",
    "        augment=False,\n",
    "        img_size=config['img_size']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True, \n",
    "        num_workers=0\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = create_efficientnet_b0(\n",
    "        drop_rate=config['dropout_rate'],\n",
    "        freezing_strategy=config['freezing']\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    if config['loss_function'] == 'crossentropy':\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "    elif config['loss_function'] == 'focal':\n",
    "        # Simple focal loss implementation\n",
    "        class FocalLoss(nn.Module):\n",
    "            def __init__(self, alpha=1, gamma=2):\n",
    "                super().__init__()\n",
    "                self.alpha = alpha\n",
    "                self.gamma = gamma\n",
    "            \n",
    "            def forward(self, inputs, targets):\n",
    "                ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "                pt = torch.exp(-ce_loss)\n",
    "                focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "                return focal_loss.mean()\n",
    "        \n",
    "        criterion = FocalLoss(gamma=config['focal_gamma'])\n",
    "    \n",
    "    # Optimizer\n",
    "    if config['optimizer'] == 'adamw':\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            betas=(config['beta1'], config['beta2'])\n",
    "        )\n",
    "    elif config['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            momentum=config['momentum'],\n",
    "            nesterov=config['nesterov']\n",
    "        )\n",
    "    elif config['optimizer'] == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            alpha=config['rmsprop_alpha']\n",
    "        )\n",
    "    \n",
    "    # Scheduler\n",
    "    if config['scheduler'] == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, \n",
    "            T_max=config['epochs'], \n",
    "            eta_min=config['min_lr']\n",
    "        )\n",
    "    elif config['scheduler'] == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config['step_size'],\n",
    "            gamma=config['step_gamma']\n",
    "        )\n",
    "    elif config['scheduler'] == 'reduce':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            patience=config['reduce_patience'],\n",
    "            factor=config['reduce_factor'],\n",
    "            min_lr=config['min_lr']\n",
    "        )\n",
    "    elif config['scheduler'] == 'warmup_cosine':\n",
    "        # Warmup + Cosine\n",
    "        warmup_epochs = config['warmup_epochs']\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lambda epoch: (epoch / warmup_epochs) if epoch < warmup_epochs \n",
    "            else 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (config['epochs'] - warmup_epochs)))\n",
    "        )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    patience = config['patience']\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    print(f\"Trial {trial_num}: LR={config['learning_rate']:.6f}, BS={config['batch_size']}, \"\n",
    "          f\"Drop={config['dropout_rate']:.3f}, Aug={config['aug_intensity']}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixup\n",
    "            if random.random() < config['mixup_prob']:\n",
    "                lam = np.random.beta(config['mixup_alpha'], config['mixup_alpha'])\n",
    "                batch_size = images.size(0)\n",
    "                index = torch.randperm(batch_size).to(device)\n",
    "                mixed_images = lam * images + (1 - lam) * images[index, :]\n",
    "                y_a, y_b = labels, labels[index]\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(mixed_images)\n",
    "                        loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(mixed_images)\n",
    "                    loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == y_a).sum().item()\n",
    "            else:\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Update scheduler\n",
    "        if config['scheduler'] == 'reduce':\n",
    "            scheduler.step(train_acc)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Evaluation with TTA\n",
    "        if epoch % config['eval_every'] == 0 or epoch == config['epochs'] - 1:\n",
    "            test_acc = evaluate_with_advanced_tta(model, test_loader, config['tta_transforms'])\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if epoch % (config['eval_every'] * 3) == 0:\n",
    "                print(f\"    Epoch {epoch:3d}: Train {train_acc:.1f}%, Test {test_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_acc = evaluate_with_advanced_tta(model, test_loader, config['tta_transforms'])\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, optimizer, scheduler\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_acc\n",
    "\n",
    "def evaluate_with_advanced_tta(model, test_loader, tta_transforms=5):\n",
    "    \"\"\"Advanced TTA evaluation\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = []\n",
    "            \n",
    "            # Original\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "            predictions.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            if tta_transforms >= 2:\n",
    "                # Horizontal flip\n",
    "                flipped = torch.flip(images, [3])\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(flipped)\n",
    "                else:\n",
    "                    outputs = model(flipped)\n",
    "                predictions.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            if tta_transforms >= 3:\n",
    "                # Scale variations\n",
    "                for scale in [0.9, 1.1]:\n",
    "                    scaled = F.interpolate(images, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "                    scaled = F.interpolate(scaled, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                    if use_amp:\n",
    "                        with autocast():\n",
    "                            outputs = model(scaled)\n",
    "                    else:\n",
    "                        outputs = model(scaled)\n",
    "                    predictions.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            if tta_transforms >= 5:\n",
    "                # Crop variations\n",
    "                for crop_factor in [0.85, 0.95]:\n",
    "                    size = int(224 * crop_factor)\n",
    "                    start = (224 - size) // 2\n",
    "                    cropped = images[:, :, start:start+size, start:start+size]\n",
    "                    cropped = F.interpolate(cropped, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                    if use_amp:\n",
    "                        with autocast():\n",
    "                            outputs = model(cropped)\n",
    "                    else:\n",
    "                        outputs = model(cropped)\n",
    "                    predictions.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Average all predictions\n",
    "            avg_output = torch.stack(predictions).mean(0)\n",
    "            _, predicted = torch.max(avg_output, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "def generate_hyperparameter_configs():\n",
    "    \"\"\"Generate comprehensive hyperparameter search space\"\"\"\n",
    "    \n",
    "    # Define search space\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.0001, 0.0003, 0.0005, 0.0007, 0.001, 0.002],\n",
    "        'weight_decay': [0.01, 0.03, 0.05, 0.07, 0.1],\n",
    "        'dropout_rate': [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "        'batch_size': [12, 16, 20, 24],\n",
    "        'optimizer': ['adamw', 'sgd'],\n",
    "        'scheduler': ['cosine', 'warmup_cosine', 'reduce'],\n",
    "        'aug_intensity': ['light', 'medium', 'heavy'],\n",
    "        'mixup_alpha': [0.2, 0.4, 0.6],\n",
    "        'mixup_prob': [0.2, 0.3, 0.5],\n",
    "        'label_smoothing': [0.0, 0.1, 0.15, 0.2],\n",
    "        'freezing': ['light', 'standard', 'heavy'],\n",
    "        'img_size': [224, 256],\n",
    "        'tta_transforms': [2, 3, 5],\n",
    "        'loss_function': ['crossentropy', 'focal']\n",
    "    }\n",
    "    \n",
    "    # Additional optimizer-specific parameters\n",
    "    optimizer_params = {\n",
    "        'adamw': {\n",
    "            'beta1': [0.9, 0.95],\n",
    "            'beta2': [0.999, 0.9999]\n",
    "        },\n",
    "        'sgd': {\n",
    "            'momentum': [0.9, 0.95],\n",
    "            'nesterov': [True, False]\n",
    "        },\n",
    "        'rmsprop': {\n",
    "            'rmsprop_alpha': [0.9, 0.99]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Scheduler-specific parameters\n",
    "    scheduler_params = {\n",
    "        'cosine': {'min_lr': [1e-7, 1e-6]},\n",
    "        'warmup_cosine': {'warmup_epochs': [5, 10], 'min_lr': [1e-7, 1e-6]},\n",
    "        'step': {'step_size': [20, 30], 'step_gamma': [0.5, 0.7]},\n",
    "        'reduce': {'reduce_patience': [5, 8], 'reduce_factor': [0.5, 0.7], 'min_lr': [1e-7, 1e-6]}\n",
    "    }\n",
    "    \n",
    "    # Loss-specific parameters\n",
    "    loss_params = {\n",
    "        'focal': {'focal_gamma': [1.5, 2.0, 2.5]}\n",
    "    }\n",
    "    \n",
    "    # Fixed parameters\n",
    "    fixed_params = {\n",
    "        'epochs': 80,\n",
    "        'patience': 20,\n",
    "        'eval_every': 5\n",
    "    }\n",
    "    \n",
    "    configs = []\n",
    "    \n",
    "    # Sample random combinations (too many to test all)\n",
    "    n_trials = 200  # Will run for many hours\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        config = fixed_params.copy()\n",
    "        \n",
    "        # Sample basic parameters\n",
    "        for param, values in param_grid.items():\n",
    "            config[param] = random.choice(values)\n",
    "        \n",
    "        # Add optimizer-specific parameters\n",
    "        if config['optimizer'] in optimizer_params:\n",
    "            for param, values in optimizer_params[config['optimizer']].items():\n",
    "                config[param] = random.choice(values)\n",
    "        \n",
    "        # Add scheduler-specific parameters\n",
    "        if config['scheduler'] in scheduler_params:\n",
    "            for param, values in scheduler_params[config['scheduler']].items():\n",
    "                config[param] = random.choice(values)\n",
    "        \n",
    "        # Add loss-specific parameters\n",
    "        if config['loss_function'] in loss_params:\n",
    "            for param, values in loss_params[config['loss_function']].items():\n",
    "                config[param] = random.choice(values)\n",
    "        \n",
    "        configs.append(config)\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# Main hyperparameter optimization\n",
    "def main():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"efficientnet_b0_optimization_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EFFICIENTNET-B0 COMPREHENSIVE HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Start time: {start_time}\")\n",
    "    print(f\"Results directory: {output_dir}\")\n",
    "    print(\"Goal: Push beyond 72.6% to maximum possible accuracy\")\n",
    "    print(\"Strategy: Systematic exploration of hyperparameter space\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading color images...\")\n",
    "    X, y = load_color_images_only(color_path)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Generate hyperparameter configurations\n",
    "    print(\"\\nGenerating hyperparameter configurations...\")\n",
    "    configs = generate_hyperparameter_configs()\n",
    "    print(f\"Total configurations to test: {len(configs)}\")\n",
    "    print(f\"Estimated runtime: {len(configs) * 15} minutes = {len(configs) * 15 / 60:.1f} hours\")\n",
    "    \n",
    "    results = []\n",
    "    best_accuracy = 0\n",
    "    best_config = None\n",
    "    \n",
    "    # Progress tracking\n",
    "    save_every = 10\n",
    "    \n",
    "    for trial_num, config in enumerate(configs, 1):\n",
    "        trial_start = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            accuracy = train_with_config(config, X_train, y_train, X_test, y_test, trial_num)\n",
    "            \n",
    "            result = {\n",
    "                'trial': trial_num,\n",
    "                'accuracy': accuracy,\n",
    "                'config': config,\n",
    "                'timestamp': trial_start.isoformat()\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_config = config.copy()\n",
    "                \n",
    "                # Save best model configuration\n",
    "                best_result = {\n",
    "                    'best_accuracy': best_accuracy,\n",
    "                    'best_config': best_config,\n",
    "                    'trial': trial_num,\n",
    "                    'timestamp': trial_start.isoformat()\n",
    "                }\n",
    "                \n",
    "                with open(os.path.join(output_dir, 'best_result.json'), 'w') as f:\n",
    "                    json.dump(best_result, f, indent=2)\n",
    "                \n",
    "                status = \"NEW BEST!\" \n",
    "                if accuracy >= 75.0:\n",
    "                    status += \" TARGET ACHIEVED!\"\n",
    "            else:\n",
    "                status = f\"{best_accuracy - accuracy:.1f}% below best\"\n",
    "            \n",
    "            trial_duration = (datetime.now() - trial_start).total_seconds() / 60\n",
    "            elapsed_total = (datetime.now() - start_time).total_seconds() / 60\n",
    "            estimated_remaining = (len(configs) - trial_num) * trial_duration\n",
    "            \n",
    "            print(f\"Trial {trial_num:3d}/{len(configs)}: {accuracy:.1f}% - {status}\")\n",
    "            print(f\"    Time: {trial_duration:.1f}min | Total: {elapsed_total:.1f}min | Est. remaining: {estimated_remaining:.1f}min\")\n",
    "            print(f\"    Best so far: {best_accuracy:.1f}%\")\n",
    "            \n",
    "            # Save results periodically\n",
    "            if trial_num % save_every == 0:\n",
    "                with open(os.path.join(output_dir, f'results_checkpoint_{trial_num}.json'), 'w') as f:\n",
    "                    json.dump({\n",
    "                        'results': results,\n",
    "                        'best_accuracy': best_accuracy,\n",
    "                        'best_config': best_config,\n",
    "                        'trials_completed': trial_num,\n",
    "                        'start_time': start_time.isoformat(),\n",
    "                        'checkpoint_time': datetime.now().isoformat()\n",
    "                    }, f, indent=2)\n",
    "                \n",
    "                print(f\"    Checkpoint saved at trial {trial_num}\")\n",
    "            \n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial_num} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Final results\n",
    "    end_time = datetime.now()\n",
    "    total_duration = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"OPTIMIZATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total time: {total_duration}\")\n",
    "    print(f\"Trials completed: {len(results)}\")\n",
    "    print(f\"Best accuracy achieved: {best_accuracy:.1f}%\")\n",
    "    \n",
    "    if best_accuracy >= 75.0:\n",
    "        print(\"SUCCESS: 75% TARGET ACHIEVED!\")\n",
    "    else:\n",
    "        print(f\"Gap to 75%: {75.0 - best_accuracy:.1f}%\")\n",
    "    \n",
    "    # Sort results by accuracy\n",
    "    results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop 10 configurations:\")\n",
    "    for i, result in enumerate(results[:10], 1):\n",
    "        print(f\"{i:2d}. {result['accuracy']:.1f}% - Trial {result['trial']}\")\n",
    "    \n",
    "    # Save final results\n",
    "    final_results = {\n",
    "        'optimization_summary': {\n",
    "            'start_time': start_time.isoformat(),\n",
    "            'end_time': end_time.isoformat(),\n",
    "            'total_duration_hours': total_duration.total_seconds() / 3600,\n",
    "            'trials_completed': len(results),\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'target_achieved': best_accuracy >= 75.0\n",
    "        },\n",
    "        'best_configuration': best_config,\n",
    "        'all_results': results\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'final_optimization_results.json'), 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nFinal results saved to: {output_dir}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "002eee88-94bc-4814-acf3-3c7ce346c9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "Mixed Precision: Enabled\n",
      "================================================================================\n",
      "EFFICIENTNET-B0 MULTI-FOLD ENSEMBLE ON COLOR IMAGES\n",
      "================================================================================\n",
      "Goal: Push beyond 72.6% using multi-fold ensemble\n",
      "Strategy: 5-fold CV with multiple optimal configurations\n",
      "\n",
      "Loading color images...\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 289\n",
      "Training pool: 288, Final test: 73\n",
      "Training distribution: Counter({np.int64(4): 70, np.int64(3): 60, np.int64(2): 60, np.int64(1): 53, np.int64(0): 45})\n",
      "Test distribution: Counter({np.int64(4): 18, np.int64(3): 15, np.int64(2): 15, np.int64(1): 13, np.int64(0): 12})\n",
      "\n",
      "Training 5 folds Ã— 2 configs = 10 models\n",
      "Estimated time: 2-3 hours\n",
      "\n",
      "============================================================\n",
      "FOLD 1/5\n",
      "============================================================\n",
      "Fold 1: Train 230, Val 58\n",
      "\n",
      "  Training best configuration...\n",
      "    Training best model for fold 1\n",
      "      Epoch  0: Train 24.3%, Val 39.7%\n",
      "      Epoch 20: Train 91.3%, Val 53.4%\n",
      "      Epoch 40: Train 88.3%, Val 53.4%\n",
      "      Epoch 60: Train 90.0%, Val 50.0%\n",
      "      Epoch 66: Train 90.0%, Val 58.6%\n",
      "      Early stopping at epoch 66\n",
      "    best: 62.1%\n",
      "\n",
      "  Training alternative configuration...\n",
      "    Training alternative model for fold 1\n",
      "      Epoch  0: Train 19.1%, Val 32.8%\n",
      "      Epoch 20: Train 78.7%, Val 53.4%\n",
      "      Epoch 40: Train 86.5%, Val 46.6%\n",
      "      Epoch 60: Train 91.7%, Val 55.2%\n",
      "      Epoch 80: Train 82.6%, Val 53.4%\n",
      "      Epoch 100: Train 73.9%, Val 53.4%\n",
      "      Epoch 102: Train 87.0%, Val 55.2%\n",
      "      Early stopping at epoch 102\n",
      "    alternative: 67.2%\n",
      "\n",
      "Fold 1 average: 64.7%\n",
      "Individual scores: ['62.1%', '67.2%']\n",
      "\n",
      "============================================================\n",
      "FOLD 2/5\n",
      "============================================================\n",
      "Fold 2: Train 230, Val 58\n",
      "\n",
      "  Training best configuration...\n",
      "    Training best model for fold 2\n",
      "      Epoch  0: Train 26.5%, Val 36.2%\n",
      "      Epoch 20: Train 90.9%, Val 58.6%\n",
      "      Epoch 40: Train 92.6%, Val 60.3%\n",
      "      Epoch 41: Train 88.3%, Val 60.3%\n",
      "      Early stopping at epoch 41\n",
      "    best: 65.5%\n",
      "\n",
      "  Training alternative configuration...\n",
      "    Training alternative model for fold 2\n",
      "      Epoch  0: Train 23.0%, Val 19.0%\n",
      "      Epoch 20: Train 77.0%, Val 46.6%\n",
      "      Epoch 40: Train 81.3%, Val 55.2%\n",
      "      Epoch 60: Train 87.8%, Val 55.2%\n",
      "      Epoch 72: Train 85.2%, Val 56.9%\n",
      "      Early stopping at epoch 72\n",
      "    alternative: 58.6%\n",
      "\n",
      "Fold 2 average: 62.1%\n",
      "Individual scores: ['65.5%', '58.6%']\n",
      "\n",
      "============================================================\n",
      "FOLD 3/5\n",
      "============================================================\n",
      "Fold 3: Train 230, Val 58\n",
      "\n",
      "  Training best configuration...\n",
      "    Training best model for fold 3\n",
      "      Epoch  0: Train 26.1%, Val 32.8%\n",
      "      Epoch 20: Train 95.7%, Val 58.6%\n",
      "      Epoch 40: Train 94.3%, Val 55.2%\n",
      "      Epoch 60: Train 90.4%, Val 51.7%\n",
      "      Epoch 65: Train 89.1%, Val 56.9%\n",
      "      Early stopping at epoch 65\n",
      "    best: 62.1%\n",
      "\n",
      "  Training alternative configuration...\n",
      "    Training alternative model for fold 3\n",
      "      Epoch  0: Train 20.9%, Val 27.6%\n",
      "      Epoch 20: Train 79.1%, Val 51.7%\n",
      "      Epoch 40: Train 92.2%, Val 65.5%\n",
      "      Epoch 60: Train 76.1%, Val 53.4%\n",
      "      Epoch 77: Train 78.7%, Val 53.4%\n",
      "      Early stopping at epoch 77\n",
      "    alternative: 67.2%\n",
      "\n",
      "Fold 3 average: 64.7%\n",
      "Individual scores: ['62.1%', '67.2%']\n",
      "\n",
      "============================================================\n",
      "FOLD 4/5\n",
      "============================================================\n",
      "Fold 4: Train 231, Val 57\n",
      "\n",
      "  Training best configuration...\n",
      "    Training best model for fold 4\n",
      "      Epoch  0: Train 25.1%, Val 31.6%\n",
      "      Epoch 20: Train 81.8%, Val 49.1%\n",
      "      Epoch 40: Train 95.2%, Val 52.6%\n",
      "      Epoch 60: Train 84.0%, Val 49.1%\n",
      "      Epoch 64: Train 91.8%, Val 47.4%\n",
      "      Early stopping at epoch 64\n",
      "    best: 59.6%\n",
      "\n",
      "  Training alternative configuration...\n",
      "    Training alternative model for fold 4\n",
      "      Epoch  0: Train 23.4%, Val 26.3%\n",
      "      Epoch 20: Train 70.6%, Val 40.4%\n",
      "      Epoch 40: Train 88.3%, Val 45.6%\n",
      "      Epoch 60: Train 84.0%, Val 49.1%\n",
      "      Epoch 80: Train 90.9%, Val 45.6%\n",
      "      Epoch 88: Train 81.8%, Val 49.1%\n",
      "      Early stopping at epoch 88\n",
      "    alternative: 54.4%\n",
      "\n",
      "Fold 4 average: 57.0%\n",
      "Individual scores: ['59.6%', '54.4%']\n",
      "\n",
      "============================================================\n",
      "FOLD 5/5\n",
      "============================================================\n",
      "Fold 5: Train 231, Val 57\n",
      "\n",
      "  Training best configuration...\n",
      "    Training best model for fold 5\n",
      "      Epoch  0: Train 29.9%, Val 42.1%\n",
      "      Epoch 20: Train 90.0%, Val 56.1%\n",
      "      Epoch 40: Train 93.9%, Val 59.6%\n",
      "      Epoch 51: Train 92.6%, Val 64.9%\n",
      "      Early stopping at epoch 51\n",
      "    best: 66.7%\n",
      "\n",
      "  Training alternative configuration...\n",
      "    Training alternative model for fold 5\n",
      "      Epoch  0: Train 19.5%, Val 28.1%\n",
      "      Epoch 20: Train 78.4%, Val 45.6%\n",
      "      Epoch 40: Train 83.1%, Val 52.6%\n",
      "      Epoch 60: Train 76.2%, Val 52.6%\n",
      "      Epoch 80: Train 91.8%, Val 45.6%\n",
      "      Early stopping at epoch 80\n",
      "    alternative: 56.1%\n",
      "\n",
      "Fold 5 average: 61.4%\n",
      "Individual scores: ['66.7%', '56.1%']\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION RESULTS\n",
      "================================================================================\n",
      "Fold averages:\n",
      "  Fold 1: 64.7%\n",
      "  Fold 2: 62.1%\n",
      "  Fold 3: 64.7%\n",
      "  Fold 4: 57.0%\n",
      "  Fold 5: 61.4%\n",
      "\n",
      "CV Mean: 62.0% Â± 2.8%\n",
      "\n",
      "================================================================================\n",
      "FINAL ENSEMBLE EVALUATION\n",
      "================================================================================\n",
      "Evaluating ensemble with comprehensive TTA...\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "Cross-validation: 62.0% Â± 2.8%\n",
      "Final ensemble test accuracy: 63.0%\n",
      "Total models in ensemble: 10\n",
      "\n",
      "COMPARISON:\n",
      "  Single EfficientNetB0 (original): 72.6%\n",
      "  Hyperparameter optimization: 69.9%\n",
      "  Multi-fold ensemble: 63.0%\n",
      "\n",
      "No improvement over single model (72.6%)\n",
      "Dataset size may be the fundamental limiting factor\n",
      "\n",
      "Ensemble saved: efficientnet_b0_multifold_20250802_233939\\efficientnet_b0_ensemble_63.0pct.pth\n",
      "Detailed results saved to: efficientnet_b0_multifold_20250802_233939\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Multi-fold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    MIXED_PRECISION_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXED_PRECISION_AVAILABLE = False\n",
    "    class autocast:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    if MIXED_PRECISION_AVAILABLE:\n",
    "        scaler = GradScaler()\n",
    "        use_amp = True\n",
    "        print(\"Mixed Precision: Enabled\")\n",
    "    else:\n",
    "        use_amp = False\n",
    "else:\n",
    "    use_amp = False\n",
    "\n",
    "color_path = r\"G:\\Dropbox\\AI Projects\\buck\\images\\squared\\color\"\n",
    "\n",
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        age_str = parts[3]\n",
    "        try:\n",
    "            age = float(age_str.replace('p', '.'))\n",
    "            if age > 5.5:\n",
    "                age = 5.5\n",
    "            return age\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def age_to_class(age):\n",
    "    age_mapping = {1.5: 0, 2.5: 1, 3.5: 2, 4.5: 3, 5.5: 4}\n",
    "    return age_mapping.get(age, None)\n",
    "\n",
    "def load_color_images_only(color_path, img_size=(224, 224)):\n",
    "    images = []\n",
    "    ages = []\n",
    "    \n",
    "    if os.path.exists(color_path):\n",
    "        for filename in os.listdir(color_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                age = parse_filename(filename)\n",
    "                if age is not None:\n",
    "                    class_idx = age_to_class(age)\n",
    "                    if class_idx is not None:\n",
    "                        img_path = os.path.join(color_path, filename)\n",
    "                        img = cv2.imread(img_path)\n",
    "                        if img is not None:\n",
    "                            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                            img_resized = cv2.resize(img_rgb, img_size)\n",
    "                            images.append(img_resized)\n",
    "                            ages.append(class_idx)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    ages = np.array(ages)\n",
    "    \n",
    "    return images, ages\n",
    "\n",
    "def optimal_augment(image):\n",
    "    \"\"\"Optimal augmentation based on hyperparameter optimization results\"\"\"\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    # Light augmentation (performed best in optimization)\n",
    "    if random.random() < 0.4:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    if random.random() < 0.3:\n",
    "        angle = random.uniform(-8, 8)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < 0.5:\n",
    "        alpha = random.uniform(0.9, 1.1)\n",
    "        beta = random.randint(-8, 8)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    if random.random() < 0.25:\n",
    "        try:\n",
    "            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            h_shift = random.randint(-5, 5)\n",
    "            hsv[:, :, 0] = np.clip(hsv[:, :, 0].astype(np.int16) + h_shift, 0, 179).astype(np.uint8)\n",
    "            s_factor = random.uniform(0.95, 1.05)\n",
    "            hsv[:, :, 1] = np.clip(hsv[:, :, 1].astype(np.float32) * s_factor, 0, 255).astype(np.uint8)\n",
    "            image = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if random.random() < 0.15:\n",
    "        noise = np.random.normal(0, 2, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class OptimalDeerDataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False):\n",
    "        self.X = X\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.augment = augment\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].copy()\n",
    "        label = self.y[idx].clone()\n",
    "        \n",
    "        if self.augment:\n",
    "            image = optimal_augment(image)\n",
    "        \n",
    "        image = torch.FloatTensor(image)\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def create_optimized_efficientnet_b0(config_name=\"best\"):\n",
    "    \"\"\"Create EfficientNetB0 with optimized configurations\"\"\"\n",
    "    \n",
    "    # Best configurations from hyperparameter optimization\n",
    "    configs = {\n",
    "        \"best\": {\"drop_rate\": 0.3, \"freezing\": \"light\"},\n",
    "        \"alternative\": {\"drop_rate\": 0.5, \"freezing\": \"standard\"},\n",
    "        \"conservative\": {\"drop_rate\": 0.4, \"freezing\": \"heavy\"}\n",
    "    }\n",
    "    \n",
    "    config = configs[config_name]\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=5, drop_rate=config[\"drop_rate\"])\n",
    "    \n",
    "    # Apply freezing strategy\n",
    "    if config[\"freezing\"] == \"light\":\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'blocks.0.' in name or 'blocks.1.' in name:\n",
    "                param.requires_grad = False\n",
    "    elif config[\"freezing\"] == \"standard\":\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('blocks.6.' in name or 'blocks.7.' in name or 'classifier' in name):\n",
    "                param.requires_grad = False\n",
    "    elif config[\"freezing\"] == \"heavy\":\n",
    "        for name, param in model.named_parameters():\n",
    "            if not ('blocks.7.' in name or 'classifier' in name):\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def train_fold_model(X_fold_train, y_fold_train, X_fold_val, y_fold_val, fold_num, config_name=\"best\"):\n",
    "    \"\"\"Train a single fold with optimal hyperparameters\"\"\"\n",
    "    \n",
    "    # Conservative hyperparameters for smaller fold datasets\n",
    "    optimal_params = {\n",
    "        \"best\": {\n",
    "            \"lr\": 0.0005, \"weight_decay\": 0.05, \"batch_size\": 16,\n",
    "            \"epochs\": 120, \"patience\": 30\n",
    "        },\n",
    "        \"alternative\": {\n",
    "            \"lr\": 0.0003, \"weight_decay\": 0.07, \"batch_size\": 12,\n",
    "            \"epochs\": 120, \"patience\": 30\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    params = optimal_params.get(config_name, optimal_params[\"best\"])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = OptimalDeerDataset(X_fold_train, y_fold_train, augment=True)\n",
    "    val_dataset = OptimalDeerDataset(X_fold_val, y_fold_val, augment=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_optimized_efficientnet_b0(config_name)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params[\"epochs\"], eta_min=1e-7)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    print(f\"    Training {config_name} model for fold {fold_num}\")\n",
    "    \n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Light mixup (based on optimization results)\n",
    "            if random.random() < 0.2:\n",
    "                lam = np.random.beta(0.4, 0.4)\n",
    "                batch_size = images.size(0)\n",
    "                index = torch.randperm(batch_size).to(device)\n",
    "                mixed_images = lam * images + (1 - lam) * images[index, :]\n",
    "                y_a, y_b = labels, labels[index]\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(mixed_images)\n",
    "                        loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(mixed_images)\n",
    "                    loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == y_a).sum().item()\n",
    "            else:\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 20 == 0 or patience_counter >= params[\"patience\"]:\n",
    "            print(f\"      Epoch {epoch:2d}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "        \n",
    "        if patience_counter >= params[\"patience\"]:\n",
    "            print(f\"      Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, best_val_acc\n",
    "\n",
    "def evaluate_with_comprehensive_tta(model, test_loader):\n",
    "    \"\"\"Comprehensive TTA evaluation\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = []\n",
    "            \n",
    "            # Original\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "            predictions.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Horizontal flip\n",
    "            flipped = torch.flip(images, [3])\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(flipped)\n",
    "            else:\n",
    "                outputs = model(flipped)\n",
    "            predictions.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Multi-scale\n",
    "            for scale in [0.9, 1.1]:\n",
    "                scaled = F.interpolate(images, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "                scaled = F.interpolate(scaled, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(scaled)\n",
    "                else:\n",
    "                    outputs = model(scaled)\n",
    "                predictions.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Crop variations\n",
    "            for crop_factor in [0.85, 0.95]:\n",
    "                size = int(224 * crop_factor)\n",
    "                start = (224 - size) // 2\n",
    "                cropped = images[:, :, start:start+size, start:start+size]\n",
    "                cropped = F.interpolate(cropped, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = model(cropped)\n",
    "                else:\n",
    "                    outputs = model(cropped)\n",
    "                predictions.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Average all TTA predictions\n",
    "            avg_output = torch.stack(predictions).mean(0)\n",
    "            _, predicted = torch.max(avg_output, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "def evaluate_ensemble_with_tta(all_fold_models, test_loader):\n",
    "    \"\"\"Evaluate full ensemble with comprehensive TTA\"\"\"\n",
    "    for fold_models in all_fold_models:\n",
    "        for model in fold_models:\n",
    "            model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            ensemble_output = torch.zeros(images.size(0), 5).to(device)\n",
    "            model_count = 0\n",
    "            \n",
    "            # Collect predictions from all models across all folds\n",
    "            for fold_models in all_fold_models:\n",
    "                for model in fold_models:\n",
    "                    tta_predictions = []\n",
    "                    \n",
    "                    # Comprehensive TTA for each model\n",
    "                    # Original\n",
    "                    if use_amp:\n",
    "                        with autocast():\n",
    "                            outputs = model(images)\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    tta_predictions.append(F.softmax(outputs, dim=1))\n",
    "                    \n",
    "                    # Horizontal flip\n",
    "                    flipped = torch.flip(images, [3])\n",
    "                    if use_amp:\n",
    "                        with autocast():\n",
    "                            outputs = model(flipped)\n",
    "                    else:\n",
    "                        outputs = model(flipped)\n",
    "                    tta_predictions.append(F.softmax(outputs, dim=1))\n",
    "                    \n",
    "                    # Multi-scale\n",
    "                    for scale in [0.9, 1.1]:\n",
    "                        scaled = F.interpolate(images, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "                        scaled = F.interpolate(scaled, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                        if use_amp:\n",
    "                            with autocast():\n",
    "                                outputs = model(scaled)\n",
    "                        else:\n",
    "                            outputs = model(scaled)\n",
    "                        tta_predictions.append(F.softmax(outputs, dim=1))\n",
    "                    \n",
    "                    # Average TTA for this model\n",
    "                    model_avg = torch.stack(tta_predictions).mean(0)\n",
    "                    ensemble_output += model_avg\n",
    "                    model_count += 1\n",
    "            \n",
    "            # Final ensemble prediction\n",
    "            ensemble_output /= model_count\n",
    "            _, predicted = torch.max(ensemble_output, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "# Main Multi-Fold Ensemble Pipeline\n",
    "def main():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"efficientnet_b0_multifold_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EFFICIENTNET-B0 MULTI-FOLD ENSEMBLE ON COLOR IMAGES\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Goal: Push beyond 72.6% using multi-fold ensemble\")\n",
    "    print(\"Strategy: 5-fold CV with multiple optimal configurations\")\n",
    "    \n",
    "    # Load color images only\n",
    "    print(\"\\nLoading color images...\")\n",
    "    X, y = load_color_images_only(color_path)\n",
    "    \n",
    "    # Reserve final test set\n",
    "    X_train_all, X_test_final, y_train_all, y_test_final = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training pool: {len(X_train_all)}, Final test: {len(X_test_final)}\")\n",
    "    print(f\"Training distribution: {Counter(y_train_all)}\")\n",
    "    print(f\"Test distribution: {Counter(y_test_final)}\")\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    n_folds = 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Model configurations to train per fold\n",
    "    model_configs = [\"best\", \"alternative\"]\n",
    "    \n",
    "    print(f\"\\nTraining {n_folds} folds Ã— {len(model_configs)} configs = {n_folds * len(model_configs)} models\")\n",
    "    print(\"Estimated time: 2-3 hours\")\n",
    "    \n",
    "    all_fold_models = []\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_all, y_train_all)):\n",
    "        fold_num = fold + 1\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FOLD {fold_num}/{n_folds}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        X_fold_train = X_train_all[train_idx]\n",
    "        y_fold_train = y_train_all[train_idx]\n",
    "        X_fold_val = X_train_all[val_idx]\n",
    "        y_fold_val = y_train_all[val_idx]\n",
    "        \n",
    "        print(f\"Fold {fold_num}: Train {len(X_fold_train)}, Val {len(X_fold_val)}\")\n",
    "        \n",
    "        fold_models = []\n",
    "        fold_scores = []\n",
    "        \n",
    "        for config_name in model_configs:\n",
    "            print(f\"\\n  Training {config_name} configuration...\")\n",
    "            \n",
    "            try:\n",
    "                model, val_acc = train_fold_model(\n",
    "                    X_fold_train, y_fold_train, X_fold_val, y_fold_val, \n",
    "                    fold_num, config_name\n",
    "                )\n",
    "                \n",
    "                fold_models.append(model)\n",
    "                fold_scores.append(val_acc)\n",
    "                \n",
    "                print(f\"    {config_name}: {val_acc:.1f}%\")\n",
    "                \n",
    "                # Save individual model\n",
    "                model_filename = f\"fold_{fold_num}_{config_name}_{val_acc:.1f}pct.pth\"\n",
    "                model_path = os.path.join(output_dir, model_filename)\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'fold': fold_num,\n",
    "                    'config': config_name,\n",
    "                    'accuracy': val_acc\n",
    "                }, model_path)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error with {config_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        all_fold_models.append(fold_models)\n",
    "        fold_avg = np.mean(fold_scores) if fold_scores else 0\n",
    "        fold_results.append(fold_avg)\n",
    "        \n",
    "        print(f\"\\nFold {fold_num} average: {fold_avg:.1f}%\")\n",
    "        print(f\"Individual scores: {[f'{score:.1f}%' for score in fold_scores]}\")\n",
    "    \n",
    "    # Cross-validation results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CROSS-VALIDATION RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    cv_mean = np.mean(fold_results)\n",
    "    cv_std = np.std(fold_results)\n",
    "    \n",
    "    print(\"Fold averages:\")\n",
    "    for i, score in enumerate(fold_results, 1):\n",
    "        print(f\"  Fold {i}: {score:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nCV Mean: {cv_mean:.1f}% Â± {cv_std:.1f}%\")\n",
    "    \n",
    "    # Final ensemble evaluation on held-out test set\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL ENSEMBLE EVALUATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    test_dataset = OptimalDeerDataset(X_test_final, y_test_final, augment=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"Evaluating ensemble with comprehensive TTA...\")\n",
    "    final_ensemble_accuracy = evaluate_ensemble_with_tta(all_fold_models, test_loader)\n",
    "    \n",
    "    # Save ensemble\n",
    "    ensemble_save_path = os.path.join(output_dir, f\"efficientnet_b0_ensemble_{final_ensemble_accuracy:.1f}pct.pth\")\n",
    "    torch.save({\n",
    "        'all_models_state_dicts': [\n",
    "            [model.state_dict() for model in fold_models] \n",
    "            for fold_models in all_fold_models\n",
    "        ],\n",
    "        'model_configs': model_configs,\n",
    "        'fold_scores': fold_results,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'final_test_accuracy': final_ensemble_accuracy,\n",
    "        'n_folds': n_folds,\n",
    "        'total_models': sum(len(fold) for fold in all_fold_models),\n",
    "        'color_only': True,\n",
    "        'architecture': 'efficientnet_b0'\n",
    "    }, ensemble_save_path)\n",
    "    \n",
    "    # Final results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"Cross-validation: {cv_mean:.1f}% Â± {cv_std:.1f}%\")\n",
    "    print(f\"Final ensemble test accuracy: {final_ensemble_accuracy:.1f}%\")\n",
    "    print(f\"Total models in ensemble: {sum(len(fold) for fold in all_fold_models)}\")\n",
    "    \n",
    "    # Compare to previous results\n",
    "    print(f\"\\nCOMPARISON:\")\n",
    "    print(f\"  Single EfficientNetB0 (original): 72.6%\")\n",
    "    print(f\"  Hyperparameter optimization: 69.9%\")\n",
    "    print(f\"  Multi-fold ensemble: {final_ensemble_accuracy:.1f}%\")\n",
    "    \n",
    "    if final_ensemble_accuracy >= 75.0:\n",
    "        print(\"\\nSUCCESS: 75% TARGET ACHIEVED!\")\n",
    "        print(\"Multi-fold ensemble approach worked!\")\n",
    "    elif final_ensemble_accuracy > 72.6:\n",
    "        improvement = final_ensemble_accuracy - 72.6\n",
    "        print(f\"\\nIMPROVEMENT: +{improvement:.1f}% over single model\")\n",
    "        gap = 75.0 - final_ensemble_accuracy\n",
    "        print(f\"Gap to 75%: {gap:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\nNo improvement over single model (72.6%)\")\n",
    "        print(\"Dataset size may be the fundamental limiting factor\")\n",
    "    \n",
    "    print(f\"\\nEnsemble saved: {ensemble_save_path}\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'total_images': len(X),\n",
    "        'train_images': len(X_train_all),\n",
    "        'test_images': len(X_test_final),\n",
    "        'n_folds': n_folds,\n",
    "        'model_configs': model_configs,\n",
    "        'fold_results': fold_results,\n",
    "        'cv_mean': float(cv_mean),\n",
    "        'cv_std': float(cv_std),\n",
    "        'final_test_accuracy': float(final_ensemble_accuracy),\n",
    "        'target_achieved': final_ensemble_accuracy >= 75.0,\n",
    "        'total_models': sum(len(fold) for fold in all_fold_models),\n",
    "        'architecture': 'efficientnet_b0',\n",
    "        'color_only': True\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'ensemble_results.json'), 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Detailed results saved to: {output_dir}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5964c76-26e4-4e5a-b5e4-b6e3e3f65215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
