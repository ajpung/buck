{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e5eff5-c905-4da2-b77d-3ed7a7ca0e9d",
   "metadata": {},
   "source": [
    "### Check RTX5090 running CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6ab5f0-9231-4ae0-8fc4-d27217483190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20250922+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 5090\n",
      "GPU memory: 31.8 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 works!\n",
      "EfficientNet works!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Check if CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not detected by PyTorch\")\n",
    "\n",
    "# Test ResNet50 specifically\n",
    "model = models.resnet50(pretrained=True).cuda()\n",
    "test_batch = torch.randn(2, 3, 224, 224).cuda()\n",
    "try:\n",
    "    output = model(test_batch)\n",
    "    print(\"ResNet50 works!\")\n",
    "except Exception as e:\n",
    "    print(f\"ResNet50 failed: {e}\")\n",
    "\n",
    "# Test EfficientNet\n",
    "try:\n",
    "    model_eff = models.efficientnet_b0(pretrained=True).cuda()\n",
    "    output_eff = model_eff(test_batch)\n",
    "    print(\"EfficientNet works!\")\n",
    "except Exception as e:\n",
    "    print(f\"EfficientNet failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71bc11f-e647-462f-a1f3-466206538d7d",
   "metadata": {},
   "source": [
    "### Process deer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43154fba-18c6-4690-a3cd-6d4e8c863549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTX 5090 Comprehensive Model Evaluation\n",
      "512x512 Images - Multiple Architectures\n",
      "============================================================\n",
      "Loading color images...\n",
      "Loaded 211 color images\n",
      "Loading grayscale images...\n",
      "Loaded 46 grayscale images\n",
      "Total images: 257\n",
      "Final dataset: 257 images\n",
      "Age distribution: {2.5: 43, 3.5: 56, 4.5: 60, 5.5: 63, 1.5: 35}\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n",
      "GPU Memory: 34.2 GB\n",
      "COMPREHENSIVE MODEL EVALUATION - 33 models, 10 folds\n",
      "Image size: 600x600\n",
      "Total experiments: 330\n",
      "Classes: 5\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B0\n",
      "Batch size: 128\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 60.2%, Val 72.3%\n",
      "    Epoch 20: Train 100.0%, Val 89.1%\n",
      "    Epoch 40: Train 100.0%, Val 87.9%\n",
      "    Early stopping at epoch 46\n",
      "    efficientnet_b0 Fold 1: Train 100.0%, Val 90.0%, Test 59.7%, Mult 5373.0\n",
      "\n",
      "[Fold 2/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 64.9%, Val 70.9%\n",
      "    Epoch 20: Train 99.7%, Val 81.3%\n",
      "    Epoch 40: Train 100.0%, Val 78.4%\n",
      "    Early stopping at epoch 44\n",
      "    efficientnet_b0 Fold 2: Train 100.0%, Val 83.3%, Test 54.7%, Mult 4556.5\n",
      "\n",
      "[Fold 3/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 65.7%, Val 67.6%\n",
      "    Epoch 20: Train 100.0%, Val 71.8%\n",
      "    Epoch 40: Train 100.0%, Val 77.2%\n",
      "    Epoch 60: Train 100.0%, Val 75.0%\n",
      "    Early stopping at epoch 62\n",
      "    efficientnet_b0 Fold 3: Train 100.0%, Val 79.2%, Test 59.0%, Mult 4672.8\n",
      "\n",
      "[Fold 4/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 61.4%, Val 75.1%\n",
      "    Epoch 20: Train 100.0%, Val 78.6%\n",
      "    Epoch 40: Train 100.0%, Val 84.6%\n",
      "    Early stopping at epoch 46\n",
      "    efficientnet_b0 Fold 4: Train 99.9%, Val 85.5%, Test 60.0%, Mult 5130.0\n",
      "\n",
      "[Fold 5/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 64.2%, Val 61.5%\n",
      "    Epoch 20: Train 100.0%, Val 68.6%\n",
      "    Epoch 40: Train 99.9%, Val 65.7%\n",
      "    Early stopping at epoch 48\n",
      "    efficientnet_b0 Fold 5: Train 100.0%, Val 68.8%, Test 56.6%, Mult 3894.1\n",
      "\n",
      "[Fold 6/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 61.6%, Val 66.2%\n",
      "    Epoch 20: Train 100.0%, Val 75.0%\n",
      "    Epoch 40: Train 100.0%, Val 71.5%\n",
      "    Epoch 60: Train 100.0%, Val 68.4%\n",
      "    Early stopping at epoch 64\n",
      "    efficientnet_b0 Fold 6: Train 100.0%, Val 78.6%, Test 63.4%, Mult 4983.2\n",
      "\n",
      "[Fold 7/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 62.3%, Val 59.3%\n",
      "    Epoch 20: Train 100.0%, Val 75.1%\n",
      "    Early stopping at epoch 29\n",
      "    efficientnet_b0 Fold 7: Train 99.4%, Val 76.1%, Test 61.9%, Mult 4710.6\n",
      "\n",
      "[Fold 8/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 61.7%, Val 69.6%\n",
      "    Epoch 20: Train 99.9%, Val 76.9%\n",
      "    Epoch 40: Train 100.0%, Val 83.9%\n",
      "    Epoch 60: Train 100.0%, Val 82.7%\n",
      "    Early stopping at epoch 64\n",
      "    efficientnet_b0 Fold 8: Train 100.0%, Val 89.1%, Test 63.3%, Mult 5640.0\n",
      "\n",
      "[Fold 9/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 63.8%, Val 67.3%\n",
      "    Epoch 20: Train 100.0%, Val 80.3%\n",
      "    Epoch 40: Train 100.0%, Val 80.3%\n",
      "    Epoch 60: Train 100.0%, Val 78.3%\n",
      "    Early stopping at epoch 62\n",
      "    efficientnet_b0 Fold 9: Train 100.0%, Val 81.5%, Test 75.0%, Mult 6112.5\n",
      "\n",
      "[Fold 10/10] efficientnet_b0\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b0...\n",
      "Parameters: 4,776,943 trainable, 19,090 frozen\n",
      "    Epoch 0: Train 64.0%, Val 63.5%\n",
      "    Epoch 20: Train 100.0%, Val 68.3%\n",
      "    Epoch 40: Train 99.9%, Val 70.5%\n",
      "    Epoch 60: Train 100.0%, Val 69.4%\n",
      "    efficientnet_b0 Fold 10: Train 100.0%, Val 71.4%, Test 63.7%, Mult 4548.2\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B1\n",
      "Batch size: 96\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 57.8%, Val 75.6%\n",
      "    Epoch 20: Train 100.0%, Val 86.6%\n",
      "    Epoch 40: Train 100.0%, Val 78.4%\n",
      "    Early stopping at epoch 44\n",
      "    efficientnet_b1 Fold 1: Train 100.0%, Val 88.8%, Test 53.7%, Mult 4768.6\n",
      "\n",
      "[Fold 2/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 60.7%, Val 61.3%\n",
      "    Epoch 20: Train 100.0%, Val 72.8%\n",
      "    Early stopping at epoch 29\n",
      "    efficientnet_b1 Fold 2: Train 99.8%, Val 80.3%, Test 63.4%, Mult 5091.0\n",
      "\n",
      "[Fold 3/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 60.6%, Val 60.4%\n",
      "    Epoch 20: Train 100.0%, Val 68.9%\n",
      "    Epoch 40: Train 100.0%, Val 70.7%\n",
      "    Early stopping at epoch 40\n",
      "    efficientnet_b1 Fold 3: Train 99.8%, Val 72.0%, Test 61.9%, Mult 4456.8\n",
      "\n",
      "[Fold 4/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 57.8%, Val 75.5%\n",
      "    Epoch 20: Train 100.0%, Val 79.7%\n",
      "    Epoch 40: Train 100.0%, Val 77.6%\n",
      "    Epoch 60: Train 100.0%, Val 83.2%\n",
      "    Early stopping at epoch 62\n",
      "    efficientnet_b1 Fold 4: Train 100.0%, Val 87.3%, Test 67.1%, Mult 5857.8\n",
      "\n",
      "[Fold 5/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 63.8%, Val 63.1%\n",
      "    Epoch 20: Train 99.9%, Val 64.6%\n",
      "    Epoch 40: Train 100.0%, Val 62.2%\n",
      "    Early stopping at epoch 54\n",
      "    efficientnet_b1 Fold 5: Train 100.0%, Val 68.2%, Test 64.1%, Mult 4371.6\n",
      "\n",
      "[Fold 6/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 59.4%, Val 66.0%\n",
      "    Epoch 20: Train 100.0%, Val 66.7%\n",
      "    Epoch 40: Train 100.0%, Val 65.2%\n",
      "    Epoch 60: Train 100.0%, Val 66.2%\n",
      "    Early stopping at epoch 61\n",
      "    efficientnet_b1 Fold 6: Train 100.0%, Val 75.3%, Test 59.3%, Mult 4465.3\n",
      "\n",
      "[Fold 7/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 60.9%, Val 55.6%\n",
      "    Epoch 20: Train 100.0%, Val 67.6%\n",
      "    Epoch 40: Train 100.0%, Val 67.6%\n",
      "    Early stopping at epoch 55\n",
      "    efficientnet_b1 Fold 7: Train 100.0%, Val 72.4%, Test 62.8%, Mult 4546.7\n",
      "\n",
      "[Fold 8/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 59.4%, Val 55.7%\n",
      "    Epoch 20: Train 99.9%, Val 80.3%\n",
      "    Epoch 40: Train 100.0%, Val 74.5%\n",
      "    Early stopping at epoch 45\n",
      "    efficientnet_b1 Fold 8: Train 99.9%, Val 80.3%, Test 51.9%, Mult 4167.6\n",
      "\n",
      "[Fold 9/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 59.7%, Val 63.0%\n",
      "    Epoch 20: Train 99.7%, Val 67.3%\n",
      "    Early stopping at epoch 33\n",
      "    efficientnet_b1 Fold 9: Train 100.0%, Val 78.1%, Test 77.1%, Mult 6021.5\n",
      "\n",
      "[Fold 10/10] efficientnet_b1\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b1...\n",
      "Parameters: 7,271,257 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 59.2%, Val 63.3%\n",
      "    Epoch 20: Train 100.0%, Val 62.7%\n",
      "    Epoch 40: Train 100.0%, Val 67.5%\n",
      "    Epoch 60: Train 100.0%, Val 67.6%\n",
      "    efficientnet_b1 Fold 10: Train 100.0%, Val 70.1%, Test 63.2%, Mult 4430.3\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B2\n",
      "Batch size: 80\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 77.1%, Val 74.8%\n",
      "    Epoch 20: Train 100.0%, Val 81.7%\n",
      "    Epoch 40: Train 100.0%, Val 81.3%\n",
      "    Early stopping at epoch 51\n",
      "    efficientnet_b2 Fold 1: Train 100.0%, Val 84.5%, Test 65.1%, Mult 5500.9\n",
      "\n",
      "[Fold 2/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 76.8%, Val 74.3%\n",
      "    Epoch 20: Train 100.0%, Val 74.7%\n",
      "    Early stopping at epoch 35\n",
      "    efficientnet_b2 Fold 2: Train 100.0%, Val 83.3%, Test 56.6%, Mult 4714.8\n",
      "\n",
      "[Fold 3/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 74.8%, Val 65.5%\n",
      "    Epoch 20: Train 100.0%, Val 72.9%\n",
      "    Early stopping at epoch 32\n",
      "    efficientnet_b2 Fold 3: Train 100.0%, Val 74.4%, Test 68.3%, Mult 5081.5\n",
      "\n",
      "[Fold 4/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 71.7%, Val 84.0%\n",
      "    Epoch 20: Train 100.0%, Val 85.1%\n",
      "    Early stopping at epoch 26\n",
      "    efficientnet_b2 Fold 4: Train 99.5%, Val 88.8%, Test 66.9%, Mult 5940.7\n",
      "\n",
      "[Fold 5/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 76.2%, Val 66.1%\n",
      "    Epoch 20: Train 100.0%, Val 68.7%\n",
      "    Early stopping at epoch 26\n",
      "    efficientnet_b2 Fold 5: Train 99.8%, Val 70.6%, Test 65.0%, Mult 4589.0\n",
      "\n",
      "[Fold 6/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 77.8%, Val 63.5%\n",
      "    Epoch 20: Train 100.0%, Val 69.6%\n",
      "    Epoch 40: Train 100.0%, Val 69.3%\n",
      "    Early stopping at epoch 51\n",
      "    efficientnet_b2 Fold 6: Train 100.0%, Val 73.0%, Test 59.6%, Mult 4350.8\n",
      "\n",
      "[Fold 7/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 76.6%, Val 69.2%\n",
      "    Epoch 20: Train 100.0%, Val 70.9%\n",
      "    Early stopping at epoch 34\n",
      "    efficientnet_b2 Fold 7: Train 100.0%, Val 78.7%, Test 61.5%, Mult 4840.1\n",
      "\n",
      "[Fold 8/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 77.0%, Val 75.8%\n",
      "    Epoch 20: Train 100.0%, Val 74.1%\n",
      "    Epoch 40: Train 100.0%, Val 72.4%\n",
      "    Early stopping at epoch 50\n",
      "    efficientnet_b2 Fold 8: Train 100.0%, Val 77.7%, Test 64.8%, Mult 5035.0\n",
      "\n",
      "[Fold 9/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 74.8%, Val 73.6%\n",
      "    Epoch 20: Train 100.0%, Val 77.1%\n",
      "    Early stopping at epoch 38\n",
      "    efficientnet_b2 Fold 9: Train 100.0%, Val 80.3%, Test 80.9%, Mult 6496.3\n",
      "\n",
      "[Fold 10/10] efficientnet_b2\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b2...\n",
      "Parameters: 8,524,603 trainable, 30,412 frozen\n",
      "    Epoch 0: Train 75.6%, Val 62.6%\n",
      "    Epoch 20: Train 100.0%, Val 69.0%\n",
      "    Epoch 40: Train 100.0%, Val 67.2%\n",
      "    Early stopping at epoch 48\n",
      "    efficientnet_b2 Fold 10: Train 100.0%, Val 71.8%, Test 68.4%, Mult 4911.1\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B3\n",
      "Batch size: 64\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 73.9%, Val 81.5%\n",
      "    Epoch 20: Train 99.9%, Val 86.6%\n",
      "    Epoch 40: Train 99.9%, Val 81.5%\n",
      "    Early stopping at epoch 46\n",
      "    efficientnet_b3 Fold 1: Train 100.0%, Val 87.3%, Test 61.3%, Mult 5351.5\n",
      "\n",
      "[Fold 2/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 75.8%, Val 67.7%\n",
      "    Epoch 20: Train 100.0%, Val 72.7%\n",
      "    Epoch 40: Train 100.0%, Val 70.2%\n",
      "    Epoch 60: Train 100.0%, Val 80.0%\n",
      "    efficientnet_b3 Fold 2: Train 100.0%, Val 81.0%, Test 65.2%, Mult 5281.2\n",
      "\n",
      "[Fold 3/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 77.8%, Val 67.8%\n",
      "    Epoch 20: Train 99.9%, Val 67.2%\n",
      "    Early stopping at epoch 28\n",
      "    efficientnet_b3 Fold 3: Train 99.9%, Val 76.6%, Test 65.4%, Mult 5009.6\n",
      "\n",
      "[Fold 4/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 75.8%, Val 83.7%\n",
      "    Epoch 20: Train 100.0%, Val 84.5%\n",
      "    Epoch 40: Train 100.0%, Val 83.7%\n",
      "    Early stopping at epoch 41\n",
      "    efficientnet_b3 Fold 4: Train 100.0%, Val 86.1%, Test 56.8%, Mult 4890.5\n",
      "\n",
      "[Fold 5/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 77.6%, Val 66.9%\n",
      "    Epoch 20: Train 100.0%, Val 67.7%\n",
      "    Early stopping at epoch 30\n",
      "    efficientnet_b3 Fold 5: Train 100.0%, Val 72.5%, Test 54.3%, Mult 3936.7\n",
      "\n",
      "[Fold 6/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 78.8%, Val 64.6%\n",
      "    Epoch 20: Train 100.0%, Val 78.1%\n",
      "    Early stopping at epoch 36\n",
      "    efficientnet_b3 Fold 6: Train 99.9%, Val 81.7%, Test 61.4%, Mult 5016.4\n",
      "\n",
      "[Fold 7/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 78.3%, Val 67.4%\n",
      "    Epoch 20: Train 100.0%, Val 69.2%\n",
      "    Epoch 40: Train 100.0%, Val 71.3%\n",
      "    Early stopping at epoch 43\n",
      "    efficientnet_b3 Fold 7: Train 100.0%, Val 75.6%, Test 65.1%, Mult 4921.6\n",
      "\n",
      "[Fold 8/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 76.4%, Val 69.2%\n",
      "    Epoch 20: Train 100.0%, Val 81.2%\n",
      "    Epoch 40: Train 100.0%, Val 80.3%\n",
      "    Epoch 60: Train 100.0%, Val 74.2%\n",
      "    Early stopping at epoch 60\n",
      "    efficientnet_b3 Fold 8: Train 100.0%, Val 83.1%, Test 61.1%, Mult 5077.4\n",
      "\n",
      "[Fold 9/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 76.2%, Val 67.8%\n",
      "    Epoch 20: Train 100.0%, Val 77.3%\n",
      "    Epoch 40: Train 99.9%, Val 81.7%\n",
      "    Epoch 60: Train 100.0%, Val 77.8%\n",
      "    Early stopping at epoch 65\n",
      "    efficientnet_b3 Fold 9: Train 99.9%, Val 81.7%, Test 73.6%, Mult 6013.1\n",
      "\n",
      "[Fold 10/10] efficientnet_b3\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b3...\n",
      "Parameters: 11,563,007 trainable, 52,782 frozen\n",
      "    Epoch 0: Train 76.9%, Val 52.2%\n",
      "    Epoch 20: Train 99.9%, Val 62.2%\n",
      "    Epoch 40: Train 100.0%, Val 63.5%\n",
      "    Early stopping at epoch 59\n",
      "    efficientnet_b3 Fold 10: Train 100.0%, Val 72.4%, Test 67.8%, Mult 4908.7\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B4\n",
      "Batch size: 48\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 63.3%, Val 68.9%\n",
      "    Epoch 20: Train 100.0%, Val 76.9%\n",
      "    Early stopping at epoch 34\n",
      "    efficientnet_b4 Fold 1: Train 100.0%, Val 80.7%, Test 57.2%, Mult 4616.0\n",
      "\n",
      "[Fold 2/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 64.3%, Val 66.2%\n",
      "    Epoch 20: Train 100.0%, Val 76.1%\n",
      "    Epoch 40: Train 100.0%, Val 74.8%\n",
      "    Early stopping at epoch 56\n",
      "    efficientnet_b4 Fold 2: Train 100.0%, Val 78.1%, Test 57.4%, Mult 4482.9\n",
      "\n",
      "[Fold 3/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 69.4%, Val 64.0%\n",
      "    Epoch 20: Train 100.0%, Val 70.6%\n",
      "    Early stopping at epoch 27\n",
      "    efficientnet_b4 Fold 3: Train 99.8%, Val 71.4%, Test 69.3%, Mult 4948.0\n",
      "\n",
      "[Fold 4/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 67.8%, Val 78.4%\n",
      "    Epoch 20: Train 100.0%, Val 83.7%\n",
      "    Epoch 40: Train 100.0%, Val 82.4%\n",
      "    Early stopping at epoch 57\n",
      "    efficientnet_b4 Fold 4: Train 100.0%, Val 86.9%, Test 68.3%, Mult 5935.3\n",
      "\n",
      "[Fold 5/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 70.2%, Val 65.9%\n",
      "    Epoch 20: Train 100.0%, Val 64.4%\n",
      "    Early stopping at epoch 33\n",
      "    efficientnet_b4 Fold 5: Train 100.0%, Val 72.1%, Test 57.6%, Mult 4153.0\n",
      "\n",
      "[Fold 6/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 70.4%, Val 56.1%\n",
      "    Epoch 20: Train 100.0%, Val 75.0%\n",
      "    Epoch 40: Train 100.0%, Val 70.2%\n",
      "    Early stopping at epoch 51\n",
      "    efficientnet_b4 Fold 6: Train 100.0%, Val 78.0%, Test 64.6%, Mult 5038.8\n",
      "\n",
      "[Fold 7/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 67.6%, Val 55.4%\n",
      "    Epoch 20: Train 100.0%, Val 71.7%\n",
      "    Epoch 40: Train 100.0%, Val 73.2%\n",
      "    Early stopping at epoch 44\n",
      "    efficientnet_b4 Fold 7: Train 100.0%, Val 73.4%, Test 58.7%, Mult 4308.6\n",
      "\n",
      "[Fold 8/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 66.2%, Val 60.9%\n",
      "    Epoch 20: Train 100.0%, Val 69.1%\n",
      "    Epoch 40: Train 100.0%, Val 73.8%\n",
      "    Epoch 60: Train 100.0%, Val 76.0%\n",
      "    efficientnet_b4 Fold 8: Train 100.0%, Val 78.7%, Test 53.4%, Mult 4202.6\n",
      "\n",
      "[Fold 9/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 64.9%, Val 67.6%\n",
      "    Epoch 20: Train 100.0%, Val 75.5%\n",
      "    Epoch 40: Train 100.0%, Val 77.1%\n",
      "    Early stopping at epoch 43\n",
      "    efficientnet_b4 Fold 9: Train 100.0%, Val 79.5%, Test 61.1%, Mult 4857.5\n",
      "\n",
      "[Fold 10/10] efficientnet_b4\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b4...\n",
      "Parameters: 18,527,469 trainable, 71,776 frozen\n",
      "    Epoch 0: Train 69.2%, Val 53.0%\n",
      "    Epoch 20: Train 100.0%, Val 61.1%\n",
      "    Early stopping at epoch 39\n",
      "    efficientnet_b4 Fold 10: Train 100.0%, Val 64.5%, Test 60.8%, Mult 3921.6\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B5\n",
      "Batch size: 32\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 86.8%, Val 70.2%\n",
      "    Epoch 20: Train 100.0%, Val 72.8%\n",
      "    Epoch 40: Train 100.0%, Val 73.7%\n",
      "    Early stopping at epoch 43\n",
      "    efficientnet_b5 Fold 1: Train 100.0%, Val 83.3%, Test 68.1%, Mult 5672.7\n",
      "\n",
      "[Fold 2/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 85.8%, Val 74.5%\n",
      "    Epoch 20: Train 100.0%, Val 78.7%\n",
      "    Epoch 40: Train 100.0%, Val 80.2%\n",
      "    Epoch 60: Train 100.0%, Val 82.6%\n",
      "    efficientnet_b5 Fold 2: Train 100.0%, Val 85.5%, Test 69.2%, Mult 5916.6\n",
      "\n",
      "[Fold 3/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 89.1%, Val 62.8%\n",
      "    Epoch 20: Train 100.0%, Val 77.7%\n",
      "    Epoch 40: Train 100.0%, Val 78.3%\n",
      "    Early stopping at epoch 59\n",
      "    efficientnet_b5 Fold 3: Train 100.0%, Val 82.7%, Test 67.0%, Mult 5540.9\n",
      "\n",
      "[Fold 4/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 84.1%, Val 78.6%\n",
      "    Epoch 20: Train 100.0%, Val 83.1%\n",
      "    Epoch 40: Train 100.0%, Val 86.7%\n",
      "    Epoch 60: Train 100.0%, Val 82.8%\n",
      "    Early stopping at epoch 62\n",
      "    efficientnet_b5 Fold 4: Train 100.0%, Val 88.3%, Test 63.5%, Mult 5607.1\n",
      "\n",
      "[Fold 5/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 88.8%, Val 67.2%\n",
      "    Epoch 20: Train 100.0%, Val 67.2%\n",
      "    Epoch 40: Train 100.0%, Val 61.1%\n",
      "    Early stopping at epoch 44\n",
      "    efficientnet_b5 Fold 5: Train 100.0%, Val 68.8%, Test 59.5%, Mult 4093.6\n",
      "\n",
      "[Fold 6/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 86.8%, Val 68.6%\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Epoch 40: Train 100.0%, Val 70.6%\n",
      "    Early stopping at epoch 51\n",
      "    efficientnet_b5 Fold 6: Train 100.0%, Val 78.3%, Test 52.3%, Mult 4095.1\n",
      "\n",
      "[Fold 7/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 88.3%, Val 68.4%\n",
      "    Epoch 20: Train 100.0%, Val 70.5%\n",
      "    Epoch 40: Train 100.0%, Val 67.0%\n",
      "    Early stopping at epoch 50\n",
      "    efficientnet_b5 Fold 7: Train 100.0%, Val 78.1%, Test 72.5%, Mult 5662.2\n",
      "\n",
      "[Fold 8/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 86.3%, Val 69.1%\n",
      "    Epoch 20: Train 100.0%, Val 71.1%\n",
      "    Early stopping at epoch 27\n",
      "    efficientnet_b5 Fold 8: Train 100.0%, Val 75.9%, Test 59.7%, Mult 4531.2\n",
      "\n",
      "[Fold 9/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 86.1%, Val 69.9%\n",
      "    Epoch 20: Train 100.0%, Val 77.2%\n",
      "    Early stopping at epoch 38\n",
      "    efficientnet_b5 Fold 9: Train 100.0%, Val 82.4%, Test 69.5%, Mult 5726.8\n",
      "\n",
      "[Fold 10/10] efficientnet_b5\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b5...\n",
      "Parameters: 29,392,895 trainable, 129,590 frozen\n",
      "    Epoch 0: Train 86.0%, Val 58.8%\n",
      "    Epoch 20: Train 100.0%, Val 61.7%\n",
      "    Epoch 40: Train 100.0%, Val 61.2%\n",
      "    Epoch 60: Train 100.0%, Val 63.0%\n",
      "    Early stopping at epoch 70\n",
      "    efficientnet_b5 Fold 10: Train 100.0%, Val 66.8%, Test 65.1%, Mult 4348.7\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B6\n",
      "Batch size: 24\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 86.4%, Val 81.2%\n",
      "    Epoch 20: Train 100.0%, Val 74.9%\n",
      "    Early stopping at epoch 27\n",
      "    efficientnet_b6 Fold 1: Train 100.0%, Val 81.5%, Test 70.8%, Mult 5770.2\n",
      "\n",
      "[Fold 2/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 89.8%, Val 77.2%\n",
      "    Epoch 20: Train 100.0%, Val 76.2%\n",
      "    Early stopping at epoch 27\n",
      "    efficientnet_b6 Fold 2: Train 99.9%, Val 86.1%, Test 62.1%, Mult 5346.8\n",
      "\n",
      "[Fold 3/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 88.3%, Val 74.2%\n",
      "    Epoch 20: Train 100.0%, Val 77.3%\n",
      "    Epoch 40: Train 100.0%, Val 74.4%\n",
      "    Early stopping at epoch 45\n",
      "    efficientnet_b6 Fold 3: Train 100.0%, Val 77.3%, Test 71.3%, Mult 5511.5\n",
      "\n",
      "[Fold 4/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 89.3%, Val 79.4%\n",
      "    Epoch 20: Train 100.0%, Val 86.1%\n",
      "    Epoch 40: Train 100.0%, Val 82.6%\n",
      "    Early stopping at epoch 50\n",
      "    efficientnet_b6 Fold 4: Train 100.0%, Val 91.2%, Test 65.8%, Mult 6001.0\n",
      "\n",
      "[Fold 5/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 90.2%, Val 63.0%\n",
      "    Epoch 20: Train 100.0%, Val 63.4%\n",
      "    Early stopping at epoch 27\n",
      "    efficientnet_b6 Fold 5: Train 100.0%, Val 73.1%, Test 60.1%, Mult 4393.3\n",
      "\n",
      "[Fold 6/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 89.3%, Val 62.7%\n",
      "    Epoch 20: Train 100.0%, Val 73.5%\n",
      "    Epoch 40: Train 100.0%, Val 75.0%\n",
      "    Early stopping at epoch 56\n",
      "    efficientnet_b6 Fold 6: Train 100.0%, Val 78.7%, Test 59.3%, Mult 4666.9\n",
      "\n",
      "[Fold 7/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 90.2%, Val 75.8%\n",
      "    Epoch 20: Train 100.0%, Val 69.1%\n",
      "    Early stopping at epoch 25\n",
      "    efficientnet_b6 Fold 7: Train 90.2%, Val 75.8%, Test 71.6%, Mult 5427.3\n",
      "\n",
      "[Fold 8/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 86.0%, Val 69.0%\n",
      "    Epoch 20: Train 100.0%, Val 74.5%\n",
      "    Epoch 40: Train 100.0%, Val 73.3%\n",
      "    Epoch 60: Train 100.0%, Val 67.4%\n",
      "    Early stopping at epoch 60\n",
      "    efficientnet_b6 Fold 8: Train 100.0%, Val 82.6%, Test 63.8%, Mult 5269.9\n",
      "\n",
      "[Fold 9/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 86.5%, Val 72.0%\n",
      "    Epoch 20: Train 100.0%, Val 76.9%\n",
      "    Early stopping at epoch 38\n",
      "    efficientnet_b6 Fold 9: Train 100.0%, Val 80.3%, Test 71.9%, Mult 5773.6\n",
      "\n",
      "[Fold 10/10] efficientnet_b6\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b6...\n",
      "Parameters: 41,881,837 trainable, 166,640 frozen\n",
      "    Epoch 0: Train 88.1%, Val 61.1%\n",
      "    Epoch 20: Train 99.9%, Val 59.6%\n",
      "    Early stopping at epoch 30\n",
      "    efficientnet_b6 Fold 10: Train 99.9%, Val 67.8%, Test 64.5%, Mult 4373.1\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: EFFICIENTNET_B7\n",
      "Batch size: 16\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 89.0%, Val 78.6%\n",
      "    Epoch 20: Train 100.0%, Val 74.4%\n",
      "    Early stopping at epoch 26\n",
      "    efficientnet_b7 Fold 1: Train 99.9%, Val 81.4%, Test 61.9%, Mult 5038.7\n",
      "\n",
      "[Fold 2/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 88.9%, Val 75.1%\n",
      "    Epoch 20: Train 99.6%, Val 72.0%\n",
      "    Early stopping at epoch 33\n",
      "    efficientnet_b7 Fold 2: Train 100.0%, Val 79.6%, Test 69.0%, Mult 5492.4\n",
      "\n",
      "[Fold 3/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 90.1%, Val 80.7%\n",
      "    Epoch 20: Train 100.0%, Val 75.9%\n",
      "    Early stopping at epoch 25\n",
      "    efficientnet_b7 Fold 3: Train 90.1%, Val 80.7%, Test 68.6%, Mult 5536.0\n",
      "\n",
      "[Fold 4/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 89.9%, Val 84.0%\n",
      "    Epoch 20: Train 100.0%, Val 83.7%\n",
      "    Epoch 40: Train 100.0%, Val 86.0%\n",
      "    Early stopping at epoch 50\n",
      "    efficientnet_b7 Fold 4: Train 100.0%, Val 94.5%, Test 67.5%, Mult 6378.8\n",
      "\n",
      "[Fold 5/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 89.4%, Val 63.2%\n",
      "    Epoch 20: Train 100.0%, Val 65.1%\n",
      "    Early stopping at epoch 33\n",
      "    efficientnet_b7 Fold 5: Train 98.2%, Val 72.9%, Test 60.8%, Mult 4432.3\n",
      "\n",
      "[Fold 6/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 90.8%, Val 71.0%\n",
      "    Epoch 20: Train 100.0%, Val 75.6%\n",
      "    Early stopping at epoch 36\n",
      "    efficientnet_b7 Fold 6: Train 100.0%, Val 80.6%, Test 60.4%, Mult 4868.2\n",
      "\n",
      "[Fold 7/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 89.5%, Val 66.3%\n",
      "    Epoch 20: Train 100.0%, Val 63.8%\n",
      "    Early stopping at epoch 26\n",
      "    efficientnet_b7 Fold 7: Train 99.9%, Val 71.1%, Test 64.9%, Mult 4614.4\n",
      "\n",
      "[Fold 8/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 90.5%, Val 71.0%\n",
      "    Epoch 20: Train 100.0%, Val 75.4%\n",
      "    Epoch 40: Train 100.0%, Val 76.3%\n",
      "    Early stopping at epoch 46\n",
      "    efficientnet_b7 Fold 8: Train 100.0%, Val 78.2%, Test 69.1%, Mult 5403.6\n",
      "\n",
      "[Fold 9/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 89.8%, Val 83.6%\n",
      "    Epoch 20: Train 100.0%, Val 74.3%\n",
      "    Early stopping at epoch 25\n",
      "    efficientnet_b7 Fold 9: Train 89.8%, Val 83.6%, Test 77.0%, Mult 6437.2\n",
      "\n",
      "[Fold 10/10] efficientnet_b7\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading efficientnet_b7...\n",
      "Parameters: 64,964,605 trainable, 266,200 frozen\n",
      "    Epoch 0: Train 90.0%, Val 57.0%\n",
      "    Epoch 20: Train 100.0%, Val 67.0%\n",
      "    Early stopping at epoch 36\n",
      "    efficientnet_b7 Fold 10: Train 100.0%, Val 72.0%, Test 70.4%, Mult 5068.8\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: RESNET18\n",
      "Batch size: 256\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] resnet18\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 45.1%, Val 57.6%\n",
      "    Epoch 20: Train 100.0%, Val 82.2%\n",
      "    Epoch 40: Train 100.0%, Val 82.0%\n",
      "    Early stopping at epoch 40\n",
      "    resnet18 Fold 1: Train 100.0%, Val 87.4%, Test 56.9%, Mult 4973.1\n",
      "\n",
      "[Fold 2/10] resnet18\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 47.9%, Val 44.9%\n",
      "    Epoch 20: Train 100.0%, Val 80.0%\n",
      "    Early stopping at epoch 39\n",
      "    resnet18 Fold 2: Train 100.0%, Val 81.5%, Test 64.2%, Mult 5232.3\n",
      "\n",
      "[Fold 3/10] resnet18\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 48.0%, Val 65.9%\n",
      "    Epoch 20: Train 100.0%, Val 74.9%\n",
      "    Early stopping at epoch 30\n",
      "    resnet18 Fold 3: Train 100.0%, Val 79.4%, Test 61.4%, Mult 4875.2\n",
      "\n",
      "[Fold 4/10] resnet18\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 45.2%, Val 54.2%\n",
      "    Epoch 20: Train 100.0%, Val 86.2%\n",
      "    Epoch 40: Train 100.0%, Val 83.7%\n",
      "    Early stopping at epoch 45\n",
      "    resnet18 Fold 4: Train 100.0%, Val 86.2%, Test 59.7%, Mult 5146.1\n",
      "\n",
      "[Fold 5/10] resnet18\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 49.4%, Val 56.9%\n",
      "    Epoch 20: Train 100.0%, Val 65.6%\n",
      "    Early stopping at epoch 31\n",
      "    resnet18 Fold 5: Train 100.0%, Val 70.1%, Test 54.5%, Mult 3820.5\n",
      "\n",
      "[Fold 6/10] resnet18\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 49.6%, Val 65.4%\n",
      "    Epoch 20: Train 100.0%, Val 81.6%\n",
      "    Epoch 40: Train 100.0%, Val 81.1%\n",
      "    Epoch 60: Train 100.0%, Val 81.5%\n",
      "    Early stopping at epoch 76\n",
      "    resnet18 Fold 6: Train 100.0%, Val 82.7%, Test 51.3%, Mult 4242.5\n",
      "\n",
      "[Fold 7/10] resnet18\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 49.9%, Val 47.5%\n",
      "    Epoch 20: Train 100.0%, Val 63.1%\n",
      "    Early stopping at epoch 31\n",
      "    resnet18 Fold 7: Train 100.0%, Val 67.6%, Test 66.2%, Mult 4475.1\n",
      "\n",
      "[Fold 8/10] resnet18\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 48.1%, Val 63.2%\n",
      "    Epoch 20: Train 100.0%, Val 80.1%\n",
      "    Early stopping at epoch 38\n",
      "    resnet18 Fold 8: Train 100.0%, Val 83.0%, Test 64.7%, Mult 5370.1\n",
      "\n",
      "[Fold 9/10] resnet18\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 48.4%, Val 53.7%\n",
      "    Epoch 20: Train 100.0%, Val 70.4%\n",
      "    Epoch 40: Train 100.0%, Val 70.6%\n",
      "    Early stopping at epoch 51\n",
      "    resnet18 Fold 9: Train 100.0%, Val 72.8%, Test 69.6%, Mult 5066.9\n",
      "\n",
      "[Fold 10/10] resnet18\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet18...\n",
      "Parameters: 10,888,709 trainable, 683,072 frozen\n",
      "    Epoch 0: Train 46.0%, Val 62.3%\n",
      "    Epoch 20: Train 100.0%, Val 62.9%\n",
      "    Early stopping at epoch 33\n",
      "    resnet18 Fold 10: Train 100.0%, Val 67.9%, Test 62.6%, Mult 4250.5\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: RESNET34\n",
      "Batch size: 192\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] resnet34\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 63.0%, Val 67.7%\n",
      "    Epoch 20: Train 100.0%, Val 84.1%\n",
      "    Early stopping at epoch 35\n",
      "    resnet34 Fold 1: Train 100.0%, Val 90.4%, Test 59.7%, Mult 5396.9\n",
      "\n",
      "[Fold 2/10] resnet34\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 65.0%, Val 68.3%\n",
      "    Epoch 20: Train 100.0%, Val 80.1%\n",
      "    Epoch 40: Train 100.0%, Val 78.7%\n",
      "    Epoch 60: Train 100.0%, Val 76.8%\n",
      "    Early stopping at epoch 60\n",
      "    resnet34 Fold 2: Train 100.0%, Val 84.4%, Test 59.3%, Mult 5004.9\n",
      "\n",
      "[Fold 3/10] resnet34\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 63.1%, Val 69.8%\n",
      "    Epoch 20: Train 99.5%, Val 67.3%\n",
      "    Early stopping at epoch 27\n",
      "    resnet34 Fold 3: Train 99.5%, Val 74.9%, Test 61.9%, Mult 4636.3\n",
      "\n",
      "[Fold 4/10] resnet34\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 62.1%, Val 74.1%\n",
      "    Epoch 20: Train 99.9%, Val 83.6%\n",
      "    Epoch 40: Train 100.0%, Val 78.8%\n",
      "    Early stopping at epoch 40\n",
      "    resnet34 Fold 4: Train 100.0%, Val 87.2%, Test 63.8%, Mult 5563.4\n",
      "\n",
      "[Fold 5/10] resnet34\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 63.9%, Val 60.0%\n",
      "    Epoch 20: Train 99.7%, Val 65.3%\n",
      "    Early stopping at epoch 38\n",
      "    resnet34 Fold 5: Train 99.9%, Val 69.8%, Test 52.7%, Mult 3678.5\n",
      "\n",
      "[Fold 6/10] resnet34\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 63.5%, Val 58.7%\n",
      "    Epoch 20: Train 100.0%, Val 79.2%\n",
      "    Early stopping at epoch 30\n",
      "    resnet34 Fold 6: Train 99.4%, Val 83.3%, Test 55.0%, Mult 4581.5\n",
      "\n",
      "[Fold 7/10] resnet34\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 62.0%, Val 55.5%\n",
      "    Epoch 20: Train 100.0%, Val 74.3%\n",
      "    Epoch 40: Train 99.8%, Val 71.2%\n",
      "    Early stopping at epoch 55\n",
      "    resnet34 Fold 7: Train 98.8%, Val 79.9%, Test 62.2%, Mult 4969.8\n",
      "\n",
      "[Fold 8/10] resnet34\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 57.8%, Val 77.4%\n",
      "    Epoch 20: Train 100.0%, Val 81.3%\n",
      "    Early stopping at epoch 34\n",
      "    resnet34 Fold 8: Train 99.7%, Val 86.0%, Test 59.8%, Mult 5142.8\n",
      "\n",
      "[Fold 9/10] resnet34\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 61.7%, Val 60.5%\n",
      "    Epoch 20: Train 100.0%, Val 72.9%\n",
      "    Early stopping at epoch 29\n",
      "    resnet34 Fold 9: Train 100.0%, Val 74.6%, Test 76.1%, Mult 5677.1\n",
      "\n",
      "[Fold 10/10] resnet34\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet34...\n",
      "Parameters: 20,332,037 trainable, 1,347,904 frozen\n",
      "    Epoch 0: Train 62.2%, Val 57.6%\n",
      "    Epoch 20: Train 99.7%, Val 64.4%\n",
      "    Epoch 40: Train 100.0%, Val 67.4%\n",
      "    Epoch 60: Train 100.0%, Val 64.6%\n",
      "    Early stopping at epoch 71\n",
      "    resnet34 Fold 10: Train 100.0%, Val 73.5%, Test 69.4%, Mult 5100.9\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: RESNET50\n",
      "Batch size: 128\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] resnet50\n",
      "Dataset: 5000 samples from 164 base images\n",
      "Dataset: 1000 samples from 41 base images\n",
      "Dataset: 1000 samples from 52 base images\n",
      "Loading resnet50...\n",
      "Parameters: 23,244,805 trainable, 1,444,928 frozen\n",
      "    Epoch 0: Train 59.6%, Val 69.1%\n",
      "    Epoch 20: Train 100.0%, Val 80.5%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RTX 5090 Optimized Configuration\n",
    "IMAGE_SIZE = (600, 600)  # Scaled up from 224x224\n",
    "AUGMENTATION_TARGET = 1000\n",
    "NUM_FOLDS = 10\n",
    "NUM_WORKERS = 0\n",
    "MIXED_PRECISION = True\n",
    "COMPILE_MODEL = False\n",
    "\n",
    "# Model configurations with appropriate batch sizes for 512x512 on RTX 5090\n",
    "MODEL_CONFIGS = {\n",
    "    # EfficientNet family\n",
    "    'efficientnet_b0': {'model_fn': models.efficientnet_b0, 'batch_size': 128, 'freeze_layers': 3},\n",
    "    'efficientnet_b1': {'model_fn': models.efficientnet_b1, 'batch_size': 96, 'freeze_layers': 3},\n",
    "    'efficientnet_b2': {'model_fn': models.efficientnet_b2, 'batch_size': 80, 'freeze_layers': 3},\n",
    "    'efficientnet_b3': {'model_fn': models.efficientnet_b3, 'batch_size': 64, 'freeze_layers': 3},\n",
    "    'efficientnet_b4': {'model_fn': models.efficientnet_b4, 'batch_size': 48, 'freeze_layers': 3},\n",
    "    'efficientnet_b5': {'model_fn': models.efficientnet_b5, 'batch_size': 32, 'freeze_layers': 3},\n",
    "    'efficientnet_b6': {'model_fn': models.efficientnet_b6, 'batch_size': 24, 'freeze_layers': 3},\n",
    "    'efficientnet_b7': {'model_fn': models.efficientnet_b7, 'batch_size': 16, 'freeze_layers': 3},\n",
    "    \n",
    "    # ResNet family\n",
    "    'resnet18': {'model_fn': models.resnet18, 'batch_size': 256, 'freeze_layers': 2},\n",
    "    'resnet34': {'model_fn': models.resnet34, 'batch_size': 192, 'freeze_layers': 2},\n",
    "    'resnet50': {'model_fn': models.resnet50, 'batch_size': 128, 'freeze_layers': 2},\n",
    "    'resnet101': {'model_fn': models.resnet101, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'resnet152': {'model_fn': models.resnet152, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # DenseNet family\n",
    "    'densenet121': {'model_fn': models.densenet121, 'batch_size': 96, 'freeze_layers': 2},\n",
    "    'densenet169': {'model_fn': models.densenet169, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'densenet201': {'model_fn': models.densenet201, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # MobileNet family\n",
    "    'mobilenet_v2': {'model_fn': models.mobilenet_v2, 'batch_size': 192, 'freeze_layers': 3},\n",
    "    'mobilenet_v3_small': {'model_fn': models.mobilenet_v3_small, 'batch_size': 256, 'freeze_layers': 3},\n",
    "    'mobilenet_v3_large': {'model_fn': models.mobilenet_v3_large, 'batch_size': 192, 'freeze_layers': 3},\n",
    "    \n",
    "    # RegNet family\n",
    "    'regnet_y_400mf': {'model_fn': models.regnet_y_400mf, 'batch_size': 128, 'freeze_layers': 2},\n",
    "    'regnet_y_800mf': {'model_fn': models.regnet_y_800mf, 'batch_size': 96, 'freeze_layers': 2},\n",
    "    'regnet_y_1_6gf': {'model_fn': models.regnet_y_1_6gf, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'regnet_y_3_2gf': {'model_fn': models.regnet_y_3_2gf, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # ConvNeXt family\n",
    "    'convnext_tiny': {'model_fn': models.convnext_tiny, 'batch_size': 96, 'freeze_layers': 2},\n",
    "    'convnext_small': {'model_fn': models.convnext_small, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'convnext_base': {'model_fn': models.convnext_base, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # MaxViT family\n",
    "    'maxvit_t': {'model_fn': models.maxvit_t, 'batch_size': 48, 'freeze_layers': 2},\n",
    "    \n",
    "    # Swin Transformer family\n",
    "    'swin_t': {'model_fn': models.swin_t, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    'swin_s': {'model_fn': models.swin_s, 'batch_size': 48, 'freeze_layers': 2},\n",
    "    'swin_b': {'model_fn': models.swin_b, 'batch_size': 32, 'freeze_layers': 2},\n",
    "    \n",
    "    # Vision Transformer family\n",
    "    'vit_b_16': {'model_fn': models.vit_b_16, 'batch_size': 64, 'freeze_layers': 6},\n",
    "    'vit_b_32': {'model_fn': models.vit_b_32, 'batch_size': 96, 'freeze_layers': 6},\n",
    "    'vit_l_16': {'model_fn': models.vit_l_16, 'batch_size': 32, 'freeze_layers': 8},\n",
    "}\n",
    "\n",
    "# Training hyperparameters\n",
    "TRAINING_CONFIG = {\n",
    "    'backbone_lr': 0.0001,\n",
    "    'classifier_lr': 0.0005,\n",
    "    'optimizer': 'adamw',\n",
    "    'weight_decay': 0.05,\n",
    "    'scheduler': 'cosine',\n",
    "    'label_smoothing': 0.1,\n",
    "    'dropout': 0.3,\n",
    "    'max_epochs': 80,\n",
    "    'patience': 25,\n",
    "    'augmentation_strength': 'medium'\n",
    "}\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    color_path = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\trail cam\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "    gray_path = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\trail cam\\\\images\\\\squared\\\\grayscale\\\\*_NDA.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    \n",
    "    return np.array(filtered_images, dtype=np.uint8), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image(image, strength='medium'):\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    if strength == 'light':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.5, 0.3, 0.6, 0.2, 0.1\n",
    "        rot_range, bright_range = 8, (0.85, 1.15)\n",
    "    elif strength == 'medium':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.7, 0.5, 0.8, 0.4, 0.3\n",
    "        rot_range, bright_range = 12, (0.75, 1.25)\n",
    "    else:  # heavy\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.8, 0.6, 0.9, 0.5, 0.4\n",
    "        rot_range, bright_range = 18, (0.65, 1.35)\n",
    "    \n",
    "    if random.random() < rot_prob:\n",
    "        angle = random.uniform(-rot_range, rot_range)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < flip_prob:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    if random.random() < bright_prob:\n",
    "        alpha = random.uniform(*bright_range)\n",
    "        beta = random.randint(-20, 20)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    if random.random() < gamma_prob:\n",
    "        gamma = random.uniform(0.85, 1.15)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    if random.random() < noise_prob:\n",
    "        noise = np.random.normal(0, 5, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class OptimizedDataset(Dataset):\n",
    "    def __init__(self, base_images, labels, aug_strength='medium', target_per_class=1000, training=True):\n",
    "        self.base_images = base_images\n",
    "        self.labels = np.array(labels)\n",
    "        self.aug_strength = aug_strength\n",
    "        self.training = training\n",
    "        self.target_per_class = target_per_class\n",
    "        \n",
    "        unique_classes = np.unique(labels)\n",
    "        self.class_to_indices = {}\n",
    "        for cls in unique_classes:\n",
    "            self.class_to_indices[cls] = np.where(self.labels == cls)[0]\n",
    "        \n",
    "        self.num_classes = len(unique_classes)\n",
    "        self.class_list = sorted(unique_classes)\n",
    "        self.length = self.num_classes * self.target_per_class\n",
    "        \n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(3, 1, 1)\n",
    "        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(3, 1, 1)\n",
    "        \n",
    "        print(f\"Dataset: {self.length} samples from {len(base_images)} base images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        class_idx = idx // self.target_per_class\n",
    "        within_class_idx = idx % self.target_per_class\n",
    "        \n",
    "        target_class = self.class_list[class_idx]\n",
    "        available_indices = self.class_to_indices[target_class]\n",
    "        \n",
    "        base_idx = available_indices[within_class_idx % len(available_indices)]\n",
    "        image = self.base_images[base_idx].copy()\n",
    "        \n",
    "        if self.training and within_class_idx >= len(available_indices):\n",
    "            image = enhanced_augment_image(image, self.aug_strength)\n",
    "        \n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.transpose(2, 0, 1)\n",
    "        \n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = np.flip(image, axis=2).copy()\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return torch.from_numpy(image.astype(np.float32)), target_class\n",
    "\n",
    "class MultiModelTrainer:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"multimodel_512_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.results = []\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "            \n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            \n",
    "            if MIXED_PRECISION:\n",
    "                self.scaler = torch.amp.GradScaler('cuda')\n",
    "            else:\n",
    "                self.scaler = None\n",
    "    \n",
    "    def create_model(self, model_name, model_config):\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        model = model_config['model_fn'](weights='DEFAULT')\n",
    "        \n",
    "        freeze_layers = model_config.get('freeze_layers', 2)\n",
    "        \n",
    "        # MobileNet family\n",
    "        if 'mobilenet' in model_name:\n",
    "            layers_to_freeze = list(model.features.children())[:freeze_layers]\n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            original_features = model.classifier[-1].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        # EfficientNet, DenseNet\n",
    "        elif hasattr(model, 'features') and hasattr(model, 'classifier'):\n",
    "            layers_to_freeze = list(model.features.children())[:freeze_layers]\n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            if isinstance(model.classifier, nn.Sequential):\n",
    "                original_features = model.classifier[-1].in_features\n",
    "            else:\n",
    "                original_features = model.classifier.in_features\n",
    "            \n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        # ResNet\n",
    "        elif 'resnet' in model_name:\n",
    "            layers_to_freeze = [model.conv1, model.bn1]\n",
    "            if freeze_layers >= 1:\n",
    "                layers_to_freeze.append(model.layer1)\n",
    "            if freeze_layers >= 2:\n",
    "                layers_to_freeze.append(model.layer2)\n",
    "            \n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            original_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        # RegNet\n",
    "        elif 'regnet' in model_name:\n",
    "            layers_to_freeze = [model.stem]\n",
    "            if hasattr(model, 'trunk_output') and hasattr(model.trunk_output, 'block1'):\n",
    "                if freeze_layers >= 1:\n",
    "                    layers_to_freeze.append(model.trunk_output.block1)\n",
    "                if freeze_layers >= 2 and hasattr(model.trunk_output, 'block2'):\n",
    "                    layers_to_freeze.append(model.trunk_output.block2)\n",
    "            \n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            original_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        # ConvNeXt\n",
    "        elif 'convnext' in model_name:\n",
    "            if hasattr(model, 'features'):\n",
    "                layers_to_freeze = list(model.features.children())[:freeze_layers]\n",
    "                for layer in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            \n",
    "            original_features = model.classifier[2].in_features\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Flatten(1),\n",
    "                nn.LayerNorm(original_features),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        # MaxViT, Swin, ViT\n",
    "        elif hasattr(model, 'head'):\n",
    "            if hasattr(model, 'encoder') and hasattr(model.encoder, 'layers'):\n",
    "                layers_to_freeze = list(model.encoder.layers.children())[:freeze_layers]\n",
    "                for layer in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            \n",
    "            original_features = model.head.in_features\n",
    "            model.head = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Parameters: {trainable_params:,} trainable, {frozen_params:,} frozen\")\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def get_optimizer(self, model):\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if any(keyword in name for keyword in ['classifier', 'fc', 'head']):\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        param_groups = [\n",
    "            {'params': backbone_params, 'lr': TRAINING_CONFIG['backbone_lr']},\n",
    "            {'params': classifier_params, 'lr': TRAINING_CONFIG['classifier_lr']}\n",
    "        ]\n",
    "        \n",
    "        return optim.AdamW(param_groups, weight_decay=TRAINING_CONFIG['weight_decay'], fused=True)\n",
    "    \n",
    "    def get_scheduler(self, optimizer):\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=TRAINING_CONFIG['max_epochs'], eta_min=1e-6\n",
    "        )\n",
    "    \n",
    "    def train_model(self, model, model_name, train_loader, val_loader, test_loader, fold_num):\n",
    "        optimizer = self.get_optimizer(model)\n",
    "        scheduler = self.get_scheduler(optimizer)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=TRAINING_CONFIG['label_smoothing'])\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        best_train_acc = 0.0\n",
    "        best_test_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(TRAINING_CONFIG['max_epochs']):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if MIXED_PRECISION and self.scaler:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.step(optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    if MIXED_PRECISION:\n",
    "                        with torch.amp.autocast('cuda'):\n",
    "                            outputs = model(images)\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_train_acc = train_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "                \n",
    "                # Test accuracy when validation improves\n",
    "                model.eval()\n",
    "                test_correct = 0\n",
    "                test_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for images, labels in test_loader:\n",
    "                        images, labels = images.to(self.device), labels.to(self.device)\n",
    "                        \n",
    "                        if MIXED_PRECISION:\n",
    "                            with torch.amp.autocast('cuda'):\n",
    "                                outputs1 = model(images)\n",
    "                                outputs2 = model(torch.flip(images, [3]))\n",
    "                                outputs = (outputs1 + outputs2) / 2\n",
    "                        else:\n",
    "                            outputs1 = model(images)\n",
    "                            outputs2 = model(torch.flip(images, [3]))\n",
    "                            outputs = (outputs1 + outputs2) / 2\n",
    "                        \n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        test_total += labels.size(0)\n",
    "                        test_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                best_test_acc = 100 * test_correct / test_total\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= TRAINING_CONFIG['patience']:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Calculate multiplicative metric\n",
    "        multiplicative_score = (best_val_acc / 100) * (best_test_acc / 100) * 10000\n",
    "        \n",
    "        # Save model\n",
    "        save_path = os.path.join(self.save_dir, f\"{model_name}_fold{fold_num}_val{best_val_acc:.1f}_test{best_test_acc:.1f}_mult{multiplicative_score:.1f}.pth\")\n",
    "        torch.save({\n",
    "            'model_state_dict': best_state,\n",
    "            'model_name': model_name,\n",
    "            'fold_number': fold_num,\n",
    "            'train_accuracy': best_train_acc,\n",
    "            'validation_accuracy': best_val_acc,\n",
    "            'test_accuracy': best_test_acc,\n",
    "            'multiplicative_score': multiplicative_score,\n",
    "            'image_size': IMAGE_SIZE,\n",
    "            'training_config': TRAINING_CONFIG\n",
    "        }, save_path)\n",
    "        \n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'fold': fold_num,\n",
    "            'train_acc': best_train_acc,\n",
    "            'val_acc': best_val_acc,\n",
    "            'test_acc': best_test_acc,\n",
    "            'multiplicative_score': multiplicative_score,\n",
    "            'save_path': save_path\n",
    "        }\n",
    "        \n",
    "        print(f\"    {model_name} Fold {fold_num}: Train {best_train_acc:.1f}%, Val {best_val_acc:.1f}%, Test {best_test_acc:.1f}%, Mult {multiplicative_score:.1f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_comprehensive_evaluation(self, images, ages, sources):\n",
    "        print(f\"COMPREHENSIVE MODEL EVALUATION - {len(MODEL_CONFIGS)} models, {NUM_FOLDS} folds\")\n",
    "        print(f\"Image size: {IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}\")\n",
    "        print(f\"Total experiments: {len(MODEL_CONFIGS) * NUM_FOLDS}\")\n",
    "        \n",
    "        unique_ages = sorted(list(set(ages)))\n",
    "        label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "        y_indices = np.array([label_mapping[age] for age in ages])\n",
    "        \n",
    "        print(f\"Classes: {len(unique_ages)}\")\n",
    "        \n",
    "        for model_name, model_config in MODEL_CONFIGS.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"TESTING MODEL: {model_name.upper()}\")\n",
    "            print(f\"Batch size: {model_config['batch_size']}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            for fold in range(1, NUM_FOLDS + 1):\n",
    "                print(f\"\\n[Fold {fold}/{NUM_FOLDS}] {model_name}\")\n",
    "                \n",
    "                try:\n",
    "                    # Data splitting\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        images, y_indices, test_size=0.2, random_state=fold * 42, stratify=y_indices\n",
    "                    )\n",
    "                    \n",
    "                    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "                        X_train, y_train, test_size=0.2, random_state=fold * 42 + 1, stratify=y_train\n",
    "                    )\n",
    "                    \n",
    "                    # Create datasets\n",
    "                    train_dataset = OptimizedDataset(X_train_final, y_train_final, \n",
    "                                                   TRAINING_CONFIG['augmentation_strength'], AUGMENTATION_TARGET, True)\n",
    "                    val_dataset = OptimizedDataset(X_val, y_val, 'light', 200, False)\n",
    "                    test_dataset = OptimizedDataset(X_test, y_test, 'light', 200, False)\n",
    "                    \n",
    "                    # Create data loaders\n",
    "                    batch_size = model_config['batch_size']\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "                    \n",
    "                    # Create and train model\n",
    "                    model = self.create_model(model_name, model_config)\n",
    "                    result = self.train_model(model, model_name, train_loader, val_loader, test_loader, fold)\n",
    "                    self.results.append(result)\n",
    "                    \n",
    "                    # Cleanup\n",
    "                    del model, train_dataset, val_dataset, test_dataset, train_loader, val_loader, test_loader\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    FAILED: {str(e)}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        self.save_results()\n",
    "        self.print_summary()\n",
    "    \n",
    "    def save_results(self):\n",
    "        results_path = os.path.join(self.save_dir, \"comprehensive_results.json\")\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"\\nResults saved to: {results_path}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Group by model\n",
    "        model_results = {}\n",
    "        for result in self.results:\n",
    "            model_name = result['model_name']\n",
    "            if model_name not in model_results:\n",
    "                model_results[model_name] = []\n",
    "            model_results[model_name].append(result)\n",
    "        \n",
    "        # Print summary for each model\n",
    "        for model_name, results in model_results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "            \n",
    "            avg_train = np.mean([r['train_acc'] for r in results])\n",
    "            avg_val = np.mean([r['val_acc'] for r in results])\n",
    "            avg_test = np.mean([r['test_acc'] for r in results])\n",
    "            avg_mult = np.mean([r['multiplicative_score'] for r in results])\n",
    "            \n",
    "            print(f\"{model_name:20} | Train: {avg_train:5.1f}% | Val: {avg_val:5.1f}% | Test: {avg_test:5.1f}% | Mult: {avg_mult:6.1f}\")\n",
    "        \n",
    "        # Best overall model\n",
    "        if self.results:\n",
    "            best_result = max(self.results, key=lambda x: x['multiplicative_score'])\n",
    "            print(f\"\\nBest model: {best_result['model_name']} (Fold {best_result['fold']})\")\n",
    "            print(f\"Multiplicative score: {best_result['multiplicative_score']:.1f}\")\n",
    "            print(f\"Val: {best_result['val_acc']:.1f}%, Test: {best_result['test_acc']:.1f}%\")\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"RTX 5090 Comprehensive Model Evaluation\")\n",
    "    print(\"512x512 Images - Multiple Architectures\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    images, ages, sources = load_combined_data()\n",
    "    \n",
    "    trainer = MultiModelTrainer(num_classes=len(set(ages)))\n",
    "    trainer.run_comprehensive_evaluation(images, ages, sources)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nComplete evaluation finished in: {elapsed:.1f} minutes\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561f6ee-b954-43c4-b2f7-fcf0cb6639ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
