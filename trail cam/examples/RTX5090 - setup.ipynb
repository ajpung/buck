{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e5eff5-c905-4da2-b77d-3ed7a7ca0e9d",
   "metadata": {},
   "source": [
    "### Check RTX5090 running CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6ab5f0-9231-4ae0-8fc4-d27217483190",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20250922+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 5090\n",
      "GPU memory: 31.8 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 works!\n",
      "EfficientNet works!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Check if CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"❌ CUDA not detected by PyTorch\")\n",
    "\n",
    "# Test ResNet50 specifically\n",
    "model = models.resnet50(pretrained=True).cuda()\n",
    "test_batch = torch.randn(2, 3, 224, 224).cuda()\n",
    "try:\n",
    "    output = model(test_batch)\n",
    "    print(\"ResNet50 works!\")\n",
    "except Exception as e:\n",
    "    print(f\"ResNet50 failed: {e}\")\n",
    "\n",
    "# Test EfficientNet\n",
    "try:\n",
    "    model_eff = models.efficientnet_b0(pretrained=True).cuda()\n",
    "    output_eff = model_eff(test_batch)\n",
    "    print(\"EfficientNet works!\")\n",
    "except Exception as e:\n",
    "    print(f\"EfficientNet failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71bc11f-e647-462f-a1f3-466206538d7d",
   "metadata": {},
   "source": [
    "### Process deer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43154fba-18c6-4690-a3cd-6d4e8c863549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTX 5090 Comprehensive Model Evaluation\n",
      "512x512 Images - Multiple Architectures\n",
      "============================================================\n",
      "Loading color images...\n",
      "Loaded 202 color images\n",
      "Loading grayscale images...\n",
      "Loaded 40 grayscale images\n",
      "Total images: 242\n",
      "Final dataset: 242 images\n",
      "Age distribution: {2.5: 41, 3.5: 51, 4.5: 57, 5.5: 60, 1.5: 33}\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n",
      "GPU Memory: 34.2 GB\n",
      "COMPREHENSIVE MODEL EVALUATION - 17 models, 10 folds\n",
      "Image size: 600x600\n",
      "Total experiments: 170\n",
      "Classes: 5\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: MOBILENET_V2\n",
      "Batch size: 192\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] mobilenet_v2\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v2...\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-7ebf99e0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 13.6M/13.6M [00:00<00:00, 40.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 3,005,397 trainable, 6,960 frozen\n",
      "    Epoch 0: Train 42.9%, Val 56.6%\n",
      "    Epoch 20: Train 100.0%, Val 76.9%\n",
      "    Early stopping at epoch 32\n",
      "    mobilenet_v2 Fold 1: Train 99.7%, Val 80.5%, Test 61.8%, Mult 4974.9\n",
      "\n",
      "[Fold 2/10] mobilenet_v2\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v2...\n",
      "Parameters: 3,005,397 trainable, 6,960 frozen\n",
      "    Epoch 0: Train 46.5%, Val 44.8%\n",
      "    Epoch 20: Train 100.0%, Val 56.6%\n",
      "    Epoch 40: Train 100.0%, Val 59.2%\n",
      "    Early stopping at epoch 52\n",
      "    mobilenet_v2 Fold 2: Train 100.0%, Val 62.3%, Test 63.4%, Mult 3949.8\n",
      "\n",
      "[Fold 3/10] mobilenet_v2\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v2...\n",
      "Parameters: 3,005,397 trainable, 6,960 frozen\n",
      "    Epoch 0: Train 44.4%, Val 48.8%\n",
      "    Epoch 20: Train 100.0%, Val 63.4%\n",
      "    Epoch 40: Train 100.0%, Val 67.6%\n",
      "    Epoch 60: Train 100.0%, Val 65.4%\n",
      "    Early stopping at epoch 68\n",
      "    mobilenet_v2 Fold 3: Train 99.9%, Val 71.8%, Test 68.4%, Mult 4911.1\n",
      "\n",
      "[Fold 4/10] mobilenet_v2\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v2...\n",
      "Parameters: 3,005,397 trainable, 6,960 frozen\n",
      "    Epoch 0: Train 48.1%, Val 62.7%\n",
      "    Epoch 20: Train 99.9%, Val 68.0%\n",
      "    Early stopping at epoch 27\n",
      "    mobilenet_v2 Fold 4: Train 96.9%, Val 76.5%, Test 64.1%, Mult 4903.6\n",
      "\n",
      "[Fold 5/10] mobilenet_v2\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v2...\n",
      "Parameters: 3,005,397 trainable, 6,960 frozen\n",
      "    Epoch 0: Train 48.2%, Val 50.4%\n",
      "    Epoch 20: Train 99.9%, Val 63.7%\n",
      "    Early stopping at epoch 31\n",
      "    mobilenet_v2 Fold 5: Train 99.5%, Val 69.3%, Test 63.9%, Mult 4428.3\n",
      "\n",
      "[Fold 6/10] mobilenet_v2\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v2...\n",
      "Parameters: 3,005,397 trainable, 6,960 frozen\n",
      "    Epoch 0: Train 44.6%, Val 54.1%\n",
      "    Epoch 20: Train 99.9%, Val 65.5%\n",
      "    Epoch 40: Train 100.0%, Val 65.3%\n",
      "    Early stopping at epoch 53\n",
      "    mobilenet_v2 Fold 6: Train 100.0%, Val 75.1%, Test 56.6%, Mult 4250.7\n",
      "\n",
      "[Fold 7/10] mobilenet_v2\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v2...\n",
      "Parameters: 3,005,397 trainable, 6,960 frozen\n",
      "    Epoch 0: Train 48.1%, Val 53.7%\n",
      "    Epoch 20: Train 99.9%, Val 56.8%\n",
      "    Epoch 40: Train 100.0%, Val 61.5%\n",
      "    Epoch 60: Train 100.0%, Val 60.8%\n",
      "    Early stopping at epoch 74\n",
      "    mobilenet_v2 Fold 7: Train 100.0%, Val 69.5%, Test 69.2%, Mult 4809.4\n",
      "\n",
      "[Fold 8/10] mobilenet_v2\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v2...\n",
      "Parameters: 3,005,397 trainable, 6,960 frozen\n",
      "    Epoch 0: Train 51.0%, Val 42.7%\n",
      "    Epoch 20: Train 100.0%, Val 63.8%\n",
      "    Epoch 40: Train 100.0%, Val 59.6%\n",
      "    Early stopping at epoch 46\n",
      "    mobilenet_v2 Fold 8: Train 100.0%, Val 65.0%, Test 63.3%, Mult 4114.5\n",
      "\n",
      "[Fold 9/10] mobilenet_v2\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v2...\n",
      "Parameters: 3,005,397 trainable, 6,960 frozen\n",
      "    Epoch 0: Train 46.9%, Val 39.2%\n",
      "    Epoch 20: Train 100.0%, Val 61.7%\n",
      "    Epoch 40: Train 100.0%, Val 58.3%\n",
      "    Epoch 60: Train 100.0%, Val 61.7%\n",
      "    Early stopping at epoch 67\n",
      "    mobilenet_v2 Fold 9: Train 100.0%, Val 65.2%, Test 62.5%, Mult 4075.0\n",
      "\n",
      "[Fold 10/10] mobilenet_v2\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v2...\n",
      "Parameters: 3,005,397 trainable, 6,960 frozen\n",
      "    Epoch 0: Train 47.6%, Val 52.0%\n",
      "    Epoch 20: Train 100.0%, Val 75.7%\n",
      "    Early stopping at epoch 31\n",
      "    mobilenet_v2 Fold 10: Train 99.7%, Val 77.1%, Test 65.5%, Mult 5050.0\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: MOBILENET_V3_SMALL\n",
      "Batch size: 256\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] mobilenet_v3_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_small...\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\mobilenet_v3_small-047dcff4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9.83M/9.83M [00:00<00:00, 32.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 1,579,349 trainable, 5,072 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (256x576 and 1024x512)\n",
      "\n",
      "[Fold 2/10] mobilenet_v3_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_small...\n",
      "Parameters: 1,579,349 trainable, 5,072 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (256x576 and 1024x512)\n",
      "\n",
      "[Fold 3/10] mobilenet_v3_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_small...\n",
      "Parameters: 1,579,349 trainable, 5,072 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (256x576 and 1024x512)\n",
      "\n",
      "[Fold 4/10] mobilenet_v3_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_small...\n",
      "Parameters: 1,579,349 trainable, 5,072 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (256x576 and 1024x512)\n",
      "\n",
      "[Fold 5/10] mobilenet_v3_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_small...\n",
      "Parameters: 1,579,349 trainable, 5,072 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (256x576 and 1024x512)\n",
      "\n",
      "[Fold 6/10] mobilenet_v3_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_small...\n",
      "Parameters: 1,579,349 trainable, 5,072 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (256x576 and 1024x512)\n",
      "\n",
      "[Fold 7/10] mobilenet_v3_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_small...\n",
      "Parameters: 1,579,349 trainable, 5,072 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (256x576 and 1024x512)\n",
      "\n",
      "[Fold 8/10] mobilenet_v3_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_small...\n",
      "Parameters: 1,579,349 trainable, 5,072 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (256x576 and 1024x512)\n",
      "\n",
      "[Fold 9/10] mobilenet_v3_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_small...\n",
      "Parameters: 1,579,349 trainable, 5,072 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (256x576 and 1024x512)\n",
      "\n",
      "[Fold 10/10] mobilenet_v3_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_small...\n",
      "Parameters: 1,579,349 trainable, 5,072 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (256x576 and 1024x512)\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: MOBILENET_V3_LARGE\n",
      "Batch size: 192\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] mobilenet_v3_large\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_large...\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-5c1a4163.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\mobilenet_v3_large-5c1a4163.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 21.1M/21.1M [00:00<00:00, 55.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 3,756,069 trainable, 4,368 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (192x960 and 1280x512)\n",
      "\n",
      "[Fold 2/10] mobilenet_v3_large\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_large...\n",
      "Parameters: 3,756,069 trainable, 4,368 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (192x960 and 1280x512)\n",
      "\n",
      "[Fold 3/10] mobilenet_v3_large\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_large...\n",
      "Parameters: 3,756,069 trainable, 4,368 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (192x960 and 1280x512)\n",
      "\n",
      "[Fold 4/10] mobilenet_v3_large\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_large...\n",
      "Parameters: 3,756,069 trainable, 4,368 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (192x960 and 1280x512)\n",
      "\n",
      "[Fold 5/10] mobilenet_v3_large\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_large...\n",
      "Parameters: 3,756,069 trainable, 4,368 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (192x960 and 1280x512)\n",
      "\n",
      "[Fold 6/10] mobilenet_v3_large\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_large...\n",
      "Parameters: 3,756,069 trainable, 4,368 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (192x960 and 1280x512)\n",
      "\n",
      "[Fold 7/10] mobilenet_v3_large\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_large...\n",
      "Parameters: 3,756,069 trainable, 4,368 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (192x960 and 1280x512)\n",
      "\n",
      "[Fold 8/10] mobilenet_v3_large\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_large...\n",
      "Parameters: 3,756,069 trainable, 4,368 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (192x960 and 1280x512)\n",
      "\n",
      "[Fold 9/10] mobilenet_v3_large\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_large...\n",
      "Parameters: 3,756,069 trainable, 4,368 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (192x960 and 1280x512)\n",
      "\n",
      "[Fold 10/10] mobilenet_v3_large\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading mobilenet_v3_large...\n",
      "Parameters: 3,756,069 trainable, 4,368 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (192x960 and 1280x512)\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: REGNET_Y_400MF\n",
      "Batch size: 128\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] regnet_y_400mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_400mf...\n",
      "Downloading: \"https://download.pytorch.org/models/regnet_y_400mf-e6988f5f.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\regnet_y_400mf-e6988f5f.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 16.8M/16.8M [00:00<00:00, 47.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 2/10] regnet_y_400mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_400mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 3/10] regnet_y_400mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_400mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 4/10] regnet_y_400mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_400mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 5/10] regnet_y_400mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_400mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 6/10] regnet_y_400mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_400mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 7/10] regnet_y_400mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_400mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 8/10] regnet_y_400mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_400mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 9/10] regnet_y_400mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_400mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 10/10] regnet_y_400mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_400mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: REGNET_Y_800MF\n",
      "Batch size: 96\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] regnet_y_800mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_800mf...\n",
      "Downloading: \"https://download.pytorch.org/models/regnet_y_800mf-58fc7688.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\regnet_y_800mf-58fc7688.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 24.8M/24.8M [00:00<00:00, 52.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 2/10] regnet_y_800mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_800mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 3/10] regnet_y_800mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_800mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 4/10] regnet_y_800mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_800mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 5/10] regnet_y_800mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_800mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 6/10] regnet_y_800mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_800mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 7/10] regnet_y_800mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_800mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 8/10] regnet_y_800mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_800mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 9/10] regnet_y_800mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_800mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 10/10] regnet_y_800mf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_800mf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: REGNET_Y_1_6GF\n",
      "Batch size: 80\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] regnet_y_1_6gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_1_6gf...\n",
      "Downloading: \"https://download.pytorch.org/models/regnet_y_1_6gf-0d7bc02a.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\regnet_y_1_6gf-0d7bc02a.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 43.2M/43.2M [00:00<00:00, 67.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 2/10] regnet_y_1_6gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_1_6gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 3/10] regnet_y_1_6gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_1_6gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 4/10] regnet_y_1_6gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_1_6gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 5/10] regnet_y_1_6gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_1_6gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 6/10] regnet_y_1_6gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_1_6gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 7/10] regnet_y_1_6gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_1_6gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 8/10] regnet_y_1_6gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_1_6gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 9/10] regnet_y_1_6gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_1_6gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 10/10] regnet_y_1_6gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_1_6gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: REGNET_Y_3_2GF\n",
      "Batch size: 64\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] regnet_y_3_2gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_3_2gf...\n",
      "Downloading: \"https://download.pytorch.org/models/regnet_y_3_2gf-9180c971.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\regnet_y_3_2gf-9180c971.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 74.6M/74.6M [00:00<00:00, 82.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 2/10] regnet_y_3_2gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_3_2gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 3/10] regnet_y_3_2gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_3_2gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 4/10] regnet_y_3_2gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_3_2gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 5/10] regnet_y_3_2gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_3_2gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 6/10] regnet_y_3_2gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_3_2gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 7/10] regnet_y_3_2gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_3_2gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 8/10] regnet_y_3_2gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_3_2gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 9/10] regnet_y_3_2gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_3_2gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "[Fold 10/10] regnet_y_3_2gf\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading regnet_y_3_2gf...\n",
      "    FAILED: cannot access local variable 'layers_to_freeze' where it is not associated with a value\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: CONVNEXT_TINY\n",
      "Batch size: 96\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] convnext_tiny\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_tiny...\n",
      "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\convnext_tiny-983f1562.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 109M/109M [00:01<00:00, 84.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 28,102,149 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (73728x1 and 768x512)\n",
      "\n",
      "[Fold 2/10] convnext_tiny\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_tiny...\n",
      "Parameters: 28,102,149 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (73728x1 and 768x512)\n",
      "\n",
      "[Fold 3/10] convnext_tiny\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_tiny...\n",
      "Parameters: 28,102,149 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (73728x1 and 768x512)\n",
      "\n",
      "[Fold 4/10] convnext_tiny\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_tiny...\n",
      "Parameters: 28,102,149 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (73728x1 and 768x512)\n",
      "\n",
      "[Fold 5/10] convnext_tiny\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_tiny...\n",
      "Parameters: 28,102,149 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (73728x1 and 768x512)\n",
      "\n",
      "[Fold 6/10] convnext_tiny\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_tiny...\n",
      "Parameters: 28,102,149 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (73728x1 and 768x512)\n",
      "\n",
      "[Fold 7/10] convnext_tiny\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_tiny...\n",
      "Parameters: 28,102,149 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (73728x1 and 768x512)\n",
      "\n",
      "[Fold 8/10] convnext_tiny\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_tiny...\n",
      "Parameters: 28,102,149 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (73728x1 and 768x512)\n",
      "\n",
      "[Fold 9/10] convnext_tiny\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_tiny...\n",
      "Parameters: 28,102,149 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (73728x1 and 768x512)\n",
      "\n",
      "[Fold 10/10] convnext_tiny\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_tiny...\n",
      "Parameters: 28,102,149 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (73728x1 and 768x512)\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: CONVNEXT_SMALL\n",
      "Batch size: 80\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] convnext_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_small...\n",
      "Downloading: \"https://download.pytorch.org/models/convnext_small-0c510722.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\convnext_small-0c510722.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 192M/192M [00:02<00:00, 100MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 49,736,709 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (61440x1 and 768x512)\n",
      "\n",
      "[Fold 2/10] convnext_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_small...\n",
      "Parameters: 49,736,709 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (61440x1 and 768x512)\n",
      "\n",
      "[Fold 3/10] convnext_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_small...\n",
      "Parameters: 49,736,709 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (61440x1 and 768x512)\n",
      "\n",
      "[Fold 4/10] convnext_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_small...\n",
      "Parameters: 49,736,709 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (61440x1 and 768x512)\n",
      "\n",
      "[Fold 5/10] convnext_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_small...\n",
      "Parameters: 49,736,709 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (61440x1 and 768x512)\n",
      "\n",
      "[Fold 6/10] convnext_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_small...\n",
      "Parameters: 49,736,709 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (61440x1 and 768x512)\n",
      "\n",
      "[Fold 7/10] convnext_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_small...\n",
      "Parameters: 49,736,709 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (61440x1 and 768x512)\n",
      "\n",
      "[Fold 8/10] convnext_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_small...\n",
      "Parameters: 49,736,709 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (61440x1 and 768x512)\n",
      "\n",
      "[Fold 9/10] convnext_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_small...\n",
      "Parameters: 49,736,709 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (61440x1 and 768x512)\n",
      "\n",
      "[Fold 10/10] convnext_small\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_small...\n",
      "Parameters: 49,736,709 trainable, 242,784 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (61440x1 and 768x512)\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: CONVNEXT_BASE\n",
      "Batch size: 64\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] convnext_base\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_base...\n",
      "Downloading: \"https://download.pytorch.org/models/convnext_base-6075fbad.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\convnext_base-6075fbad.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 338M/338M [00:03<00:00, 99.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 87,799,813 trainable, 422,016 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1024x512)\n",
      "\n",
      "[Fold 2/10] convnext_base\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_base...\n",
      "Parameters: 87,799,813 trainable, 422,016 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1024x512)\n",
      "\n",
      "[Fold 3/10] convnext_base\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_base...\n",
      "Parameters: 87,799,813 trainable, 422,016 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1024x512)\n",
      "\n",
      "[Fold 4/10] convnext_base\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_base...\n",
      "Parameters: 87,799,813 trainable, 422,016 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1024x512)\n",
      "\n",
      "[Fold 5/10] convnext_base\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_base...\n",
      "Parameters: 87,799,813 trainable, 422,016 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1024x512)\n",
      "\n",
      "[Fold 6/10] convnext_base\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_base...\n",
      "Parameters: 87,799,813 trainable, 422,016 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1024x512)\n",
      "\n",
      "[Fold 7/10] convnext_base\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_base...\n",
      "Parameters: 87,799,813 trainable, 422,016 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1024x512)\n",
      "\n",
      "[Fold 8/10] convnext_base\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_base...\n",
      "Parameters: 87,799,813 trainable, 422,016 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1024x512)\n",
      "\n",
      "[Fold 9/10] convnext_base\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_base...\n",
      "Parameters: 87,799,813 trainable, 422,016 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1024x512)\n",
      "\n",
      "[Fold 10/10] convnext_base\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading convnext_base...\n",
      "Parameters: 87,799,813 trainable, 422,016 frozen\n",
      "    FAILED: mat1 and mat2 shapes cannot be multiplied (65536x1 and 1024x512)\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: MAXVIT_T\n",
      "Batch size: 48\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] maxvit_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading maxvit_t...\n",
      "Downloading: \"https://download.pytorch.org/models/maxvit_t-bc5ab103.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\maxvit_t-bc5ab103.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 119M/119M [00:01<00:00, 96.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 30,919,624 trainable, 0 frozen\n",
      "    FAILED: shape '[48, 64, 21, 7, 21, 7]' is invalid for input of size 69120000\n",
      "\n",
      "[Fold 2/10] maxvit_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading maxvit_t...\n",
      "Parameters: 30,919,624 trainable, 0 frozen\n",
      "    FAILED: shape '[48, 64, 21, 7, 21, 7]' is invalid for input of size 69120000\n",
      "\n",
      "[Fold 3/10] maxvit_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading maxvit_t...\n",
      "Parameters: 30,919,624 trainable, 0 frozen\n",
      "    FAILED: shape '[48, 64, 21, 7, 21, 7]' is invalid for input of size 69120000\n",
      "\n",
      "[Fold 4/10] maxvit_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading maxvit_t...\n",
      "Parameters: 30,919,624 trainable, 0 frozen\n",
      "    FAILED: shape '[48, 64, 21, 7, 21, 7]' is invalid for input of size 69120000\n",
      "\n",
      "[Fold 5/10] maxvit_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading maxvit_t...\n",
      "Parameters: 30,919,624 trainable, 0 frozen\n",
      "    FAILED: shape '[48, 64, 21, 7, 21, 7]' is invalid for input of size 69120000\n",
      "\n",
      "[Fold 6/10] maxvit_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading maxvit_t...\n",
      "Parameters: 30,919,624 trainable, 0 frozen\n",
      "    FAILED: shape '[48, 64, 21, 7, 21, 7]' is invalid for input of size 69120000\n",
      "\n",
      "[Fold 7/10] maxvit_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading maxvit_t...\n",
      "Parameters: 30,919,624 trainable, 0 frozen\n",
      "    FAILED: shape '[48, 64, 21, 7, 21, 7]' is invalid for input of size 69120000\n",
      "\n",
      "[Fold 8/10] maxvit_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading maxvit_t...\n",
      "Parameters: 30,919,624 trainable, 0 frozen\n",
      "    FAILED: shape '[48, 64, 21, 7, 21, 7]' is invalid for input of size 69120000\n",
      "\n",
      "[Fold 9/10] maxvit_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading maxvit_t...\n",
      "Parameters: 30,919,624 trainable, 0 frozen\n",
      "    FAILED: shape '[48, 64, 21, 7, 21, 7]' is invalid for input of size 69120000\n",
      "\n",
      "[Fold 10/10] maxvit_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading maxvit_t...\n",
      "Parameters: 30,919,624 trainable, 0 frozen\n",
      "    FAILED: shape '[48, 64, 21, 7, 21, 7]' is invalid for input of size 69120000\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: SWIN_T\n",
      "Batch size: 64\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] swin_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_t...\n",
      "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\swin_t-704ceda3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 108M/108M [00:01<00:00, 82.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 28,058,764 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 53.0%, Val 59.1%\n",
      "    Epoch 20: Train 99.9%, Val 72.5%\n",
      "    Early stopping at epoch 33\n",
      "    swin_t Fold 1: Train 99.9%, Val 76.1%, Test 58.0%, Mult 4413.8\n",
      "\n",
      "[Fold 2/10] swin_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_t...\n",
      "Parameters: 28,058,764 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 56.2%, Val 46.0%\n",
      "    Epoch 20: Train 100.0%, Val 55.2%\n",
      "    Early stopping at epoch 32\n",
      "    swin_t Fold 2: Train 99.9%, Val 58.3%, Test 65.7%, Mult 3830.3\n",
      "\n",
      "[Fold 3/10] swin_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_t...\n",
      "Parameters: 28,058,764 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 57.2%, Val 54.3%\n",
      "    Epoch 20: Train 100.0%, Val 65.6%\n",
      "    Epoch 40: Train 100.0%, Val 69.1%\n",
      "    Epoch 60: Train 100.0%, Val 64.5%\n",
      "    Early stopping at epoch 70\n",
      "    swin_t Fold 3: Train 100.0%, Val 69.2%, Test 60.1%, Mult 4158.9\n",
      "\n",
      "[Fold 4/10] swin_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_t...\n",
      "Parameters: 28,058,764 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 52.2%, Val 61.6%\n",
      "    Epoch 20: Train 100.0%, Val 75.1%\n",
      "    Early stopping at epoch 33\n",
      "    swin_t Fold 4: Train 99.9%, Val 82.2%, Test 67.7%, Mult 5564.9\n",
      "\n",
      "[Fold 5/10] swin_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_t...\n",
      "Parameters: 28,058,764 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 59.9%, Val 60.0%\n",
      "    Epoch 20: Train 99.9%, Val 64.0%\n",
      "    Epoch 40: Train 100.0%, Val 64.8%\n",
      "    Early stopping at epoch 51\n",
      "    swin_t Fold 5: Train 100.0%, Val 71.0%, Test 68.3%, Mult 4849.3\n",
      "\n",
      "[Fold 6/10] swin_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_t...\n",
      "Parameters: 28,058,764 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 52.2%, Val 68.9%\n",
      "    Epoch 20: Train 99.9%, Val 75.8%\n",
      "    Epoch 40: Train 100.0%, Val 76.9%\n",
      "    Epoch 60: Train 100.0%, Val 76.1%\n",
      "    Early stopping at epoch 69\n",
      "    swin_t Fold 6: Train 100.0%, Val 81.2%, Test 56.3%, Mult 4571.6\n",
      "\n",
      "[Fold 7/10] swin_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_t...\n",
      "Parameters: 28,058,764 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 55.6%, Val 54.1%\n",
      "    Epoch 20: Train 100.0%, Val 57.8%\n",
      "    Epoch 40: Train 99.9%, Val 52.1%\n",
      "    Early stopping at epoch 48\n",
      "    swin_t Fold 7: Train 99.8%, Val 61.5%, Test 56.9%, Mult 3499.3\n",
      "\n",
      "[Fold 8/10] swin_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_t...\n",
      "Parameters: 28,058,764 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 56.1%, Val 57.1%\n",
      "    Epoch 20: Train 100.0%, Val 64.1%\n",
      "    Epoch 40: Train 100.0%, Val 66.0%\n",
      "    Early stopping at epoch 54\n",
      "    swin_t Fold 8: Train 99.9%, Val 68.5%, Test 57.7%, Mult 3952.5\n",
      "\n",
      "[Fold 9/10] swin_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_t...\n",
      "Parameters: 28,058,764 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 58.1%, Val 56.9%\n",
      "    Epoch 20: Train 99.9%, Val 57.2%\n",
      "    Early stopping at epoch 28\n",
      "    swin_t Fold 9: Train 99.1%, Val 60.9%, Test 66.3%, Mult 4037.7\n",
      "\n",
      "[Fold 10/10] swin_t\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_t...\n",
      "Parameters: 28,058,764 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 55.9%, Val 54.5%\n",
      "    Epoch 20: Train 99.9%, Val 71.7%\n",
      "    Epoch 40: Train 99.9%, Val 68.1%\n",
      "    Early stopping at epoch 45\n",
      "    swin_t Fold 10: Train 99.9%, Val 71.7%, Test 66.4%, Mult 4760.9\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: SWIN_S\n",
      "Batch size: 48\n",
      "============================================================\n",
      "\n",
      "[Fold 1/10] swin_s\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_s...\n",
      "Downloading: \"https://download.pytorch.org/models/swin_s-5e29d889.pth\" to C:\\Users\\aaron/.cache\\torch\\hub\\checkpoints\\swin_s-5e29d889.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 190M/190M [00:03<00:00, 63.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 49,376,668 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 64.3%, Val 60.2%\n",
      "    Epoch 20: Train 99.8%, Val 72.4%\n",
      "    Early stopping at epoch 39\n",
      "    swin_s Fold 1: Train 100.0%, Val 77.2%, Test 56.5%, Mult 4361.8\n",
      "\n",
      "[Fold 2/10] swin_s\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_s...\n",
      "Parameters: 49,376,668 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 62.8%, Val 46.8%\n",
      "    Epoch 20: Train 99.9%, Val 52.5%\n",
      "    Epoch 40: Train 100.0%, Val 53.0%\n",
      "    Early stopping at epoch 52\n",
      "    swin_s Fold 2: Train 100.0%, Val 56.5%, Test 65.0%, Mult 3672.5\n",
      "\n",
      "[Fold 3/10] swin_s\n",
      "Dataset: 5000 samples from 154 base images\n",
      "Dataset: 1000 samples from 39 base images\n",
      "Dataset: 1000 samples from 49 base images\n",
      "Loading swin_s...\n",
      "Parameters: 49,376,668 trainable, 229,590 frozen\n",
      "    Epoch 0: Train 67.0%, Val 60.1%\n",
      "    FAILED: CUDA error: unknown error\n",
      "Search for `cudaErrorUnknown' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: unknown error\nSearch for `cudaErrorUnknown' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 616\u001b[39m, in \u001b[36mMultiModelTrainer.run_comprehensive_evaluation\u001b[39m\u001b[34m(self, images, ages, sources)\u001b[39m\n\u001b[32m    615\u001b[39m model = \u001b[38;5;28mself\u001b[39m.create_model(model_name, model_config)\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[38;5;28mself\u001b[39m.results.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 464\u001b[39m, in \u001b[36mMultiModelTrainer.train_model\u001b[39m\u001b[34m(self, model, model_name, train_loader, val_loader, test_loader, fold_num)\u001b[39m\n\u001b[32m    462\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.step(optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: unknown error\nSearch for `cudaErrorUnknown' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 692\u001b[39m\n\u001b[32m    689\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mComplete evaluation finished in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 686\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    683\u001b[39m images, ages, sources = load_combined_data()\n\u001b[32m    685\u001b[39m trainer = MultiModelTrainer(num_classes=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(ages)))\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_comprehensive_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m elapsed = (time.time() - start_time) / \u001b[32m60\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mComplete evaluation finished in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 626\u001b[39m, in \u001b[36mMultiModelTrainer.run_comprehensive_evaluation\u001b[39m\u001b[34m(self, images, ages, sources)\u001b[39m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    FAILED: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m     gc.collect()\n\u001b[32m    628\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dropbox\\AI Projects\\buck\\buck-env\\Lib\\site-packages\\torch\\cuda\\memory.py:224\u001b[39m, in \u001b[36mempty_cache\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[33;03m`nvidia-smi`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    221\u001b[39m \u001b[33;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: unknown error\nSearch for `cudaErrorUnknown' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RTX 5090 Optimized Configuration\n",
    "IMAGE_SIZE = (600, 600)  # Scaled up from 224x224\n",
    "AUGMENTATION_TARGET = 1000\n",
    "NUM_FOLDS = 10\n",
    "NUM_WORKERS = 0\n",
    "MIXED_PRECISION = True\n",
    "COMPILE_MODEL = False\n",
    "\n",
    "# Model configurations with appropriate batch sizes for 512x512 on RTX 5090\n",
    "MODEL_CONFIGS = {\n",
    "    # EfficientNet family\n",
    "    'efficientnet_b0': {'model_fn': models.efficientnet_b0, 'batch_size': 128, 'freeze_layers': 3},\n",
    "    'efficientnet_b1': {'model_fn': models.efficientnet_b1, 'batch_size': 96, 'freeze_layers': 3},\n",
    "    'efficientnet_b2': {'model_fn': models.efficientnet_b2, 'batch_size': 80, 'freeze_layers': 3},\n",
    "    'efficientnet_b3': {'model_fn': models.efficientnet_b3, 'batch_size': 64, 'freeze_layers': 3},\n",
    "    'efficientnet_b4': {'model_fn': models.efficientnet_b4, 'batch_size': 48, 'freeze_layers': 3},\n",
    "    'efficientnet_b5': {'model_fn': models.efficientnet_b5, 'batch_size': 32, 'freeze_layers': 3},\n",
    "    'efficientnet_b6': {'model_fn': models.efficientnet_b6, 'batch_size': 24, 'freeze_layers': 3},\n",
    "    'efficientnet_b7': {'model_fn': models.efficientnet_b7, 'batch_size': 16, 'freeze_layers': 3},\n",
    "    \n",
    "    # ResNet family\n",
    "    'resnet18': {'model_fn': models.resnet18, 'batch_size': 256, 'freeze_layers': 2},\n",
    "    'resnet34': {'model_fn': models.resnet34, 'batch_size': 192, 'freeze_layers': 2},\n",
    "    'resnet50': {'model_fn': models.resnet50, 'batch_size': 128, 'freeze_layers': 2},\n",
    "    'resnet101': {'model_fn': models.resnet101, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'resnet152': {'model_fn': models.resnet152, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # DenseNet family\n",
    "    'densenet121': {'model_fn': models.densenet121, 'batch_size': 96, 'freeze_layers': 2},\n",
    "    'densenet169': {'model_fn': models.densenet169, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'densenet201': {'model_fn': models.densenet201, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # MobileNet family\n",
    "    'mobilenet_v2': {'model_fn': models.mobilenet_v2, 'batch_size': 192, 'freeze_layers': 3},\n",
    "    'mobilenet_v3_small': {'model_fn': models.mobilenet_v3_small, 'batch_size': 256, 'freeze_layers': 3},\n",
    "    'mobilenet_v3_large': {'model_fn': models.mobilenet_v3_large, 'batch_size': 192, 'freeze_layers': 3},\n",
    "    \n",
    "    # RegNet family\n",
    "    'regnet_y_400mf': {'model_fn': models.regnet_y_400mf, 'batch_size': 128, 'freeze_layers': 2},\n",
    "    'regnet_y_800mf': {'model_fn': models.regnet_y_800mf, 'batch_size': 96, 'freeze_layers': 2},\n",
    "    'regnet_y_1_6gf': {'model_fn': models.regnet_y_1_6gf, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'regnet_y_3_2gf': {'model_fn': models.regnet_y_3_2gf, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # ConvNeXt family\n",
    "    'convnext_tiny': {'model_fn': models.convnext_tiny, 'batch_size': 96, 'freeze_layers': 2},\n",
    "    'convnext_small': {'model_fn': models.convnext_small, 'batch_size': 80, 'freeze_layers': 2},\n",
    "    'convnext_base': {'model_fn': models.convnext_base, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    \n",
    "    # MaxViT family\n",
    "    'maxvit_t': {'model_fn': models.maxvit_t, 'batch_size': 48, 'freeze_layers': 2},\n",
    "    \n",
    "    # Swin Transformer family\n",
    "    'swin_t': {'model_fn': models.swin_t, 'batch_size': 64, 'freeze_layers': 2},\n",
    "    'swin_s': {'model_fn': models.swin_s, 'batch_size': 48, 'freeze_layers': 2},\n",
    "    'swin_b': {'model_fn': models.swin_b, 'batch_size': 32, 'freeze_layers': 2},\n",
    "    \n",
    "    # Vision Transformer family\n",
    "    'vit_b_16': {'model_fn': models.vit_b_16, 'batch_size': 64, 'freeze_layers': 6},\n",
    "    'vit_b_32': {'model_fn': models.vit_b_32, 'batch_size': 96, 'freeze_layers': 6},\n",
    "    'vit_l_16': {'model_fn': models.vit_l_16, 'batch_size': 32, 'freeze_layers': 8},\n",
    "}\n",
    "\n",
    "# Training hyperparameters\n",
    "TRAINING_CONFIG = {\n",
    "    'backbone_lr': 0.0001,\n",
    "    'classifier_lr': 0.0005,\n",
    "    'optimizer': 'adamw',\n",
    "    'weight_decay': 0.05,\n",
    "    'scheduler': 'cosine',\n",
    "    'label_smoothing': 0.1,\n",
    "    'dropout': 0.3,\n",
    "    'max_epochs': 80,\n",
    "    'patience': 25,\n",
    "    'augmentation_strength': 'medium'\n",
    "}\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    color_path = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\color\\\\*_NDA.png\"\n",
    "    gray_path = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\images\\\\squared\\\\grayscale\\\\*_NDA.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    ages_grouped = [5.5 if age >= 5.5 else age for age in ages]\n",
    "    \n",
    "    age_counts = Counter(ages_grouped)\n",
    "    valid_ages = {age for age, count in age_counts.items() if count >= 3}\n",
    "    \n",
    "    filtered_images = []\n",
    "    filtered_ages = []\n",
    "    filtered_sources = []\n",
    "    \n",
    "    for img, age, source in zip(images, ages_grouped, sources):\n",
    "        if age in valid_ages:\n",
    "            filtered_images.append(img)\n",
    "            filtered_ages.append(age)\n",
    "            filtered_sources.append(source)\n",
    "    \n",
    "    print(f\"Final dataset: {len(filtered_images)} images\")\n",
    "    print(f\"Age distribution: {dict(Counter(filtered_ages))}\")\n",
    "    \n",
    "    return np.array(filtered_images, dtype=np.uint8), filtered_ages, filtered_sources\n",
    "\n",
    "def enhanced_augment_image(image, strength='medium'):\n",
    "    if image.dtype != np.uint8:\n",
    "        image = image.astype(np.uint8)\n",
    "    \n",
    "    if strength == 'light':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.5, 0.3, 0.6, 0.2, 0.1\n",
    "        rot_range, bright_range = 8, (0.85, 1.15)\n",
    "    elif strength == 'medium':\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.7, 0.5, 0.8, 0.4, 0.3\n",
    "        rot_range, bright_range = 12, (0.75, 1.25)\n",
    "    else:  # heavy\n",
    "        rot_prob, flip_prob, bright_prob, gamma_prob, noise_prob = 0.8, 0.6, 0.9, 0.5, 0.4\n",
    "        rot_range, bright_range = 18, (0.65, 1.35)\n",
    "    \n",
    "    if random.random() < rot_prob:\n",
    "        angle = random.uniform(-rot_range, rot_range)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    if random.random() < flip_prob:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    if len(image.shape) == 3 and image.shape[2] == 3 and random.random() < 0.3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    if random.random() < bright_prob:\n",
    "        alpha = random.uniform(*bright_range)\n",
    "        beta = random.randint(-20, 20)\n",
    "        image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    \n",
    "    if random.random() < gamma_prob:\n",
    "        gamma = random.uniform(0.85, 1.15)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    if random.random() < noise_prob:\n",
    "        noise = np.random.normal(0, 5, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class OptimizedDataset(Dataset):\n",
    "    def __init__(self, base_images, labels, aug_strength='medium', target_per_class=1000, training=True):\n",
    "        self.base_images = base_images\n",
    "        self.labels = np.array(labels)\n",
    "        self.aug_strength = aug_strength\n",
    "        self.training = training\n",
    "        self.target_per_class = target_per_class\n",
    "        \n",
    "        unique_classes = np.unique(labels)\n",
    "        self.class_to_indices = {}\n",
    "        for cls in unique_classes:\n",
    "            self.class_to_indices[cls] = np.where(self.labels == cls)[0]\n",
    "        \n",
    "        self.num_classes = len(unique_classes)\n",
    "        self.class_list = sorted(unique_classes)\n",
    "        self.length = self.num_classes * self.target_per_class\n",
    "        \n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(3, 1, 1)\n",
    "        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(3, 1, 1)\n",
    "        \n",
    "        print(f\"Dataset: {self.length} samples from {len(base_images)} base images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        class_idx = idx // self.target_per_class\n",
    "        within_class_idx = idx % self.target_per_class\n",
    "        \n",
    "        target_class = self.class_list[class_idx]\n",
    "        available_indices = self.class_to_indices[target_class]\n",
    "        \n",
    "        base_idx = available_indices[within_class_idx % len(available_indices)]\n",
    "        image = self.base_images[base_idx].copy()\n",
    "        \n",
    "        if self.training and within_class_idx >= len(available_indices):\n",
    "            image = enhanced_augment_image(image, self.aug_strength)\n",
    "        \n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.transpose(2, 0, 1)\n",
    "        \n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = np.flip(image, axis=2).copy()\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return torch.from_numpy(image.astype(np.float32)), target_class\n",
    "\n",
    "class MultiModelTrainer:\n",
    "    def __init__(self, num_classes, save_dir=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if save_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            self.save_dir = f\"multimodel_512_{timestamp}\"\n",
    "        else:\n",
    "            self.save_dir = save_dir\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.results = []\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "            \n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            \n",
    "            if MIXED_PRECISION:\n",
    "                self.scaler = torch.amp.GradScaler('cuda')\n",
    "            else:\n",
    "                self.scaler = None\n",
    "    \n",
    "    def create_model(self, model_name, model_config):\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        model = model_config['model_fn'](weights='DEFAULT')\n",
    "        \n",
    "        # Freeze layers based on model type\n",
    "        freeze_layers = model_config.get('freeze_layers', 2)\n",
    "        \n",
    "        if hasattr(model, 'features'):  # EfficientNet, DenseNet, etc.\n",
    "            layers_to_freeze = list(model.features.children())[:freeze_layers]\n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            if hasattr(model, 'classifier'):\n",
    "                original_features = model.classifier[-1].in_features if isinstance(model.classifier, nn.Sequential) else model.classifier.in_features\n",
    "                model.classifier = nn.Sequential(\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                    nn.Linear(original_features, 512),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                    nn.Linear(256, self.num_classes)\n",
    "                )\n",
    "        \n",
    "        elif hasattr(model, 'fc'):  # ResNet, RegNet\n",
    "            if hasattr(model, 'layer1'):  # ResNet\n",
    "                layers_to_freeze = [model.conv1, model.bn1]\n",
    "                if freeze_layers >= 1:\n",
    "                    layers_to_freeze.append(model.layer1)\n",
    "                if freeze_layers >= 2:\n",
    "                    layers_to_freeze.append(model.layer2)\n",
    "            \n",
    "            for layer in layers_to_freeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            original_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        elif hasattr(model, 'head'):  # Vision Transformers, Swin, ConvNeXt\n",
    "            # Freeze some transformer blocks\n",
    "            if hasattr(model, 'encoder') and hasattr(model.encoder, 'layers'):  # ViT\n",
    "                layers_to_freeze = list(model.encoder.layers.children())[:freeze_layers]\n",
    "                for layer in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            elif hasattr(model, 'features'):  # ConvNeXt\n",
    "                layers_to_freeze = list(model.features.children())[:freeze_layers]\n",
    "                for layer in layers_to_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            \n",
    "            original_features = model.head.in_features\n",
    "            model.head = nn.Sequential(\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout']),\n",
    "                nn.Linear(original_features, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.5),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(TRAINING_CONFIG['dropout'] * 0.25),\n",
    "                nn.Linear(256, self.num_classes)\n",
    "            )\n",
    "        \n",
    "        frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Parameters: {trainable_params:,} trainable, {frozen_params:,} frozen\")\n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def get_optimizer(self, model):\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if any(keyword in name for keyword in ['classifier', 'fc', 'head']):\n",
    "                    classifier_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        param_groups = [\n",
    "            {'params': backbone_params, 'lr': TRAINING_CONFIG['backbone_lr']},\n",
    "            {'params': classifier_params, 'lr': TRAINING_CONFIG['classifier_lr']}\n",
    "        ]\n",
    "        \n",
    "        return optim.AdamW(param_groups, weight_decay=TRAINING_CONFIG['weight_decay'], fused=True)\n",
    "    \n",
    "    def get_scheduler(self, optimizer):\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=TRAINING_CONFIG['max_epochs'], eta_min=1e-6\n",
    "        )\n",
    "    \n",
    "    def train_model(self, model, model_name, train_loader, val_loader, test_loader, fold_num):\n",
    "        optimizer = self.get_optimizer(model)\n",
    "        scheduler = self.get_scheduler(optimizer)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=TRAINING_CONFIG['label_smoothing'])\n",
    "        \n",
    "        best_val_acc = 0.0\n",
    "        best_train_acc = 0.0\n",
    "        best_test_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(TRAINING_CONFIG['max_epochs']):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if MIXED_PRECISION and self.scaler:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.step(optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    if MIXED_PRECISION:\n",
    "                        with torch.amp.autocast('cuda'):\n",
    "                            outputs = model(images)\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_train_acc = train_acc\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "                \n",
    "                # Test accuracy when validation improves\n",
    "                model.eval()\n",
    "                test_correct = 0\n",
    "                test_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for images, labels in test_loader:\n",
    "                        images, labels = images.to(self.device), labels.to(self.device)\n",
    "                        \n",
    "                        if MIXED_PRECISION:\n",
    "                            with torch.amp.autocast('cuda'):\n",
    "                                outputs1 = model(images)\n",
    "                                outputs2 = model(torch.flip(images, [3]))\n",
    "                                outputs = (outputs1 + outputs2) / 2\n",
    "                        else:\n",
    "                            outputs1 = model(images)\n",
    "                            outputs2 = model(torch.flip(images, [3]))\n",
    "                            outputs = (outputs1 + outputs2) / 2\n",
    "                        \n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        test_total += labels.size(0)\n",
    "                        test_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                best_test_acc = 100 * test_correct / test_total\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"    Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "            \n",
    "            if patience_counter >= TRAINING_CONFIG['patience']:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Calculate multiplicative metric\n",
    "        multiplicative_score = (best_val_acc / 100) * (best_test_acc / 100) * 10000\n",
    "        \n",
    "        # Save model\n",
    "        save_path = os.path.join(self.save_dir, f\"{model_name}_fold{fold_num}_val{best_val_acc:.1f}_test{best_test_acc:.1f}_mult{multiplicative_score:.1f}.pth\")\n",
    "        torch.save({\n",
    "            'model_state_dict': best_state,\n",
    "            'model_name': model_name,\n",
    "            'fold_number': fold_num,\n",
    "            'train_accuracy': best_train_acc,\n",
    "            'validation_accuracy': best_val_acc,\n",
    "            'test_accuracy': best_test_acc,\n",
    "            'multiplicative_score': multiplicative_score,\n",
    "            'image_size': IMAGE_SIZE,\n",
    "            'training_config': TRAINING_CONFIG\n",
    "        }, save_path)\n",
    "        \n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'fold': fold_num,\n",
    "            'train_acc': best_train_acc,\n",
    "            'val_acc': best_val_acc,\n",
    "            'test_acc': best_test_acc,\n",
    "            'multiplicative_score': multiplicative_score,\n",
    "            'save_path': save_path\n",
    "        }\n",
    "        \n",
    "        print(f\"    {model_name} Fold {fold_num}: Train {best_train_acc:.1f}%, Val {best_val_acc:.1f}%, Test {best_test_acc:.1f}%, Mult {multiplicative_score:.1f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_comprehensive_evaluation(self, images, ages, sources):\n",
    "        print(f\"COMPREHENSIVE MODEL EVALUATION - {len(MODEL_CONFIGS)} models, {NUM_FOLDS} folds\")\n",
    "        print(f\"Image size: {IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}\")\n",
    "        print(f\"Total experiments: {len(MODEL_CONFIGS) * NUM_FOLDS}\")\n",
    "        \n",
    "        unique_ages = sorted(list(set(ages)))\n",
    "        label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "        y_indices = np.array([label_mapping[age] for age in ages])\n",
    "        \n",
    "        print(f\"Classes: {len(unique_ages)}\")\n",
    "        \n",
    "        for model_name, model_config in MODEL_CONFIGS.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"TESTING MODEL: {model_name.upper()}\")\n",
    "            print(f\"Batch size: {model_config['batch_size']}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            for fold in range(1, NUM_FOLDS + 1):\n",
    "                print(f\"\\n[Fold {fold}/{NUM_FOLDS}] {model_name}\")\n",
    "                \n",
    "                try:\n",
    "                    # Data splitting\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        images, y_indices, test_size=0.2, random_state=fold * 42, stratify=y_indices\n",
    "                    )\n",
    "                    \n",
    "                    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "                        X_train, y_train, test_size=0.2, random_state=fold * 42 + 1, stratify=y_train\n",
    "                    )\n",
    "                    \n",
    "                    # Create datasets\n",
    "                    train_dataset = OptimizedDataset(X_train_final, y_train_final, \n",
    "                                                   TRAINING_CONFIG['augmentation_strength'], AUGMENTATION_TARGET, True)\n",
    "                    val_dataset = OptimizedDataset(X_val, y_val, 'light', 200, False)\n",
    "                    test_dataset = OptimizedDataset(X_test, y_test, 'light', 200, False)\n",
    "                    \n",
    "                    # Create data loaders\n",
    "                    batch_size = model_config['batch_size']\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "                    \n",
    "                    # Create and train model\n",
    "                    model = self.create_model(model_name, model_config)\n",
    "                    result = self.train_model(model, model_name, train_loader, val_loader, test_loader, fold)\n",
    "                    self.results.append(result)\n",
    "                    \n",
    "                    # Cleanup\n",
    "                    del model, train_dataset, val_dataset, test_dataset, train_loader, val_loader, test_loader\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    FAILED: {str(e)}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        self.save_results()\n",
    "        self.print_summary()\n",
    "    \n",
    "    def save_results(self):\n",
    "        results_path = os.path.join(self.save_dir, \"comprehensive_results.json\")\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"\\nResults saved to: {results_path}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Group by model\n",
    "        model_results = {}\n",
    "        for result in self.results:\n",
    "            model_name = result['model_name']\n",
    "            if model_name not in model_results:\n",
    "                model_results[model_name] = []\n",
    "            model_results[model_name].append(result)\n",
    "        \n",
    "        # Print summary for each model\n",
    "        for model_name, results in model_results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "            \n",
    "            avg_train = np.mean([r['train_acc'] for r in results])\n",
    "            avg_val = np.mean([r['val_acc'] for r in results])\n",
    "            avg_test = np.mean([r['test_acc'] for r in results])\n",
    "            avg_mult = np.mean([r['multiplicative_score'] for r in results])\n",
    "            \n",
    "            print(f\"{model_name:20} | Train: {avg_train:5.1f}% | Val: {avg_val:5.1f}% | Test: {avg_test:5.1f}% | Mult: {avg_mult:6.1f}\")\n",
    "        \n",
    "        # Best overall model\n",
    "        if self.results:\n",
    "            best_result = max(self.results, key=lambda x: x['multiplicative_score'])\n",
    "            print(f\"\\nBest model: {best_result['model_name']} (Fold {best_result['fold']})\")\n",
    "            print(f\"Multiplicative score: {best_result['multiplicative_score']:.1f}\")\n",
    "            print(f\"Val: {best_result['val_acc']:.1f}%, Test: {best_result['test_acc']:.1f}%\")\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"RTX 5090 Comprehensive Model Evaluation\")\n",
    "    print(\"512x512 Images - Multiple Architectures\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    images, ages, sources = load_combined_data()\n",
    "    \n",
    "    trainer = MultiModelTrainer(num_classes=len(set(ages)))\n",
    "    trainer.run_comprehensive_evaluation(images, ages, sources)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nComplete evaluation finished in: {elapsed:.1f} minutes\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561f6ee-b954-43c4-b2f7-fcf0cb6639ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
