{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af5a6cf4-eb3e-426d-add1-6e64378cf540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n",
      "\n",
      "Loading data...\n",
      "Loading color images...\n",
      "Loaded 368 color images\n",
      "Loading grayscale images...\n",
      "Loaded 112 grayscale images\n",
      "Total images: 480\n",
      "\n",
      "Age distribution: {np.float64(1.5): np.int64(67), np.float64(2.5): np.int64(85), np.float64(3.5): np.int64(115), np.float64(4.5): np.int64(90), np.float64(5.5): np.int64(91), np.float64(6.5): np.int64(22), np.float64(7.5): np.int64(1), np.float64(8.5): np.int64(6), np.float64(9.5): np.int64(1), np.float64(12.5): np.int64(2)}\n",
      "Classes: [np.float64(1.5), np.float64(2.5), np.float64(3.5), np.float64(4.5), np.float64(5.5), np.float64(6.5), np.float64(7.5), np.float64(8.5), np.float64(9.5), np.float64(12.5)]\n",
      "Label indices distribution: {np.int64(0): np.int64(67), np.int64(1): np.int64(85), np.int64(2): np.int64(115), np.int64(3): np.int64(90), np.int64(4): np.int64(91), np.int64(5): np.int64(22), np.int64(6): np.int64(1), np.int64(7): np.int64(6), np.int64(8): np.int64(1), np.int64(9): np.int64(2)}\n",
      "Minimum class count: 1\n",
      "\n",
      "WARNING: Cannot stratify - some classes have only 1 sample\n",
      "Using non-stratified split instead...\n",
      "Test set: 96 base images\n",
      "Dataset: 1600 samples from 96 base images\n",
      "\n",
      "Loading model...\n",
      "Model loaded from: .\\all_images_multimodel_512_20250915_184924\\efficientnet_b5_fold5_val80.4_test78.4_mult6303.4.pth\n",
      "Validation accuracy: 80.4%\n",
      "Test accuracy: 78.4%\n",
      "\n",
      "Evaluating model on held-out test set with augmentation...\n",
      "\n",
      "Test Accuracy (with augmentation): 37.00%\n",
      "Expected from checkpoint: 78.4%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IMAGE_SIZE = (600, 600)\n",
    "MODEL_PATH = \".\\\\all_images_multimodel_512_20250915_184924\\\\efficientnet_b5_fold5_val80.4_test78.4_mult6303.4.pth\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 5\n",
    "FOLD = 5\n",
    "\n",
    "def detect_and_convert_image(image):\n",
    "    if len(image.shape) == 2:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            return image\n",
    "        elif image.shape[2] == 4:\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    return image\n",
    "\n",
    "def load_combined_data():\n",
    "    color_path = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\trail cam\\\\images\\\\squared\\\\color\\\\*.png\"\n",
    "    gray_path = \"D:\\\\Dropbox\\\\AI Projects\\\\buck\\\\trail cam\\\\images\\\\squared\\\\grayscale\\\\*.png\"\n",
    "    \n",
    "    images = []\n",
    "    ages = []\n",
    "    sources = []\n",
    "    \n",
    "    print(\"Loading color images...\")\n",
    "    color_files = glob.glob(color_path)\n",
    "    for img_path in color_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('color')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'color'])} color images\")\n",
    "    \n",
    "    print(\"Loading grayscale images...\")\n",
    "    gray_files = glob.glob(gray_path)\n",
    "    for img_path in gray_files:\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = detect_and_convert_image(img)\n",
    "            img_resized = cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "            \n",
    "            filename = os.path.basename(img_path)\n",
    "            filename_no_ext = os.path.splitext(filename)[0]\n",
    "            parts = filename_no_ext.split('_')\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            \n",
    "            age_part = parts[3]\n",
    "            if 'xpx' in age_part.lower() or 'p' not in age_part:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                age_value = float(age_part.replace('p', '.'))\n",
    "                images.append(img_resized)\n",
    "                ages.append(age_value)\n",
    "                sources.append('grayscale')\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len([s for s in sources if s == 'grayscale'])} grayscale images\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    \n",
    "    return np.array(images), np.array(ages), sources\n",
    "\n",
    "def enhanced_augment_image(image, strength='light'):\n",
    "    if strength == 'light':\n",
    "        rotate_prob = 0.3\n",
    "        flip_prob = 0.3\n",
    "        brightness_prob = 0.2\n",
    "        gamma_prob = 0.2\n",
    "        noise_prob = 0.1\n",
    "    else:\n",
    "        rotate_prob = 0.5\n",
    "        flip_prob = 0.5\n",
    "        brightness_prob = 0.4\n",
    "        gamma_prob = 0.3\n",
    "        noise_prob = 0.2\n",
    "    \n",
    "    if random.random() < rotate_prob:\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)\n",
    "        image = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "    \n",
    "    if random.random() < flip_prob:\n",
    "        image = cv2.flip(image, 1)\n",
    "    \n",
    "    if random.random() < brightness_prob:\n",
    "        factor = random.uniform(0.85, 1.15)\n",
    "        image = np.clip(image * factor, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    if random.random() < gamma_prob:\n",
    "        gamma = random.uniform(0.85, 1.15)\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        image = cv2.LUT(image, table)\n",
    "    \n",
    "    if random.random() < noise_prob:\n",
    "        noise = np.random.normal(0, 5, image.shape).astype(np.int16)\n",
    "        image_int16 = image.astype(np.int16)\n",
    "        noisy_image = np.clip(image_int16 + noise, 0, 255)\n",
    "        image = noisy_image.astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class OptimizedDataset(Dataset):\n",
    "    def __init__(self, base_images, labels, aug_strength='light', target_per_class=200, training=False):\n",
    "        self.base_images = base_images\n",
    "        self.labels = np.array(labels)\n",
    "        self.aug_strength = aug_strength\n",
    "        self.training = training\n",
    "        self.target_per_class = target_per_class\n",
    "        \n",
    "        unique_classes = np.unique(labels)\n",
    "        self.class_to_indices = {}\n",
    "        for cls in unique_classes:\n",
    "            self.class_to_indices[cls] = np.where(self.labels == cls)[0]\n",
    "        \n",
    "        self.num_classes = len(unique_classes)\n",
    "        self.class_list = sorted(unique_classes)\n",
    "        self.length = self.num_classes * self.target_per_class\n",
    "        \n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(3, 1, 1)\n",
    "        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(3, 1, 1)\n",
    "        \n",
    "        print(f\"Dataset: {self.length} samples from {len(base_images)} base images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        class_idx = idx // self.target_per_class\n",
    "        within_class_idx = idx % self.target_per_class\n",
    "        \n",
    "        target_class = self.class_list[class_idx]\n",
    "        available_indices = self.class_to_indices[target_class]\n",
    "        \n",
    "        base_idx = available_indices[within_class_idx % len(available_indices)]\n",
    "        image = self.base_images[base_idx].copy()\n",
    "        \n",
    "        if within_class_idx >= len(available_indices):\n",
    "            image = enhanced_augment_image(image, self.aug_strength)\n",
    "        \n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.transpose(2, 0, 1)\n",
    "        \n",
    "        if not self.training and random.random() < 0.5:\n",
    "            image = np.flip(image, axis=2).copy()\n",
    "        \n",
    "        image = (image - self.mean) / self.std\n",
    "        \n",
    "        return torch.from_numpy(image.astype(np.float32)), target_class\n",
    "\n",
    "def load_model(model_path, num_classes, device):\n",
    "    model = models.efficientnet_b5(weights='DEFAULT')\n",
    "    \n",
    "    layers_to_freeze = list(model.features.children())[:3]\n",
    "    for layer in layers_to_freeze:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if isinstance(model.classifier, nn.Sequential):\n",
    "        original_features = model.classifier[-1].in_features\n",
    "    else:\n",
    "        original_features = model.classifier.in_features\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(original_features, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.15),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.075),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded from: {model_path}\")\n",
    "    print(f\"Validation accuracy: {checkpoint['validation_accuracy']:.1f}%\")\n",
    "    print(f\"Test accuracy: {checkpoint['test_accuracy']:.1f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs1 = model(images)\n",
    "                outputs2 = model(torch.flip(images, [3]))\n",
    "                outputs = (outputs1 + outputs2) / 2\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    \n",
    "    print(\"\\nLoading data...\")\n",
    "    images, ages, sources = load_combined_data()\n",
    "    \n",
    "    unique_ages = sorted(list(set(ages)))\n",
    "    label_mapping = {age: i for i, age in enumerate(unique_ages)}\n",
    "    y_indices = np.array([label_mapping[age] for age in ages])\n",
    "    \n",
    "    print(f\"\\nAge distribution: {dict(zip(*np.unique(ages, return_counts=True)))}\")\n",
    "    print(f\"Classes: {unique_ages}\")\n",
    "    print(f\"Label indices distribution: {dict(zip(*np.unique(y_indices, return_counts=True)))}\")\n",
    "    \n",
    "    # Check if we can stratify\n",
    "    min_class_count = min(np.bincount(y_indices))\n",
    "    print(f\"Minimum class count: {min_class_count}\")\n",
    "    \n",
    "    if min_class_count < 2:\n",
    "        print(\"\\nWARNING: Cannot stratify - some classes have only 1 sample\")\n",
    "        print(\"Using non-stratified split instead...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            images, y_indices, test_size=0.2, random_state=FOLD * 42, stratify=None\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\nReplicating fold {FOLD} train/test split...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            images, y_indices, test_size=0.2, random_state=FOLD * 42, stratify=y_indices\n",
    "        )\n",
    "    \n",
    "    print(f\"Test set: {len(X_test)} base images\")\n",
    "    \n",
    "    test_dataset = OptimizedDataset(X_test, y_test, 'light', 200, False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nLoading model...\")\n",
    "    model = load_model(MODEL_PATH, NUM_CLASSES, device)\n",
    "    \n",
    "    print(\"\\nEvaluating model on held-out test set with augmentation...\")\n",
    "    accuracy = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\nTest Accuracy (with augmentation): {accuracy:.2f}%\")\n",
    "    print(f\"Expected from checkpoint: 78.4%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5260c8-0162-457a-be5b-7beb2ed732ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BUCK Environment",
   "language": "python",
   "name": "buck-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
